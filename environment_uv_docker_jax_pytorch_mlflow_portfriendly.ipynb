{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure no other containers are running for dev container, if they are stop and remove them\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .devcontainer/.dockerignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile .devcontainer/.dockerignore\n",
    "**/.git\n",
    "**/.vscode\n",
    "**/.idea\n",
    "**/__pycache__\n",
    "**/*.pyc\n",
    "**/*.pyo\n",
    "**/*.pyd\n",
    "**/*.swp\n",
    "**/venv\n",
    "**/env\n",
    ".env\n",
    "*.code-workspace\n",
    "data/\n",
    "notebooks/**/*.ipynb_checkpoints\n",
    "*.log\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\n",
    "# --- NEW: keep build context tiny & readable ---\n",
    "mlruns/          # MLflow runs & artifacts (often huge, root-owned)\n",
    "mlruns/**        # ensure nested paths are ignored\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .dockerignore\n"
     ]
    }
   ],
   "source": [
    "%%writefile .dockerignore\n",
    "**/.git\n",
    "**/.vscode\n",
    "**/.idea\n",
    "**/__pycache__\n",
    "**/*.pyc\n",
    "**/*.pyo\n",
    "**/*.pyd\n",
    "**/*.swp\n",
    "**/venv\n",
    "**/env\n",
    ".env\n",
    "*.code-workspace\n",
    "data/\n",
    "notebooks/**/*.ipynb_checkpoints\n",
    "*.log\n",
    ".DS_Store\n",
    "Thumbs.db\n",
    "\n",
    "# --- NEW: keep build context tiny & readable ---\n",
    "mlruns/          # MLflow runs & artifacts (often huge, root-owned)\n",
    "mlruns/**        # ensure nested paths are ignored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .env.template\n"
     ]
    }
   ],
   "source": [
    "%%writefile .env.template \n",
    "ENV_NAME=docker_dev_template\n",
    "CUDA_TAG=12.8.0\n",
    "DOCKER_BUILDKIT=1\n",
    "HOST_JUPYTER_PORT=8890\n",
    "HOST_TENSORBOARD_PORT=6008\n",
    "HOST_EXPLAINER_PORT=8050\n",
    "HOST_STREAMLIT_PORT=8501\n",
    "HOST_MLFLOW_PORT=5000\n",
    "PYTHON_VER=3.10\n",
    "JAX_PLATFORM_NAME=gpu\n",
    "XLA_PYTHON_CLIENT_PREALLOCATE=true\n",
    "XLA_PYTHON_CLIENT_ALLOCATOR=platform\n",
    "XLA_PYTHON_CLIENT_MEM_FRACTION=0.95\n",
    "XLA_FLAGS=--xla_force_host_platform_device_count=1\n",
    "JAX_DISABLE_JIT=false\n",
    "JAX_ENABLE_X64=false\n",
    "TF_FORCE_GPU_ALLOW_GROWTH=false\n",
    "JAX_PREALLOCATION_SIZE_LIMIT_BYTES=8589934592\n",
    "# Snowflake config must go in a separate file or devcontainer.env\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .cursor/settings.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile .cursor/settings.json\n",
    "{\n",
    "    \"http.proxySupport\": \"off\",\n",
    "    \"update.mode\": \"manual\",\n",
    "    \"extensions.autoUpdate\": false,\n",
    "    \"extensions.autoCheckUpdates\": false,\n",
    "    \"python.defaultInterpreterPath\": \"/app/.venv/bin/python\",\n",
    "    \"jupyter.interactiveWindow.textEditor.executeSelection\": true,\n",
    "    \"jupyter.widgetScriptSources\": [\"jsdelivr.com\", \"unpkg.com\"],\n",
    "    \"jupyter.experiments.enabled\": false,\n",
    "    \"jupyter.telemetry.enabled\": false,\n",
    "    \"python.telemetry.enabled\": false,\n",
    "    \"telemetry.telemetryLevel\": \"off\"\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .devcontainer/devcontainer.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile .devcontainer/devcontainer.json\n",
    "{\n",
    "  \"name\": \"docker_dev_template_uv\",\n",
    "  \"dockerComposeFile\": [\"../docker-compose.yml\"],\n",
    "  \"service\": \"datascience\",\n",
    "  \"workspaceFolder\": \"/workspace\",\n",
    "  \"shutdownAction\": \"stopCompose\",\n",
    "  \"runArgs\": [\"--gpus\", \"all\"],\n",
    "  \"customizations\": {\n",
    "    \"vscode\": {\n",
    "      \"extensions\": [\n",
    "        \"ms-python.python\",\n",
    "        \"ms-python.vscode-pylance\",\n",
    "        \"ms-toolsai.jupyter\",\n",
    "        \"ms-toolsai.jupyter-renderers\"\n",
    "      ],\n",
    "      \"settings\": {\n",
    "        // 1. COMPREHENSIVE TELEMETRY SETTINGS\n",
    "        \"telemetry.telemetryLevel\": \"off\",\n",
    "        \"python.telemetry.enabled\": false,\n",
    "        \"jupyter.telemetry.enabled\": false,\n",
    "        \"jupyter.experiments.enabled\": false,\n",
    "        \"update.mode\": \"manual\",\n",
    "        \"extensions.autoUpdate\": false,\n",
    "        \"extensions.autoCheckUpdates\": false,\n",
    "\n",
    "        // 2. MOVE HEAVY EXTENSIONS TO LOCAL UI HOST\n",
    "        \"remote.extensionKind\": {\n",
    "          \"ms-python.python\": [\"ui\"],\n",
    "          \"ms-python.vscode-pylance\": [\"ui\"],\n",
    "          \"ms-toolsai.jupyter\": [\"ui\"],\n",
    "          \"ms-toolsai.jupyter-renderers\": [\"ui\"]\n",
    "        },\n",
    "\n",
    "        // 3. PYTHON AND JUPYTER SETTINGS\n",
    "        \"python.defaultInterpreterPath\": \"/workspace/.venv/bin/python\",\n",
    "        \"jupyter.interactiveWindow.textEditor.executeSelection\": true,\n",
    "        \"jupyter.widgetScriptSources\": [\"jsdelivr.com\", \"unpkg.com\"]\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"remoteEnv\": {\n",
    "    \"JUPYTER_ENABLE_LAB\": \"true\"\n",
    "  },\n",
    "\n",
    "  // After container creation, set up env, check UV, Python, and key libs\n",
    "  \"postCreateCommand\": [\n",
    "    \"/bin/sh\",\n",
    "    \"-c\",\n",
    "    \".devcontainer/setup_env.sh && \\\\\\necho '## uv diagnostics ##' && uv --version && \\\\\\necho '## python ##' && which python && python -V && \\\\\\nexec .devcontainer/verify_env.py\"\n",
    "  ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .devcontainer/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile .devcontainer/Dockerfile\n",
    "# .devcontainer/Dockerfile — uv‑based replacement for the previous Conda image\n",
    "# -----------------------------------------------------------------------------\n",
    "# CUDA + cuDNN base with drivers already installed --------------------------------\n",
    "ARG CUDA_TAG=12.8.0              # <── single source of truth\n",
    "FROM nvidia/cuda:${CUDA_TAG}-cudnn-devel-ubuntu22.04\n",
    "\n",
    "# ---------- build-time ARGs ---------------------------------------------------\n",
    "ARG PYTHON_VER=3.10\n",
    "ARG ENV_NAME=docker_dev_template\n",
    "ARG JAX_PREALLOCATE=true\n",
    "ARG JAX_MEM_FRAC=0.95\n",
    "ARG JAX_ALLOCATOR=platform\n",
    "ARG JAX_PREALLOC_LIMIT=8589934592\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 1) Core OS deps, build tools, & Python (system) -----------------------------\n",
    "RUN --mount=type=cache,target=/var/cache/apt \\\n",
    "    --mount=type=cache,target=/var/lib/apt \\\n",
    "    apt-get update && apt-get install -y --no-install-recommends \\\n",
    "        bash curl ca-certificates git procps htop util-linux build-essential \\\n",
    "        python3 python3-venv python3-pip python3-dev \\\n",
    "        autoconf automake libtool m4 cmake pkg-config \\\n",
    "        jags iproute2 net-tools lsof \\\n",
    "        && pkg-config --modversion jags \\\n",
    "        && apt-get clean && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Install Node.js for VS Code remote extension host\n",
    "RUN curl -fsSL https://deb.nodesource.com/setup_18.x | bash - && \\\n",
    "    apt-get update && apt-get install -y nodejs && \\\n",
    "    rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 2) Copy a *pinned* uv & uvx binary pair from the official distroless image --\n",
    "COPY --from=ghcr.io/astral-sh/uv:0.7.12 /uv /uvx /bin/\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 3) Create project dir & copy only the lock/manifest for best layer‑caching --\n",
    "WORKDIR /app\n",
    "COPY pyproject.toml uv.lock* ./\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 4) Create an in-project venv, install deps, then symlink into /workspace\n",
    "RUN --mount=type=cache,target=/root/.cache/uv \\\n",
    "    mkdir -p /workspace && \\\n",
    "    uv venv .venv --python \"${PYTHON_VER}\" --prompt \"${ENV_NAME}\" && \\\n",
    "    (uv sync --locked || (echo \"⚠️  Lock drift detected – regenerating\" \\\n",
    "        && uv lock --upgrade --quiet && uv sync)) && \\\n",
    "    ln -s /app/.venv /workspace/.venv\n",
    "\n",
    "# Promote venv for all later layers ------------------------------------------------\n",
    "ENV VIRTUAL_ENV=/app/.venv\n",
    "ENV PATH=\"/app/.venv/bin:${PATH}\"\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 5) ---------- CUDA wheels -------------------------------------------------------\n",
    "RUN --mount=type=cache,target=/root/.cache/uv \\\n",
    "    uv pip install --pre --no-cache-dir \\\n",
    "        torch torchvision torchaudio \\\n",
    "        --index-url https://download.pytorch.org/whl/nightly/cu128 && \\\n",
    "    uv pip install --no-cache-dir \\\n",
    "        \"jax[cuda12]==0.6.0\" \\\n",
    "        -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "\n",
    "# --- CUDA toolkit sanity check (robust for runtime *and* devel images) ------\n",
    "RUN set -e; \\\n",
    "    # 1️⃣ First try: any cuda-<ver> folder?\n",
    "    CUDA_REAL=\"$(ls -d /usr/local/cuda-* 2>/dev/null | sort -V | tail -n1 || true)\"; \\\n",
    "    # 2️⃣ Fallback: flat layout shipped by some runtime images\n",
    "    if [ -z \"$CUDA_REAL\" ] && [ -d /usr/local/cuda ]; then \\\n",
    "        CUDA_REAL=\"/usr/local/cuda\"; \\\n",
    "    fi; \\\n",
    "    # 3️⃣ Bail if still empty\n",
    "    if [ -z \"$CUDA_REAL\" ]; then \\\n",
    "        echo '❌  No CUDA toolkit folder found — aborting.' >&2; exit 1; \\\n",
    "    fi; \\\n",
    "    # 4️⃣ Refresh the canonical symlink only when needed\n",
    "    if [ \"$CUDA_REAL\" != \"/usr/local/cuda\" ]; then \\\n",
    "        echo \"🔧  Linking /usr/local/cuda -> $CUDA_REAL\"; \\\n",
    "        ln -sfn \"$CUDA_REAL\" /usr/local/cuda; \\\n",
    "    fi; \\\n",
    "    echo \"🟢  CUDA toolkit detected at $CUDA_REAL\"\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 6) Install PyJAGS with the cstdint header work‑around -----------------------\n",
    "RUN CPPFLAGS=\"-include cstdint\" uv pip install --no-build-isolation pyjags==1.3.8\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 7) Copy *rest* of the project after deps → fast rebuild when code changes ---\n",
    "COPY . /app\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 8) GPU‑tuning env vars (carried forward from Conda‑based image) -------------\n",
    "ENV XLA_PYTHON_CLIENT_PREALLOCATE=${JAX_PREALLOCATE}\n",
    "ENV XLA_PYTHON_CLIENT_MEM_FRACTION=${JAX_MEM_FRAC}\n",
    "ENV XLA_PYTHON_CLIENT_ALLOCATOR=${JAX_ALLOCATOR}\n",
    "ENV JAX_PLATFORM_NAME=gpu\n",
    "ENV XLA_FLAGS=\"--xla_force_host_platform_device_count=1\"\n",
    "ENV JAX_DISABLE_JIT=false\n",
    "ENV JAX_ENABLE_X64=false\n",
    "ENV TF_FORCE_GPU_ALLOW_GROWTH=false\n",
    "ENV JAX_PREALLOCATION_SIZE_LIMIT_BYTES=${JAX_PREALLOC_LIMIT}\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 9) Library path so PyJAGS & CUDA libs resolve correctly ---------------------\n",
    "ENV LD_LIBRARY_PATH=\"/app/.venv/lib:${LD_LIBRARY_PATH}\"\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# 10) Final working directory & default command ------------------------------\n",
    "WORKDIR /workspace\n",
    "CMD [\"bash\"]\n",
    "\n",
    "# 11) Force login shells & VS Code terminals to land in /workspace\n",
    "RUN echo 'cd /workspace' > /etc/profile.d/99-workspace-cd.sh\n",
    "\n",
    "# 12) Force every IPython / Jupyter kernel to start in /workspace\n",
    "RUN mkdir -p /root/.ipython/profile_default/startup && \\\n",
    "    printf \"import os, sys\\nos.chdir('/workspace')\\nsys.path.append('/workspace')\\n\" \\\n",
    "      > /root/.ipython/profile_default/startup/00-cd-workspace.py\n",
    "\n",
    "# 13) Auto-activate uv venv in every login shell\n",
    "RUN echo '. /app/.venv/bin/activate' > /etc/profile.d/10-uv-activate.sh\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .devcontainer/verify_env.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile .devcontainer/verify_env.py\n",
    "# In your host terminal:\n",
    "cat << 'EOF' > .devcontainer/verify_env.py\n",
    "#!/usr/bin/env python3\n",
    "import encodings, jupyterlab, torch, jax, sys, os\n",
    "\n",
    "print(\"## Python & library diagnostics ##\")\n",
    "print(\"Python:\", sys.executable, sys.version.split()[0])\n",
    "print(\"🟢 encodings OK\")\n",
    "print(\"🟢 jupyterlab OK\")\n",
    "print(\"🟢 torch\", torch.__version__, \"CUDA:\", torch.cuda.is_available())\n",
    "print(\"🟢 jax\", jax.__version__, \"devices:\", jax.devices())\n",
    "EOF\n",
    "chmod +x .devcontainer/verify_env.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .devcontainer/setup_env.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile .devcontainer/setup_env.sh\n",
    "#!/usr/bin/env sh\n",
    "# Copy the template only on first run so local secrets are not overwritten\n",
    "set -eu\n",
    "if [ ! -f /workspace/.env ]; then\n",
    "  echo \"📝  Generating default .env from template\"\n",
    "  cp /workspace/.env.template /workspace/.env\n",
    "fi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .devcontainer/gpu_verify.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile .devcontainer/gpu_verify.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Verify that the GPU is accessible and JAX is correctly configured.\n",
    "This script is used during container startup.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "def check_gpu():\n",
    "    print(\"Checking GPU availability...\")\n",
    "    try:\n",
    "        import jax\n",
    "        jax.config.update('jax_platform_name', 'gpu')\n",
    "        \n",
    "        # Get device count and details\n",
    "        devices = jax.devices()\n",
    "        device_count = len(devices)\n",
    "        print(f\"JAX version: {jax.__version__}\")\n",
    "        print(f\"Available devices: {device_count}\")\n",
    "        \n",
    "        for i, device in enumerate(devices):\n",
    "            print(f\"Device {i}: {device}\")\n",
    "        \n",
    "        if device_count == 0 or 'gpu' not in str(devices[0]).lower():\n",
    "            print(\"WARNING: No GPU devices found by JAX!\")\n",
    "            return False\n",
    "        \n",
    "        # Check CUDA configuration\n",
    "        import jax.tools.jax_jit\n",
    "        jit_info = jax.tools.jax_jit.get_jax_jit_flags()\n",
    "        print(f\"JIT configuration: {jit_info}\")\n",
    "        \n",
    "        # Run a simple GPU computation\n",
    "        print(\"Running a test computation on GPU...\")\n",
    "        import numpy as np\n",
    "        x = np.ones((1000, 1000))\n",
    "        result = jax.numpy.sum(x, axis=0)\n",
    "        print(f\"Test computation result shape: {result.shape}\")\n",
    "        \n",
    "        print(\"JAX GPU verification completed successfully!\")\n",
    "        return True\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"JAX not found! Make sure JAX is installed with GPU support.\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error during GPU verification: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = check_gpu()\n",
    "    if not success:\n",
    "        print(\"WARNING: GPU verification failed!\")\n",
    "        # Not exiting with error to allow container to start anyway\n",
    "        # sys.exit(1)\n",
    "    else:\n",
    "        sys.exit(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .devcontainer/jags_verify.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile .devcontainer/jags_verify.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Verify that PyJAGS is correctly installed and working.\n",
    "This script is used by the Docker container health check.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "try:\n",
    "    import pyjags\n",
    "    print(f\"PyJAGS version: {pyjags.__version__}\")\n",
    "    \n",
    "    # Create a simple model to verify that PyJAGS works\n",
    "    code = \"\"\"\n",
    "    model {\n",
    "        # Likelihood\n",
    "        y ~ dnorm(mu, 1/sigma^2)\n",
    "        \n",
    "        # Priors\n",
    "        mu ~ dnorm(0, 0.001)\n",
    "        sigma ~ dunif(0, 100)\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sample data\n",
    "    data = {'y': 0.5}\n",
    "    \n",
    "    # Initialize model with data\n",
    "    model = pyjags.Model(code, data=data, chains=1, adapt=100)\n",
    "    print(\"JAGS model initialized successfully!\")\n",
    "    \n",
    "    # Sample from the model\n",
    "    samples = model.sample(200, vars=['mu', 'sigma'])\n",
    "    print(\"JAGS sampling completed successfully!\")\n",
    "    \n",
    "    # Verify the samples\n",
    "    mu_samples = samples['mu']\n",
    "    sigma_samples = samples['sigma']\n",
    "    print(f\"mu mean: {mu_samples.mean():.4f}\")\n",
    "    print(f\"sigma mean: {sigma_samples.mean():.4f}\")\n",
    "    \n",
    "    print(\"PyJAGS verification completed successfully!\")\n",
    "    sys.exit(0)\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"PyJAGS not found!\")\n",
    "    sys.exit(1)\n",
    "except Exception as e:\n",
    "    print(f\"Error during PyJAGS verification: {e}\")\n",
    "    sys.exit(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .devcontainer/pyjags_patch.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile .devcontainer/pyjags_patch.py\n",
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def patch_pyjags_sources():\n",
    "    print(\"Downloading and patching PyJAGS source...\")\n",
    "    os.system(\"pip download --no-binary :all: pyjags==1.3.8\")\n",
    "    os.system(\"tar -xzf pyjags-1.3.8.tar.gz\")\n",
    "    os.chdir(\"pyjags-1.3.8\")\n",
    "    \n",
    "    # Add cstdint include to all cpp files\n",
    "    for root, dirs, files in os.walk(\"src\"):\n",
    "        for file in files:\n",
    "            if file.endswith(\".cpp\") or file.endswith(\".h\"):\n",
    "                filepath = os.path.join(root, file)\n",
    "                with open(filepath, 'r') as f:\n",
    "                    content = f.read()\n",
    "                if \"#include <cstdint>\" not in content:\n",
    "                    with open(filepath, 'w') as f:\n",
    "                        f.write(\"#include <cstdint>\\n\" + content)\n",
    "                    print(f\"Patched {filepath}\")\n",
    "    \n",
    "    # Build and install\n",
    "    os.system(\"pip install --no-build-isolation .\")\n",
    "    print(\"PyJAGS installation complete!\")\n",
    "    return 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.exit(patch_pyjags_sources()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting .pre-commit-config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile .pre-commit-config.yaml\n",
    "\n",
    "repos:\n",
    "  - repo: https://github.com/astral-sh/uv-pre-commit\n",
    "    rev: 0.5.7  # Use the ref you want to point at\n",
    "    hooks:\n",
    "      - id: uv-lock      # keep uv.lock in sync\n",
    "      - id: uv-export    \n",
    "        args: [--extra=dev, --output-file=requirements-dev.txt]\n",
    "  - repo: https://github.com/pre-commit/pre-commit-hooks\n",
    "    rev: v4.6.0\n",
    "    hooks:\n",
    "      - id: trailing-whitespace\n",
    "      - id: end-of-file-fixer\n",
    "      - id: check-yaml\n",
    "      - id: check-added-large-files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting docker-compose.yml\n"
     ]
    }
   ],
   "source": [
    "%%writefile docker-compose.yml\n",
    "# docker-compose.yml\n",
    "name: ${ENV_NAME:-docker_dev_template}\n",
    "\n",
    "services:\n",
    "  datascience:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: .devcontainer/Dockerfile\n",
    "      args:\n",
    "        PYTHON_VER: ${PYTHON_VER:-3.10}\n",
    "        ENV_NAME: ${ENV_NAME:-docker_dev_template}\n",
    "        JAX_PREALLOCATE: ${XLA_PYTHON_CLIENT_PREALLOCATE:-true}\n",
    "        JAX_MEM_FRAC: ${XLA_PYTHON_CLIENT_MEM_FRACTION:-0.95}\n",
    "        JAX_ALLOCATOR: ${XLA_PYTHON_CLIENT_ALLOCATOR:-platform}\n",
    "        JAX_PREALLOC_LIMIT: ${JAX_PREALLOCATION_SIZE_LIMIT_BYTES:-8589934592}\n",
    "\n",
    "    # (Removed explicit container_name to avoid \"already in use\" conflicts.)\n",
    "\n",
    "    # Enhanced restart policy to handle port conflicts\n",
    "    restart: unless-stopped\n",
    "\n",
    "    depends_on:\n",
    "      mlflow:\n",
    "        condition: service_healthy\n",
    "\n",
    "    gpus: all\n",
    "\n",
    "    environment:\n",
    "      - PYTHON_VER=${PYTHON_VER}\n",
    "      - NVIDIA_VISIBLE_DEVICES=all\n",
    "      - NVIDIA_DRIVER_CAPABILITIES=compute,utility,graphics,display\n",
    "      - JAX_PLATFORM_NAME=${JAX_PLATFORM_NAME}\n",
    "      - XLA_PYTHON_CLIENT_PREALLOCATE=${XLA_PYTHON_CLIENT_PREALLOCATE}\n",
    "      - XLA_PYTHON_CLIENT_ALLOCATOR=${XLA_PYTHON_CLIENT_ALLOCATOR}\n",
    "      - XLA_PYTHON_CLIENT_MEM_FRACTION=${XLA_PYTHON_CLIENT_MEM_FRACTION}\n",
    "      - XLA_FLAGS=${XLA_FLAGS}\n",
    "      - JAX_DISABLE_JIT=${JAX_DISABLE_JIT}\n",
    "      - JAX_ENABLE_X64=${JAX_ENABLE_X64}\n",
    "      - TF_FORCE_GPU_ALLOW_GROWTH=${TF_FORCE_GPU_ALLOW_GROWTH}\n",
    "      - JAX_PREALLOCATION_SIZE_LIMIT_BYTES=${JAX_PREALLOCATION_SIZE_LIMIT_BYTES}\n",
    "\n",
    "    volumes:\n",
    "      - .:/workspace\n",
    "      - ./mlruns:/workspace/mlruns        # new\n",
    "\n",
    "    ports:\n",
    "      # Enhanced port configuration with fallback options\n",
    "      - \"${HOST_JUPYTER_PORT:-8890}:8888\"\n",
    "      - \"${HOST_TENSORBOARD_PORT:-}:6008\"\n",
    "      - \"${HOST_EXPLAINER_PORT:-8050}:8050\"\n",
    "      - \"${HOST_STREAMLIT_PORT:-}:8501\"\n",
    "\n",
    "    command: >\n",
    "      jupyter lab\n",
    "        --ip=0.0.0.0\n",
    "        --port=8888\n",
    "        --allow-root\n",
    "        --NotebookApp.token=\"${JUPYTER_TOKEN:-jupyter}\"\n",
    "        --NotebookApp.allow_origin='*'\n",
    "\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\", \"bash --version && uv --help || exit 1\"]\n",
    "      interval: 30s\n",
    "      timeout: 5s\n",
    "      retries: 3\n",
    "\n",
    "    # Enhanced labels for better debugging\n",
    "    labels:\n",
    "      - \"com.docker.compose.project=${ENV_NAME:-docker_dev_template}\"\n",
    "      - \"com.docker.compose.service=datascience\"\n",
    "      - \"description=AI/ML Development Environment with GPU Support\"\n",
    "\n",
    "  mlflow:\n",
    "    image: ghcr.io/mlflow/mlflow:latest\n",
    "    command: >\n",
    "      mlflow server\n",
    "      --host 0.0.0.0\n",
    "      --port 5000\n",
    "      --backend-store-uri sqlite:///mlflow.db\n",
    "      --default-artifact-root /mlflow_artifacts\n",
    "    environment:\n",
    "      MLFLOW_EXPERIMENTS_DEFAULT_ARTIFACT_LOCATION: /mlflow_artifacts\n",
    "    volumes:\n",
    "      - ./mlruns:/mlflow_artifacts    # artifacts + run metadata\n",
    "      - ./mlflow_db:/mlflow_db        # SQLite backend store\n",
    "    ports:\n",
    "      - \"${HOST_MLFLOW_PORT:-5000}:5000\"\n",
    "    restart: unless-stopped\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\",\n",
    "             \"python - <<'PY'\\nimport requests,sys; requests.get('http://localhost:5000/health').raise_for_status()\\nPY\"]\n",
    "      interval: 10s\n",
    "      timeout: 3s\n",
    "      retries: 5\n",
    "      start_period: 30s\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pyproject.toml\n"
     ]
    }
   ],
   "source": [
    "%%writefile pyproject.toml\n",
    "[project]\n",
    "name = \"docker_dev_template\"\n",
    "version = \"0.1.0\"\n",
    "description = \"Pytorch and Jax GPU docker container\"\n",
    "authors = [\n",
    "  { name = \"Geoffrey Hadfield\" },\n",
    "]\n",
    "license = \"MIT\"\n",
    "readme = \"README.md\"\n",
    "\n",
    "# ─── Restrict to Python 3.10–3.12 ──────────────────────────────\n",
    "requires-python = \">=3.10,<3.13\"\n",
    "\n",
    "dependencies = [\n",
    "  \"pandas>=1.2.0\",\n",
    "  \"numpy>=1.20.0\",\n",
    "  \"matplotlib>=3.4.0\",\n",
    "  \"mlflow>=2.10.2\",\n",
    "  \"mlflow-skinny>=2.10.2\",\n",
    "  \"scikit-learn>=1.4.2\",\n",
    "  \"pymc>=5.0.0\",\n",
    "  \"arviz>=0.14.0\",\n",
    "  \"statsmodels>=0.13.0\",\n",
    "  \"jupyterlab>=3.0.0\",\n",
    "  \"seaborn>=0.11.0\",\n",
    "  \"tabulate>=0.9.0\",\n",
    "  \"shap>=0.40.0\",\n",
    "  \"xgboost>=1.5.0\",\n",
    "  \"lightgbm>=3.3.0\",\n",
    "  \"catboost>=1.2.8,<1.3.0\",\n",
    "  \"scipy>=1.7.0\",\n",
    "  \"shapash[report]>=2.3.0\",\n",
    "  \"shapiq>=0.1.0\",\n",
    "  \"explainerdashboard==0.5.1\",\n",
    "  \"ipywidgets>=8.0.0\",\n",
    "  \"nutpie>=0.7.1\",   # new: nutpie backend for PyMC\n",
    "  \"numpyro>=0.18.0,<1.0.0\",\n",
    "  \"jax==0.6.0\",\n",
    "  \"jaxlib==0.6.0\",\n",
    "  \"pytensor>=2.18.3\",  # explicit version for CUDA support\n",
    "  \"aesara>=2.9.4\",     # alternative backend option\n",
    "  \"tqdm>=4.67.0\",\n",
    "  \"pyarrow>=12.0.0\",\n",
    "  \"optuna>=3.0.0\",\n",
    "  \"optuna-integration[mlflow]>=0.2.0\",\n",
    "  \"omegaconf>=2.3.0,<2.4.0\",\n",
    "  \"hydra-core>=1.3.2,<1.4.0\",\n",
    "  \"streamlit>=1.46.1,<2.0.0\",\n",
    "  \"cloudpickle>=2.4.0\",\n",
    "]\n",
    "\n",
    "[project.optional-dependencies]\n",
    "dev = [\n",
    "  \"pytest>=7.0.0\",\n",
    "  \"black>=23.0.0\",\n",
    "  \"isort>=5.0.0\",\n",
    "  \"flake8>=5.0.0\",\n",
    "  \"mypy>=1.0.0\",\n",
    "  \"invoke>=2.2\",\n",
    "]\n",
    "\n",
    "cuda = [\n",
    "  \"cupy-cuda12x>=12.0.0\",  # For CUDA 12.x\n",
    "]\n",
    "\n",
    "[tool.pytensor]\n",
    "# Default configuration for PyTensor\n",
    "device = \"cuda\"          # Use CUDA by default if available\n",
    "floatX = \"float32\"       # Use float32 by default for better CUDA performance\n",
    "allow_gc = true          # Allow garbage collection\n",
    "optimizer = \"fast_run\"   # Fast run optimization by default\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tasks.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tasks.py\n",
    "# tasks.py  ── invoke ≥2.2\n",
    "from invoke import task, Context  # type: ignore\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pathlib\n",
    "import tempfile\n",
    "import datetime as _dt\n",
    "import atexit\n",
    "import socket\n",
    "import contextlib\n",
    "import errno\n",
    "\n",
    "\n",
    "BASE_ENV = pathlib.Path(__file__).parent\n",
    "\n",
    "\n",
    "# Track temporary env files for cleanup\n",
    "_saved_env_files: List[str] = []\n",
    "\n",
    "\n",
    "def _parse_port(port: Union[str, int, None]) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Parse and validate a port number.\n",
    "    \n",
    "    Args:\n",
    "        port: Port number as string or int, or None\n",
    "        \n",
    "    Returns:\n",
    "        Validated port number as int, or None if input was None\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: If port is invalid or out of range\n",
    "    \"\"\"\n",
    "    if port is None:\n",
    "        return None\n",
    "        \n",
    "    try:\n",
    "        port_int = int(port)\n",
    "        if not (0 < port_int < 65536):\n",
    "            raise ValueError(f\"Port {port_int} out of valid range (1-65535)\")\n",
    "        return port_int\n",
    "    except (TypeError, ValueError) as e:\n",
    "        raise ValueError(f\"Invalid port value: {port}\") from e\n",
    "\n",
    "\n",
    "def _first_free_port(start: int = 5200) -> int:\n",
    "    \"\"\"Return the first TCP port >= *start* that is unused on localhost.\"\"\"\n",
    "    print(f\"DEBUG: Searching for free port starting at {start}\")  # Debug\n",
    "    import socket\n",
    "    import contextlib\n",
    "    for port in range(start, 65535):\n",
    "        with contextlib.closing(socket.socket()) as s:\n",
    "            if s.connect_ex((\"127.0.0.1\", port)):\n",
    "                print(f\"DEBUG: Found free port {port}\")  # Debug\n",
    "                return port\n",
    "    raise RuntimeError(\"No free port found\")\n",
    "\n",
    "\n",
    "def _free_port(start=5200) -> int:\n",
    "    \"\"\"Find a free port by letting the OS assign one.\"\"\"\n",
    "    print(f\"DEBUG: Finding free port starting at {start}\")  # Debug\n",
    "    import socket\n",
    "    import contextlib\n",
    "    with contextlib.closing(\n",
    "        socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    ) as s:\n",
    "        s.bind(('', 0))\n",
    "        port = s.getsockname()[1]\n",
    "        print(f\"DEBUG: Found free port {port}\")  # Debug\n",
    "        return port\n",
    "\n",
    "\n",
    "def _port_free(host: str, port: int, timeout: float = 0.1) -> bool:\n",
    "    \"\"\"\n",
    "    Return True iff *host:port* is NOT in use.\n",
    "\n",
    "    Uses a non-blocking TCP connect – works on Linux, macOS, Windows,\n",
    "    inside or outside WSL – and does **not** rely on lsof / netstat.\n",
    "    \"\"\"\n",
    "    print(f\"DEBUG: Checking if port {port} is free on {host}\")  # Debug\n",
    "    try:\n",
    "        with contextlib.closing(\n",
    "            socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "        ) as s:\n",
    "            s.settimeout(timeout)\n",
    "            s.connect((host, port))\n",
    "            print(f\"DEBUG: Port {port} is in use\")  # Debug\n",
    "            return False      # connection succeeded ⇒ something listening\n",
    "    except (OSError, socket.timeout):\n",
    "        print(f\"DEBUG: Port {port} is free\")  # Debug\n",
    "        return True           # connection failed ⇒ port is free\n",
    "\n",
    "\n",
    "def _find_port(preferred: int, start: int = 5200) -> int:\n",
    "    \"\"\"\n",
    "    Try to use preferred port, fall back to finding first available port.\n",
    "    \n",
    "    Args:\n",
    "        preferred: The preferred port number to try first\n",
    "        start: Where to start searching if preferred port is taken\n",
    "        \n",
    "    Returns:\n",
    "        An available port number\n",
    "    \"\"\"\n",
    "    print(f\"DEBUG: Trying preferred port {preferred}\")  # Debug\n",
    "    if _port_free(\"127.0.0.1\", preferred):\n",
    "        return preferred\n",
    "    return _first_free_port(start)\n",
    "\n",
    "\n",
    "def _write_envfile(name: str, \n",
    "                   ports: Optional[dict[str, int]] = None) -> pathlib.Path:\n",
    "    \"\"\"\n",
    "    Create a throw-away .env file for the current `invoke up` run.\n",
    "    \n",
    "    Docker-compose will use this to see the chosen host-ports. We include all\n",
    "    services we know about; anything unset falls back to .env.template defaults.\n",
    "    \"\"\"\n",
    "    env_lines = [f\"ENV_NAME={name}\"]\n",
    "    mapping = {\n",
    "        \"jupyter\": \"HOST_JUPYTER_PORT\",\n",
    "        \"tensorboard\": \"HOST_TENSORBOARD_PORT\",\n",
    "        \"explainer\": \"HOST_EXPLAINER_PORT\",\n",
    "        \"streamlit\": \"HOST_STREAMLIT_PORT\",\n",
    "        \"mlflow\": \"HOST_MLFLOW_PORT\",      # NEW\n",
    "    }\n",
    "    for svc, var in mapping.items():\n",
    "        if ports and svc in ports:\n",
    "            env_lines.append(f\"{var}={ports[svc]}\")\n",
    "    env_lines.append(f\"# generated {_dt.datetime.now().isoformat()}\")\n",
    "    tmp = tempfile.NamedTemporaryFile(\n",
    "        \"w\", \n",
    "        delete=False, \n",
    "        prefix=\".env.\",\n",
    "        dir=BASE_ENV\n",
    "    )\n",
    "    tmp.write(\"\\n\".join(env_lines))\n",
    "    tmp.close()\n",
    "    _saved_env_files.append(tmp.name)\n",
    "    return pathlib.Path(tmp.name)\n",
    "\n",
    "\n",
    "# Register cleanup function\n",
    "def _cleanup_env_files() -> None:\n",
    "    \"\"\"Remove all temporary env files.\"\"\"\n",
    "    for path in _saved_env_files:\n",
    "        try:\n",
    "            os.remove(path)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "\n",
    "atexit.register(_cleanup_env_files)\n",
    "\n",
    "\n",
    "def _compose(\n",
    "    c: Context,\n",
    "    cmd: str,\n",
    "    name: str,\n",
    "    rebuild: bool = False,\n",
    "    force_pty: bool = False,\n",
    "    ports: Optional[dict[str, int]] = None,\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Wrapper around `docker compose` that also sanity-checks host ports.\n",
    "    \"\"\"\n",
    "    # ---------- NEW pre-flight check --------------------------------------\n",
    "    if ports:\n",
    "        for svc, port in ports.items():\n",
    "            if port is None:\n",
    "                continue\n",
    "            if not _port_free(\"127.0.0.1\", int(port)):\n",
    "                print(f\"❌  Host port {port} already bound – \"\n",
    "                      f\"{svc} cannot start. Choose another port (invoke up \"\n",
    "                      f\"--{svc}-port XXXXX) or free it first.\")\n",
    "                sys.exit(1)\n",
    "\n",
    "    env = {**os.environ, \"ENV_NAME\": name, \"COMPOSE_PROJECT_NAME\": name}\n",
    "    \n",
    "    # Add port overrides if provided\n",
    "    if ports:\n",
    "        port_mapping = {\n",
    "            \"jupyter\": \"HOST_JUPYTER_PORT\",\n",
    "            \"tensorboard\": \"HOST_TENSORBOARD_PORT\", \n",
    "            \"explainer\": \"HOST_EXPLAINER_PORT\",\n",
    "            \"streamlit\": \"HOST_STREAMLIT_PORT\",\n",
    "        }\n",
    "        for service, port in ports.items():\n",
    "            if service in port_mapping:\n",
    "                env[port_mapping[service]] = str(port)\n",
    "    \n",
    "    use_pty = force_pty or (os.name != \"nt\" and sys.stdin.isatty())\n",
    "\n",
    "    if not use_pty and not getattr(_compose, \"_warned\", False):\n",
    "        print(\"ℹ️  PTY not supported – running without TTY.\")\n",
    "        _compose._warned = True  # type: ignore[attr-defined]\n",
    "\n",
    "    if rebuild:\n",
    "        full_cmd = f\"docker compose -p {name} {cmd} --build\"\n",
    "    else:\n",
    "        full_cmd = f\"docker compose -p {name} {cmd}\"\n",
    "    c.run(full_cmd, env=env, pty=use_pty)\n",
    "\n",
    "\n",
    "@task(\n",
    "    help={\n",
    "        \"name\": \"Project/venv name (defaults to folder name)\",\n",
    "        \"use_pty\": \"Force PTY even on non-POSIX hosts\",\n",
    "        \"jupyter_port\": \"Jupyter Lab port (default: 8890)\",\n",
    "        \"tensorboard_port\": \"TensorBoard port (default: auto-assigned)\",\n",
    "        \"explainer_port\": \"Explainer Dashboard port (default: auto-assigned)\", \n",
    "        \"streamlit_port\": \"Streamlit port (default: auto-assigned)\",\n",
    "        \"mlflow_port\": \"MLflow UI port (default: 5000, auto-assigns if busy)\",\n",
    "    }\n",
    ")\n",
    "def up(\n",
    "    c,\n",
    "    name: Optional[str] = None,\n",
    "    rebuild: bool = False,\n",
    "    detach: bool = True,\n",
    "    use_pty: bool = False,\n",
    "    jupyter_port: Union[str, int, None] = None,\n",
    "    tensorboard_port: Union[str, int, None] = None,\n",
    "    explainer_port: Union[str, int, None] = None,\n",
    "    streamlit_port: Union[str, int, None] = None,\n",
    "    mlflow_port: Union[str, int, None] = None,\n",
    ") -> None:\n",
    "    \"\"\"Build (optionally --rebuild) & start the container with custom ports.\"\"\"\n",
    "    name = name or BASE_ENV.name\n",
    "\n",
    "    # ---------- Parse and validate all ports -----------------\n",
    "    try:\n",
    "        jupyter_port = _parse_port(jupyter_port)\n",
    "        tensorboard_port = _parse_port(tensorboard_port)\n",
    "        explainer_port = _parse_port(explainer_port)\n",
    "        streamlit_port = _parse_port(streamlit_port)\n",
    "        mlflow_port = _parse_port(mlflow_port)\n",
    "    except ValueError as e:\n",
    "        print(f\"❌ Port validation failed: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # ---------- build dynamic port map -----------------\n",
    "    ports = {}\n",
    "    if jupyter_port is not None:\n",
    "        ports[\"jupyter\"] = jupyter_port\n",
    "    if tensorboard_port is not None:\n",
    "        ports[\"tensorboard\"] = tensorboard_port\n",
    "    if explainer_port is not None:\n",
    "        ports[\"explainer\"] = explainer_port\n",
    "    if streamlit_port is not None:\n",
    "        ports[\"streamlit\"] = streamlit_port\n",
    "\n",
    "    # ---------- Explainer auto-assign (NEW) ------------\n",
    "    print(\"DEBUG: Starting explainer port assignment\")  # Debug\n",
    "    try:\n",
    "        # Try to use the explainer's version first\n",
    "        from src.mlops.explainer import _first_free_port  # type: ignore\n",
    "        print(\"DEBUG: Successfully imported _first_free_port from explainer\")  # Debug\n",
    "    except ModuleNotFoundError:\n",
    "        print(\"DEBUG: Failed to import _first_free_port, using local implementation\")  # Debug\n",
    "        # We'll use our local _first_free_port implementation\n",
    "        pass\n",
    "\n",
    "    if explainer_port is None:\n",
    "        print(\"DEBUG: No explainer port specified, finding one\")  # Debug\n",
    "        explainer_port = _find_port(8050, 5200)\n",
    "    elif not _port_free(\"127.0.0.1\", explainer_port):\n",
    "        print(f\"DEBUG: Specified explainer port {explainer_port} is in use\")  # Debug\n",
    "        sys.exit(1)\n",
    "    ports[\"explainer\"] = explainer_port\n",
    "    print(f\"🔌 Explainer host-port → {explainer_port}\")\n",
    "\n",
    "    # ----- MLflow auto-assign (default 5000) -----------\n",
    "    print(\"DEBUG: Starting MLflow port assignment\")  # Debug\n",
    "    if mlflow_port is None:\n",
    "        print(\"DEBUG: No MLflow port specified, finding one\")  # Debug\n",
    "        mlflow_port = _find_port(5000, 5200)\n",
    "    elif not _port_free(\"127.0.0.1\", mlflow_port):\n",
    "        print(f\"DEBUG: Specified MLflow port {mlflow_port} is in use\")  # Debug\n",
    "        sys.exit(1)\n",
    "    ports[\"mlflow\"] = mlflow_port\n",
    "    print(f\"🔌 MLflow host-port → {mlflow_port}\")\n",
    "\n",
    "    # Generate environment file\n",
    "    env_path = _write_envfile(name, ports)\n",
    "    compose_cmd = \"up -d\" if detach else \"up\"\n",
    "\n",
    "    _compose(\n",
    "        c,\n",
    "        f\"--env-file {env_path} {compose_cmd}\",\n",
    "        name,\n",
    "        rebuild=rebuild,\n",
    "        force_pty=use_pty,\n",
    "        ports=ports,\n",
    "    )\n",
    "\n",
    "\n",
    "@task(\n",
    "    help={\n",
    "        \"name\": \"Project/venv name (defaults to folder name)\",\n",
    "    }\n",
    ")\n",
    "def stop(c, name: Optional[str] = None) -> None:\n",
    "    \"\"\"Stop and remove dev container (keeps volumes).\"\"\"\n",
    "    name = name or BASE_ENV.name\n",
    "    cmd = f\"docker compose -p {name} down\"\n",
    "    try:\n",
    "        c.run(cmd)\n",
    "        print(f\"\\n🛑 Stopped and removed project '{name}'\")\n",
    "    except Exception:\n",
    "        print(f\"❌ No running containers found for project '{name}'\")\n",
    "\n",
    "\n",
    "@task\n",
    "def shell(c, name: str | None = None) -> None:\n",
    "    \"\"\"Open an interactive shell inside the running container.\"\"\"\n",
    "    name = name or BASE_ENV.name\n",
    "    cmd = f\"docker compose -p {name} ps -q datascience\"\n",
    "    cid = c.run(cmd, hide=True).stdout.strip()\n",
    "    c.run(f\"docker exec -it {cid} bash\", env={\"ENV_NAME\": name}, pty=False)\n",
    "\n",
    "\n",
    "@task\n",
    "def clean(c) -> None:\n",
    "    \"\"\"Prune stopped containers + dangling images.\"\"\"\n",
    "    c.run(\"docker system prune -f\")\n",
    "\n",
    "\n",
    "@task\n",
    "def ports(c, name: str | None = None) -> None:\n",
    "    \"\"\"Show current port mappings for the named project.\"\"\"\n",
    "    name = name or BASE_ENV.name\n",
    "    cmd = f\"docker compose -p {name} ps --format table\"\n",
    "    try:\n",
    "        c.run(cmd, hide=False)\n",
    "        print(f\"\\n📊 Port mappings for project '{name}':\")\n",
    "        print(\"=\" * 50)\n",
    "    except Exception:\n",
    "        print(f\"❌ No running containers found for project '{name}'\")\n",
    "        print(\"\\n💡 Usage examples:\")\n",
    "        print(\"  invoke up --name myproject --jupyter-port 8891\")\n",
    "        print(\"  invoke up --name myproject --jupyter-port 8892 \\\\\")\n",
    "        print(\"    --tensorboard-port 6009\")\n",
    "\n",
    "\n",
    "# --- utilities ---------------------------------------------------------------\n",
    "def _norm(path: str | pathlib.Path) -> str:\n",
    "    \"\"\"Return a lower-case, forward-slash, no-trailing-slash version of *path*.\"\"\"\n",
    "    p = str(path).replace(\"\\\\\", \"/\").rstrip(\"/\").lower()\n",
    "    return p\n",
    "\n",
    "def _docker_projects_from_this_repo() -> set[str]:\n",
    "    \"\"\"\n",
    "    Discover every Compose *project name* whose working_dir label ends with\n",
    "    the current repo path.\n",
    "\n",
    "    Works across Windows ↔ WSL ↔ macOS because we do suffix-match on a\n",
    "    normalised path.\n",
    "    \"\"\"\n",
    "    here_tail = _norm(pathlib.Path(__file__).parent.resolve())\n",
    "    cmd = (\n",
    "        \"docker container ls -a \"\n",
    "        \"--format '{{.Label \\\"com.docker.compose.project\\\"}} \"\n",
    "        \"{{.Label \\\"com.docker.compose.project.working_dir\\\"}}' \"\n",
    "        \"--filter label=com.docker.compose.project\"\n",
    "    )\n",
    "    projects: set[str] = set()\n",
    "    for line in os.popen(cmd).read().strip().splitlines():\n",
    "        try:\n",
    "            proj, wd = line.split(maxsplit=1)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if _norm(wd).endswith(here_tail):\n",
    "            projects.add(proj)\n",
    "    return projects\n",
    "\n",
    "# --- task --------------------------------------------------------------------\n",
    "@task(\n",
    "    help={\n",
    "        \"name\": \"Project name (defaults to folder). Ignored with --all.\",\n",
    "        \"all\":  \"Remove *all* projects launched from this repo.\",\n",
    "        \"rmi\":  \"Image-removal policy: all | local | none (default: local).\",\n",
    "    }\n",
    ")\n",
    "def down(c, name: str | None = None, all: bool = False, rmi: str = \"local\"):\n",
    "    \"\"\"\n",
    "    Stop containers **and** fully delete every artefact so next `invoke up`\n",
    "    starts from a clean slate.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    invoke down                  # nuke current-folder project\n",
    "    invoke down --name ml_project --rmi all   # wipe everything for ml_project\n",
    "    invoke down --all            # tear down every project from this repo\n",
    "    \"\"\"\n",
    "    if rmi not in {\"all\", \"local\", \"none\"}:\n",
    "        raise ValueError(\"--rmi must be all | local | none\")\n",
    "\n",
    "    targets = _docker_projects_from_this_repo() if all else {name or BASE_ENV.name}\n",
    "    flags = \"-v --remove-orphans\"\n",
    "    if rmi != \"none\":\n",
    "        flags += f\" --rmi {rmi}\"\n",
    "\n",
    "    for proj in targets:\n",
    "        try:\n",
    "            c.run(f\"docker compose -p {proj} down {flags}\")\n",
    "            print(f\"🗑️  Removed project '{proj}'\")\n",
    "        except Exception:\n",
    "            print(f\"⚠️  Nothing to remove for '{proj}'\")\n",
    "\n",
    "\n",
    "@task(\n",
    "    help={\n",
    "        \"yaml\": \"Path to dashboard.yaml file\",\n",
    "        \"port\": \"Port to serve on (default: 8150)\",\n",
    "        \"host\": \"Host to bind to (default: 0.0.0.0)\",\n",
    "    }\n",
    ")\n",
    "def dashboard(c, yaml: str, port: int = 8150, host: str = \"0.0.0.0\") -> None:\n",
    "    \"\"\"\n",
    "    Serve a saved ExplainerDashboard from a YAML configuration file.\n",
    "    \n",
    "    This task allows you to re-serve dashboards that were previously saved\n",
    "    with build_and_log_dashboard(save_yaml=True).\n",
    "    \n",
    "    Examples:\n",
    "        invoke dashboard --yaml dashboard.yaml\n",
    "        invoke dashboard --yaml dashboard.yaml --port 8200\n",
    "    \"\"\"\n",
    "    import sys\n",
    "    from pathlib import Path\n",
    "    from src.mlops.explainer import load_dashboard_yaml\n",
    "    \n",
    "    yaml_path = Path(yaml)\n",
    "    if not yaml_path.exists():\n",
    "        print(f\"❌ Dashboard YAML file not found: {yaml_path}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    # Check if port is available\n",
    "    if not _port_free(host, port):\n",
    "        print(f\"❌ Port {port} is already in use on {host}\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    try:\n",
    "        print(f\"🔄 Loading dashboard from {yaml_path}\")\n",
    "        dashboard_obj = load_dashboard_yaml(yaml_path)\n",
    "        \n",
    "        print(f\"🌐 Serving ExplainerDashboard on {host}:{port}\")\n",
    "        dashboard_obj.run(port=port, host=host, use_waitress=True, open_browser=False)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to load or serve dashboard: {e}\")\n",
    "        sys.exit(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/diagnose_devcontainer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/diagnose_devcontainer.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Comprehensive diagnostic script for dev container issues.\n",
    "Run this inside the container to diagnose Python environment and remote extension problems.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def run_command(cmd, description):\n",
    "    \"\"\"Run a command and return its output.\"\"\"\n",
    "    print(f\"\\n🔍 {description}\")\n",
    "    print(\"=\" * 60)\n",
    "    try:\n",
    "        result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"✅ {result.stdout.strip()}\")\n",
    "        else:\n",
    "            print(f\"❌ Error (code {result.returncode}): {result.stderr.strip()}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Exception: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def check_paths_and_environment():\n",
    "    \"\"\"Check Python paths and environment variables.\"\"\"\n",
    "    print(\"\\n🐍 PYTHON ENVIRONMENT DIAGNOSTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Python executable and version\n",
    "    print(f\"Python executable: {sys.executable}\")\n",
    "    print(f\"Python version: {sys.version}\")\n",
    "    print(f\"Python path: {sys.path[:3]}...\")  # First few paths\n",
    "    \n",
    "    # Environment variables\n",
    "    print(f\"\\nVIRTUAL_ENV: {os.environ.get('VIRTUAL_ENV', 'Not set')}\")\n",
    "    print(f\"PATH (first 3): {':'.join(os.environ.get('PATH', '').split(':')[:3])}\")\n",
    "    \n",
    "    # Virtual environment validation\n",
    "    venv_path = Path('/app/.venv')\n",
    "    if venv_path.exists():\n",
    "        print(f\"✅ Virtual environment exists at {venv_path}\")\n",
    "        print(f\"   - bin directory: {list(venv_path.glob('bin/python*'))}\")\n",
    "        print(f\"   - site-packages: {(venv_path / 'lib/python3.10/site-packages').exists()}\")\n",
    "    else:\n",
    "        print(f\"❌ Virtual environment NOT found at {venv_path}\")\n",
    "\n",
    "\n",
    "def check_key_packages():\n",
    "    \"\"\"Check if key packages are importable.\"\"\"\n",
    "    print(\"\\n📦 PACKAGE IMPORT TESTS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    packages = [\n",
    "        'jax', 'torch', 'numpy', 'pandas', 'matplotlib', \n",
    "        'jupyterlab', 'streamlit', 'sklearn'\n",
    "    ]\n",
    "    \n",
    "    for package in packages:\n",
    "        try:\n",
    "            if package == 'sklearn':\n",
    "                import sklearn\n",
    "                version = sklearn.__version__\n",
    "            else:\n",
    "                module = __import__(package)\n",
    "                version = getattr(module, '__version__', 'unknown')\n",
    "            print(f\"✅ {package}: {version}\")\n",
    "        except ImportError as e:\n",
    "            print(f\"❌ {package}: Import failed - {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  {package}: {e}\")\n",
    "\n",
    "\n",
    "def check_gpu_environment():\n",
    "    \"\"\"Check GPU-related environment variables.\"\"\"\n",
    "    print(\"\\n🎮 GPU ENVIRONMENT VARIABLES\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    gpu_env_vars = [\n",
    "        'XLA_PYTHON_CLIENT_PREALLOCATE',\n",
    "        'XLA_PYTHON_CLIENT_ALLOCATOR', \n",
    "        'XLA_PYTHON_CLIENT_MEM_FRACTION',\n",
    "        'JAX_PLATFORM_NAME',\n",
    "        'XLA_FLAGS',\n",
    "        'JAX_DISABLE_JIT',\n",
    "        'JAX_ENABLE_X64',\n",
    "        'JAX_PREALLOCATION_SIZE_LIMIT_BYTES',\n",
    "        'TF_FORCE_GPU_ALLOW_GROWTH',\n",
    "        'NVIDIA_VISIBLE_DEVICES',\n",
    "        'NVIDIA_DRIVER_CAPABILITIES'\n",
    "    ]\n",
    "    \n",
    "    for var in gpu_env_vars:\n",
    "        value = os.environ.get(var, 'Not set')\n",
    "        print(f\"   {var}: {value}\")\n",
    "\n",
    "\n",
    "def check_gpu_support():\n",
    "    \"\"\"Check GPU support for JAX and PyTorch with enhanced diagnostics.\"\"\"\n",
    "    print(\"\\n🎮 ENHANCED GPU SUPPORT CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # JAX GPU check with detailed info\n",
    "    try:\n",
    "        import jax\n",
    "        print(f\"JAX version: {jax.__version__}\")\n",
    "        \n",
    "        devices = jax.devices()\n",
    "        print(f\"JAX devices: {devices}\")\n",
    "        \n",
    "        if devices:\n",
    "            for i, device in enumerate(devices):\n",
    "                print(f\"   Device {i}: {device}\")\n",
    "                \n",
    "        if any('gpu' in str(device).lower() or 'cuda' in str(device).lower() for device in devices):\n",
    "            print(\"✅ JAX GPU/CUDA support detected!\")\n",
    "            \n",
    "            # Test a simple computation\n",
    "            try:\n",
    "                import jax.numpy as jnp\n",
    "                x = jnp.ones((1000, 1000))\n",
    "                result = jnp.sum(x)\n",
    "                print(f\"   ✅ JAX GPU computation test passed: sum = {result}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️  JAX GPU computation test failed: {e}\")\n",
    "        else:\n",
    "            print(\"⚠️  JAX GPU support not detected\")\n",
    "            print(\"   This might be due to GPU architecture compatibility\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ JAX GPU check failed: {e}\")\n",
    "    \n",
    "    # PyTorch GPU check with enhanced info\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"\\nPyTorch version: {torch.__version__}\")\n",
    "        print(f\"PyTorch CUDA available: {torch.cuda.is_available()}\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            device_count = torch.cuda.device_count()\n",
    "            print(f\"✅ PyTorch CUDA device count: {device_count}\")\n",
    "            \n",
    "            for i in range(device_count):\n",
    "                try:\n",
    "                    device_name = torch.cuda.get_device_name(i)\n",
    "                    memory_total = torch.cuda.get_device_properties(i).total_memory\n",
    "                    print(f\"   Device {i}: {device_name}\")\n",
    "                    print(f\"     Total memory: {memory_total / (1024**3):.1f} GB\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   Device {i}: Error getting info - {e}\")\n",
    "            \n",
    "            # Test a simple computation\n",
    "            try:\n",
    "                device = torch.device('cuda:0')\n",
    "                x = torch.ones(1000, 1000, device=device)\n",
    "                result = torch.sum(x)\n",
    "                print(f\"   ✅ PyTorch GPU computation test passed: sum = {result}\")\n",
    "            except Exception as e:\n",
    "                print(f\"   ⚠️  PyTorch GPU computation test failed: {e}\")\n",
    "        else:\n",
    "            print(\"⚠️  PyTorch CUDA not available\")\n",
    "            print(\"   Check CUDA installation and GPU compatibility\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ PyTorch GPU check failed: {e}\")\n",
    "\n",
    "\n",
    "def check_workspace_mount():\n",
    "    \"\"\"Check if workspace is properly mounted.\"\"\"\n",
    "    print(\"\\n📁 WORKSPACE MOUNT CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    workspace_path = Path('/workspace')\n",
    "    if workspace_path.exists():\n",
    "        print(f\"✅ /workspace directory exists\")\n",
    "        try:\n",
    "            contents = list(workspace_path.iterdir())[:10]  # First 10 items\n",
    "            print(f\"   Contents (first 10): {[p.name for p in contents]}\")\n",
    "            \n",
    "            # Check for specific expected files\n",
    "            expected_files = ['.devcontainer', 'pyproject.toml', 'docker-compose.yml']\n",
    "            for file in expected_files:\n",
    "                if (workspace_path / file).exists():\n",
    "                    print(f\"   ✅ Found: {file}\")\n",
    "                else:\n",
    "                    print(f\"   ❌ Missing: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error reading workspace: {e}\")\n",
    "    else:\n",
    "        print(f\"❌ /workspace directory does not exist\")\n",
    "\n",
    "\n",
    "def check_dev_container_config():\n",
    "    \"\"\"Check dev container configuration.\"\"\"\n",
    "    print(\"\\n⚙️  DEV CONTAINER CONFIG CHECK\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    config_path = Path('/workspace/.devcontainer/devcontainer.json')\n",
    "    if config_path.exists():\n",
    "        print(\"✅ devcontainer.json found\")\n",
    "        try:\n",
    "            with open(config_path) as f:\n",
    "                config = json.load(f)\n",
    "            print(f\"   Name: {config.get('name', 'Not specified')}\")\n",
    "            print(f\"   Python path: {config.get('customizations', {}).get('vscode', {}).get('settings', {}).get('python.defaultInterpreterPath', 'Not specified')}\")\n",
    "            print(f\"   Workspace folder: {config.get('workspaceFolder', 'Not specified')}\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ❌ Error reading config: {e}\")\n",
    "    else:\n",
    "        print(\"❌ devcontainer.json not found\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Run all diagnostic checks.\"\"\"\n",
    "    print(\"🔍 DEV CONTAINER COMPREHENSIVE DIAGNOSTICS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Running from: {os.getcwd()}\")\n",
    "    print(f\"User: {os.getenv('USER', 'unknown')}\")\n",
    "    print(f\"Container hostname: {os.getenv('HOSTNAME', 'unknown')}\")\n",
    "    \n",
    "    # System commands\n",
    "    run_command(\"uv --version\", \"UV Version\")\n",
    "    run_command(\"which python\", \"Python Location\")\n",
    "    run_command(\"ls -la /app/.venv/\", \"Virtual Environment Contents\")\n",
    "    run_command(\"mount | grep workspace\", \"Workspace Mount Status\")\n",
    "    run_command(\"nvidia-smi\", \"NVIDIA GPU Status\")\n",
    "    \n",
    "    # Python-based checks\n",
    "    check_paths_and_environment()\n",
    "    check_gpu_environment()\n",
    "    check_key_packages()\n",
    "    check_gpu_support()\n",
    "    check_workspace_mount()\n",
    "    check_dev_container_config()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"🎯 SUMMARY & RECOMMENDATIONS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"If you see issues:\")\n",
    "    print(\"1. ❌ Virtual env missing → Check Dockerfile uv sync step\")\n",
    "    print(\"2. ❌ Workspace not mounted → Check devcontainer.json mounts config\")\n",
    "    print(\"3. ❌ Packages missing → Check uv.lock and pip install steps\")\n",
    "    print(\"4. ⚠️  GPU not detected → Check docker-compose.yml gpu settings\")\n",
    "    print(\"5. 🔧 For VS Code issues → Check python.defaultInterpreterPath setting\")\n",
    "    print(\"6. 🎮 For GPU issues → Check NVIDIA drivers and CUDA compatibility\")\n",
    "    print(\"\\n✅ All checks passed = ready for development!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting tests/test_pytorch_jax_gpu.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile tests/test_pytorch_jax_gpu.py\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Test script to verify that PyTorch and JAX can access the GPU,\n",
    "and that PyJAGS is working correctly.\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "\n",
    "def test_pytorch_gpu():\n",
    "    \"\"\"Test PyTorch GPU availability and basic operations.\"\"\"\n",
    "    print(\"\\n=== Testing PyTorch GPU ===\")\n",
    "    try:\n",
    "        import torch\n",
    "        print(f\"PyTorch version: {torch.__version__}\")\n",
    "        print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "        \n",
    "        if not torch.cuda.is_available():\n",
    "            print(\"❌ PyTorch CUDA not available!\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
    "        print(f\"Current device: {torch.cuda.current_device()}\")\n",
    "        print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n",
    "        \n",
    "        # Run a simple test computation\n",
    "        x = torch.rand(1000, 1000).cuda()\n",
    "        y = torch.rand(1000, 1000).cuda()\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        \n",
    "        start.record()\n",
    "        z = torch.matmul(x, y)\n",
    "        end.record()\n",
    "        \n",
    "        # Wait for GPU computation to finish\n",
    "        torch.cuda.synchronize()\n",
    "        print(f\"Matrix multiplication time: {start.elapsed_time(end):.2f} ms\")\n",
    "        print(f\"Result shape: {z.shape}\")\n",
    "        print(\"✅ PyTorch GPU test passed!\")\n",
    "        return True\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"❌ PyTorch not found!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during PyTorch GPU test: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_jax_gpu():\n",
    "    \"\"\"Test JAX GPU availability and basic operations.\"\"\"\n",
    "    print(\"\\n=== Testing JAX GPU ===\")\n",
    "    try:\n",
    "        import jax\n",
    "        import jax.numpy as jnp\n",
    "        \n",
    "        print(f\"JAX version: {jax.__version__}\")\n",
    "        \n",
    "        # Force GPU platform\n",
    "        jax.config.update('jax_platform_name', 'gpu')\n",
    "        \n",
    "        # Get device count and details\n",
    "        devices = jax.devices()\n",
    "        device_count = len(devices)\n",
    "        print(f\"Available devices: {device_count}\")\n",
    "        \n",
    "        for i, device in enumerate(devices):\n",
    "            print(f\"Device {i}: {device}\")\n",
    "        \n",
    "        if device_count == 0 or 'cuda' not in str(devices[0]).lower():\n",
    "            print(\"❌ No GPU devices found by JAX!\")\n",
    "            return False\n",
    "        \n",
    "        # Check CUDA configuration\n",
    "        jit_info = jax.config.values\n",
    "        print(f\"JAX configuration: {jit_info}\")\n",
    "        \n",
    "        # Run a simple GPU computation\n",
    "        print(\"Running a test computation on GPU...\")\n",
    "        try:\n",
    "            x = jnp.ones((1000, 1000))\n",
    "            y = jnp.ones((1000, 1000))\n",
    "            \n",
    "            # Use JIT compilation for better performance\n",
    "            @jax.jit\n",
    "            def matmul(a, b):\n",
    "                return jnp.matmul(a, b)\n",
    "            \n",
    "            result = matmul(x, y)\n",
    "            print(f\"Result shape: {result.shape}\")\n",
    "            \n",
    "            print(\"✅ JAX GPU test passed!\")\n",
    "            return True\n",
    "        except RuntimeError as e:\n",
    "            if \"ptxas too old\" in str(e):\n",
    "                print(f\"⚠️ JAX GPU detected but CUDA compatibility issue: {e}\")\n",
    "                print(\"⚠️ JAX can see the GPU but there's a CUDA version compatibility issue.\")\n",
    "                print(\"⚠️ This is considered a partial success since the GPU is detected.\")\n",
    "                return True\n",
    "            else:\n",
    "                raise\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"❌ JAX not found!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during JAX GPU test: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "def test_pyjags():\n",
    "    \"\"\"Test PyJAGS installation and basic functionality.\"\"\"\n",
    "    print(\"\\n=== Testing PyJAGS ===\")\n",
    "    try:\n",
    "        import pyjags\n",
    "        print(f\"PyJAGS version: {pyjags.__version__}\")\n",
    "        \n",
    "        # Create a simple model to verify that PyJAGS works\n",
    "        code = \"\"\"\n",
    "        model {\n",
    "            # Likelihood\n",
    "            y ~ dnorm(mu, 1/sigma^2)\n",
    "            \n",
    "            # Priors\n",
    "            mu ~ dnorm(0, 0.001)\n",
    "            sigma ~ dunif(0, 100)\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        # Sample data\n",
    "        data = {'y': 0.5}\n",
    "        \n",
    "        # Initialize model with data\n",
    "        model = pyjags.Model(code, data=data, chains=1, adapt=100)\n",
    "        print(\"JAGS model initialized successfully!\")\n",
    "        \n",
    "        # Sample from the model\n",
    "        samples = model.sample(200, vars=['mu', 'sigma'])\n",
    "        print(\"JAGS sampling completed successfully!\")\n",
    "        \n",
    "        # Verify the samples\n",
    "        mu_samples = samples['mu']\n",
    "        sigma_samples = samples['sigma']\n",
    "        print(f\"mu mean: {mu_samples.mean():.4f}\")\n",
    "        print(f\"sigma mean: {sigma_samples.mean():.4f}\")\n",
    "        \n",
    "        print(\"✅ PyJAGS test passed!\")\n",
    "        return True\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"❌ PyJAGS not found!\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error during PyJAGS test: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Running GPU and PyJAGS verification tests...\")\n",
    "    \n",
    "    pytorch_success = test_pytorch_gpu()\n",
    "    jax_success = test_jax_gpu()\n",
    "    pyjags_success = test_pyjags()\n",
    "    \n",
    "    print(\"\\n=== Test Summary ===\")\n",
    "    print(f\"PyTorch GPU: {'✅ PASS' if pytorch_success else '❌ FAIL'}\")\n",
    "    print(f\"JAX GPU: {'✅ PASS' if jax_success else '❌ FAIL'}\")\n",
    "    print(f\"PyJAGS: {'✅ PASS' if pyjags_success else '❌ FAIL'}\")\n",
    "    \n",
    "    if pytorch_success and jax_success and pyjags_success:\n",
    "        print(\"\\n🎉 All tests passed! The container is working correctly.\")\n",
    "        sys.exit(0)\n",
    "    else:\n",
    "        print(\"\\n❌ Some tests failed. Please check the output for details.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
