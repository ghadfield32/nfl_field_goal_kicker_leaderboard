{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/mlops/config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/mlops/config.py\n",
        "\"\"\"Central MLflow configuration for consistent experiment tracking.\"\"\"\n",
        "import os\n",
        "\n",
        "# ─── MLflow configuration ──────────────────────────────────────────────────\n",
        "# Use Docker service-name so this works inside the compose network\n",
        "# Falls back to local file store for standalone usage\n",
        "TRACKING_URI = os.getenv(\"MLFLOW_TRACKING_URI\", \"http://mlflow:5000\")\n",
        "EXPERIMENT_NAME = \"iris_classification\"\n",
        "ARTIFACT_ROOT = os.getenv(\"MLFLOW_ARTIFACT_ROOT\", \"./mlruns\")\n",
        "\n",
        "# ─── Model registry ────────────────────────────────────────────────────────\n",
        "MODEL_NAME = \"iris_classifier\"\n",
        "MODEL_STAGE_PRODUCTION = \"Production\"\n",
        "MODEL_STAGE_STAGING = \"Staging\"\n",
        "\n",
        "# ─── Dataset defaults ──────────────────────────────────────────────────────\n",
        "RANDOM_STATE = 42\n",
        "TEST_SIZE = 0.2 \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/mlops/logging.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/mlops/logging.py\n",
        "\"\"\"\n",
        "Extended MLflow logging helpers.\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "import mlflow\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from typing import Dict, Any, Sequence, Optional\n",
        "from matplotlib.figure import Figure\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_recall_fscore_support,\n",
        "    classification_report,\n",
        "    confusion_matrix,\n",
        "    roc_auc_score,\n",
        "    log_loss,\n",
        "    matthews_corrcoef,\n",
        ")\n",
        "\n",
        "\n",
        "def _log_fig(fig: Figure, name: str) -> None:\n",
        "    \"\"\"Log a Matplotlib figure directly without temp files.\"\"\"\n",
        "    mlflow.log_figure(fig, artifact_file=name)\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "def log_full_metrics(\n",
        "    y_true, y_pred, *, label_list: Optional[Sequence[int]] = None, prefix: str = \"\"\n",
        ") -> Dict[str, float]:\n",
        "    \"\"\"\n",
        "    Compute & log *all* useful classification metrics.\n",
        "\n",
        "    Returns a flat dict so callers can unit-test easily.\n",
        "    \n",
        "    Args:\n",
        "        y_true: True labels\n",
        "        y_pred: Predicted labels  \n",
        "        label_list: Optional list of label integers (for compatibility)\n",
        "        prefix: Optional prefix for metric names\n",
        "        \n",
        "    Returns:\n",
        "        Dictionary of all calculated metrics\n",
        "    \"\"\"\n",
        "    # (1) macro metrics ------------------------------------------------------\n",
        "    macro = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=\"macro\", zero_division=\"warn\"\n",
        "    )\n",
        "    metrics: Dict[str, float] = {\n",
        "        \"accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"precision_macro\": float(macro[0]),\n",
        "        \"recall_macro\": float(macro[1]),\n",
        "        \"f1_macro\": float(macro[2]),\n",
        "    }\n",
        "\n",
        "    # (2) per-class ----------------------------------------------------------\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=\"warn\")\n",
        "    if isinstance(report, dict):\n",
        "        for klass, d in report.items():\n",
        "            if isinstance(klass, str) and klass.isdigit():  # skip 'accuracy', 'macro avg', …\n",
        "                k = int(klass)\n",
        "                if isinstance(d, dict):\n",
        "                    precision_val = d.get(\"precision\", 0.0)\n",
        "                    recall_val = d.get(\"recall\", 0.0)\n",
        "                    f1_val = d.get(\"f1-score\", 0.0)\n",
        "                    support_val = d.get(\"support\", 0.0)\n",
        "                    \n",
        "                    metrics[f\"precision_{k}\"] = float(precision_val) if precision_val is not None else 0.0\n",
        "                    metrics[f\"recall_{k}\"] = float(recall_val) if recall_val is not None else 0.0\n",
        "                    metrics[f\"f1_{k}\"] = float(f1_val) if f1_val is not None else 0.0\n",
        "                    metrics[f\"support_{k}\"] = float(support_val) if support_val is not None else 0.0\n",
        "\n",
        "    # (3) derived – try/except so we never crash ----------------------------\n",
        "    try:\n",
        "        metrics[\"roc_auc_ovr_weighted\"] = roc_auc_score(\n",
        "            y_true, pd.get_dummies(y_pred), multi_class=\"ovr\", average=\"weighted\"\n",
        "        )\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        metrics[\"log_loss\"] = log_loss(y_true, pd.get_dummies(y_pred))\n",
        "    except Exception:\n",
        "        pass\n",
        "    try:\n",
        "        metrics[\"mcc\"] = matthews_corrcoef(y_true, y_pred)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # (4) optional prefix for nested CV, etc. -------------------------------\n",
        "    if prefix:\n",
        "        metrics = {f\"{prefix}_{k}\": v for k, v in metrics.items()}\n",
        "\n",
        "    mlflow.log_metrics(metrics)\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def log_confusion_matrix(\n",
        "    y_true, y_pred, *, class_names: Optional[Sequence[str]] = None, artifact_name: str = \"confusion_matrix.png\"\n",
        ") -> None:\n",
        "    \"\"\"Create + log confusion matrix using mlflow.log_figure.\"\"\"\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt=\"d\",\n",
        "        cmap=\"Blues\",\n",
        "        xticklabels=class_names if class_names is not None else \"auto\",\n",
        "        yticklabels=class_names if class_names is not None else \"auto\",\n",
        "        ax=ax,\n",
        "    )\n",
        "    ax.set_xlabel(\"Predicted\")\n",
        "    ax.set_ylabel(\"Actual\")\n",
        "    ax.set_title(\"Confusion Matrix\")\n",
        "    _log_fig(fig, artifact_name)\n",
        "\n",
        "\n",
        "def log_feature_importance(\n",
        "    feature_names: list, importances: list, artifact_name: str = \"feature_importance.png\"\n",
        "):\n",
        "    \"\"\"Bar plot logged via mlflow.log_figure (no disk I/O).\"\"\"\n",
        "    imp_df = (\n",
        "        pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
        "        .sort_values(\"importance\")\n",
        "    )\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.barplot(data=imp_df, x=\"importance\", y=\"feature\", ax=ax)\n",
        "    ax.set_title(\"Feature Importances\")\n",
        "    _log_fig(fig, artifact_name)\n",
        "\n",
        "\n",
        "def log_parameters(params: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Log parameters to MLflow.\n",
        "    \n",
        "    Args:\n",
        "        params: Dictionary of parameter names and values\n",
        "    \"\"\"\n",
        "    mlflow.log_params(params)\n",
        "\n",
        "\n",
        "def log_dataset_info(X_train, X_test, y_train, y_test) -> None:\n",
        "    \"\"\"\n",
        "    Log dataset information as parameters.\n",
        "    \n",
        "    Args:\n",
        "        X_train: Training features\n",
        "        X_test: Test features\n",
        "        y_train: Training labels\n",
        "        y_test: Test labels\n",
        "    \"\"\"\n",
        "    dataset_params = {\n",
        "        \"train_size\": len(X_train),\n",
        "        \"test_size\": len(X_test),\n",
        "        \"n_features\": (X_train.shape[1] if hasattr(X_train, \"shape\") else len(X_train[0])),\n",
        "        \"n_classes\": (len(set(y_train)) if hasattr(y_train, \"__iter__\") else 1),\n",
        "    }\n",
        "\n",
        "    log_parameters(dataset_params)\n",
        "\n",
        "\n",
        "# Legacy compatibility - keep old function name as alias\n",
        "log_model_metrics = log_full_metrics "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/mlops/experiment.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/mlops/experiment.py\n",
        "\n",
        "\"\"\"Training utilities with MLflow integration.\"\"\"\n",
        "import mlflow\n",
        "import optuna\n",
        "from typing import Optional\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from .config import RANDOM_STATE, TEST_SIZE\n",
        "from .experiment_utils import setup_mlflow_experiment\n",
        "\n",
        "# Re-export for convenience\n",
        "__all__ = ['setup_mlflow_experiment', 'load_and_prepare_iris_data',\n",
        "           'train_logistic_regression', 'train_random_forest_with_optimization']\n",
        "from .logging import (\n",
        "    log_model_metrics,\n",
        "    log_confusion_matrix,\n",
        "    log_feature_importance,\n",
        "    log_dataset_info,\n",
        "    log_parameters\n",
        ")\n",
        "\n",
        "\n",
        "# ─────────────────────────── src/mlops/training.py (excerpt) ───────────────\n",
        "def load_and_prepare_iris_data(\n",
        "    test_size: float = TEST_SIZE,\n",
        "    random_state: int = RANDOM_STATE\n",
        ") -> DatasetTuple:\n",
        "    iris = load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=test_size, random_state=random_state\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_test_scaled  = scaler.transform(X_test)\n",
        "\n",
        "    # ✅ re-wrap as DataFrame so feature names propagate downstream\n",
        "    import pandas as pd\n",
        "    feat_names = iris.feature_names\n",
        "    X_train_df = pd.DataFrame(X_train_scaled, columns=feat_names)\n",
        "    X_test_df  = pd.DataFrame(X_test_scaled,  columns=feat_names)\n",
        "\n",
        "    return (X_train_df, X_test_df, y_train, y_test,\n",
        "            feat_names, list(iris.target_names), scaler)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/mlops/model_registry.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/mlops/model_registry.py\n",
        "\"\"\"MLflow model registry utilities.\"\"\"\n",
        "import mlflow\n",
        "from typing import Optional, Dict, Any\n",
        "from .config import MODEL_NAME, MODEL_STAGE_PRODUCTION\n",
        "import re\n",
        "\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "_ALLOWED = re.compile(\"[^0-9A-Za-z_-]\")\n",
        "\n",
        "def sanitize_model_name(name: str) -> str:\n",
        "    \"\"\"\n",
        "    Sanitize model name to comply with MLflow's naming restrictions.\n",
        "    Only allows alphanumeric characters, dashes, and underscores.\n",
        "    \"\"\"\n",
        "    return re.sub(r'[^A-Za-z0-9_-]', '_', name)\n",
        "\n",
        "\n",
        "def make_model_name(model_type: str, metric_name: str, metric_value: float) -> str:\n",
        "    \"\"\"\n",
        "    Create a standardized model name from type and metric.\n",
        "    Example: 'rf_accuracy_99_93' for a Random Forest with 99.93% accuracy\n",
        "    \"\"\"\n",
        "    # Convert float to string with 2 decimal places and remove the dot\n",
        "    metric_str = f\"{metric_value:.2f}\".replace('.', '_')\n",
        "    name = f\"{model_type}_{metric_name}_{metric_str}\"\n",
        "    return sanitize_model_name(name)\n",
        "\n",
        "\n",
        "def register_model(\n",
        "    model_uri: str,\n",
        "    model_name: str,\n",
        "    description: Optional[str] = None,\n",
        "    overwrite: bool = True,\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Register *model_uri* under *model_name*.\n",
        "    If a registered model with that exact name already exists and\n",
        "    ``overwrite is True`` it is deleted (all versions) before re-creation.\n",
        "    \n",
        "    Args:\n",
        "        model_uri: URI of the model to register (e.g. 'runs:/run_id/model')\n",
        "        model_name: Name to register the model under\n",
        "        description: Optional description for the model version\n",
        "        overwrite: If True, delete existing model with same name before registering\n",
        "    \n",
        "    Returns:\n",
        "        Version number of the newly registered model as a string\n",
        "    \"\"\"\n",
        "    client = MlflowClient()\n",
        "    model_name = sanitize_model_name(model_name)\n",
        "\n",
        "    if overwrite:\n",
        "        try:                          # -- delete whole entry if present\n",
        "            client.delete_registered_model(model_name)\n",
        "            print(f\"🗑️  Removed previous '{model_name}'\")\n",
        "        except Exception:\n",
        "            pass                      # not present → nothing to delete\n",
        "\n",
        "    client.create_registered_model(model_name)\n",
        "    mv = client.create_model_version(\n",
        "        name=model_name,\n",
        "        source=model_uri,\n",
        "        description=description,\n",
        "    )\n",
        "    print(f\"✅ Registered {model_name} v{mv.version}\")\n",
        "    return mv.version\n",
        "\n",
        "\n",
        "def promote_model_to_stage(model_name: Optional[str] = None,\n",
        "                           version: Optional[str] = None,\n",
        "                           stage: str = MODEL_STAGE_PRODUCTION) -> None:\n",
        "    \"\"\"\n",
        "    Promote a model version to a specific stage using the fluent client.\n",
        "    \n",
        "    Args:\n",
        "        model_name: Name of the registered model\n",
        "        version: Version to promote (if None, promotes latest)\n",
        "        stage: Target stage\n",
        "    \"\"\"\n",
        "    name = model_name or MODEL_NAME\n",
        "    client = mlflow.tracking.MlflowClient()\n",
        "    \n",
        "    try:\n",
        "        # Get latest version if not specified\n",
        "        if version is None:\n",
        "            latest = client.get_latest_versions(name, stages=[\"None\"])\n",
        "            if not latest:\n",
        "                raise ValueError(f\"No versions found for model {name}\")\n",
        "            version = latest[0].version\n",
        "        \n",
        "        # Transition to stage\n",
        "        client.transition_model_version_stage(\n",
        "            name=name,\n",
        "            version=version,\n",
        "            stage=stage\n",
        "        )\n",
        "        print(f\"Promoted model {name} version {version} to {stage}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Failed to promote model: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def load_model_from_registry(model_name: Optional[str] = None,\n",
        "                             stage: str = MODEL_STAGE_PRODUCTION):\n",
        "    \"\"\"\n",
        "    Load a model from the registry by name and stage.\n",
        "    \n",
        "    Args:\n",
        "        model_name: Name of the registered model\n",
        "        stage: Stage to load from\n",
        "        \n",
        "    Returns:\n",
        "        Loaded model\n",
        "    \"\"\"\n",
        "    name = model_name or MODEL_NAME\n",
        "    model_uri = f\"models:/{name}/{stage}\"\n",
        "    \n",
        "    try:\n",
        "        model = mlflow.sklearn.load_model(model_uri)\n",
        "        print(f\"Loaded model {name} from {stage} stage\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load model from registry: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def load_model_from_run(run_id: str, artifact_path: str = \"model\"):\n",
        "    \"\"\"\n",
        "    Load a model from a specific run.\n",
        "    \n",
        "    Args:\n",
        "        run_id: MLflow run ID\n",
        "        artifact_path: Path to the model artifact\n",
        "        \n",
        "    Returns:\n",
        "        Loaded model\n",
        "    \"\"\"\n",
        "    model_uri = f\"runs:/{run_id}/{artifact_path}\"\n",
        "    \n",
        "    try:\n",
        "        model = mlflow.sklearn.load_model(model_uri)\n",
        "        print(f\"Loaded model from run {run_id}\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to load model from run: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def get_model_info(model_name: Optional[str] = None,\n",
        "                   stage: str = MODEL_STAGE_PRODUCTION) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Get information about a registered model using the fluent client.\n",
        "    \n",
        "    Args:\n",
        "        model_name: Name of the registered model\n",
        "        stage: Stage to get info for\n",
        "        \n",
        "    Returns:\n",
        "        Model information dictionary\n",
        "    \"\"\"\n",
        "    name = model_name or MODEL_NAME\n",
        "    client = mlflow.tracking.MlflowClient()\n",
        "    \n",
        "    try:\n",
        "        model_version = client.get_latest_versions(name, stages=[stage])[0]\n",
        "        \n",
        "        return {\n",
        "            \"name\": model_version.name,\n",
        "            \"version\": model_version.version,\n",
        "            \"stage\": model_version.current_stage,\n",
        "            \"description\": model_version.description,\n",
        "            \"creation_timestamp\": model_version.creation_timestamp,\n",
        "            \"last_updated_timestamp\": model_version.last_updated_timestamp,\n",
        "            \"run_id\": model_version.run_id\n",
        "        }\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to get model info: {e}\")\n",
        "        raise \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/mlops/shapiq_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/mlops/shapiq_utils.py\n",
        "\"\"\"\n",
        "SHAP-IQ (Shapley Interaction) utilities for MLflow integration.\n",
        "\n",
        "This module provides functions to compute and log Shapley interaction values\n",
        "for machine learning models. Shapley interactions help understand how features\n",
        "work together to influence model predictions.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import mlflow\n",
        "from shapiq import TabularExplainer\n",
        "from typing import Optional, Sequence, Union\n",
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def compute_shapiq_interactions(\n",
        "    model,\n",
        "    X: pd.DataFrame,\n",
        "    feature_names: Sequence[str],\n",
        "    max_order: int = 2,\n",
        "    budget: int = 256,\n",
        "    n_samples: Optional[int] = None,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Robust wrapper around shapiq.TabularExplainer to return a tidy DataFrame\n",
        "    with Shapley-interaction values.  Handles the two public APIs:\n",
        "      •  .dict_values   (mapping)\n",
        "      •  .values        (np.ndarray)  →  use  .to_dict()\n",
        "    \"\"\"\n",
        "    logger.info(\n",
        "        \"Computing SHAP-IQ (max_order=%s, budget=%s, n_samples=%s)\",\n",
        "        max_order,\n",
        "        budget,\n",
        "        n_samples,\n",
        "    )\n",
        "\n",
        "    X_sample = (\n",
        "        X.sample(n=n_samples, random_state=42) if n_samples and len(X) > n_samples else X\n",
        "    )\n",
        "\n",
        "    explainer = TabularExplainer(\n",
        "        model=model,\n",
        "        data=X_sample.values,\n",
        "        index=\"k-SII\",\n",
        "        max_order=max_order,\n",
        "    )\n",
        "\n",
        "    rows: list[dict[str, Any]] = []\n",
        "    for i, vec in enumerate(X_sample.values):\n",
        "        try:\n",
        "            iv = explainer.explain(vec, budget=budget)\n",
        "\n",
        "            # --- unify both APIs ------------------------------------------------\n",
        "            if hasattr(iv, \"dict_values\"):                    # shapiq ≥ 0.4\n",
        "                items = iv.dict_values.items()\n",
        "            elif hasattr(iv, \"to_dict\"):                      # fallback\n",
        "                items = iv.to_dict().items()\n",
        "            else:\n",
        "                # last resort – try attribute access\n",
        "                items = dict(iv.values).items()\n",
        "\n",
        "            for combo, val in items:\n",
        "                rows.append(\n",
        "                    {\n",
        "                        \"sample_idx\": i,\n",
        "                        \"combination\": combo,\n",
        "                        \"value\": float(val),\n",
        "                        \"order\": len(combo),\n",
        "                        \"feature_names\": tuple(feature_names[j] for j in combo)\n",
        "                        if combo\n",
        "                        else (),\n",
        "                    }\n",
        "                )\n",
        "        except Exception as exc:  # noqa: BLE001\n",
        "            logger.warning(\"SHAP-IQ failed on sample %s: %s\", i, exc)\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    logger.info(\"✓ %s interaction rows computed\", len(df))\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def log_shapiq_interactions(\n",
        "    model,\n",
        "    X: pd.DataFrame,\n",
        "    feature_names: Sequence[str],\n",
        "    max_order: int = 2,\n",
        "    top_n: int = 10,\n",
        "    budget: int = 256,\n",
        "    n_samples: Optional[int] = None,\n",
        "    output_path: Optional[str] = None\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Compute Shapley interaction values and log them to MLflow.\n",
        "\n",
        "    This function:\n",
        "    1. Computes interactions using compute_shapiq_interactions\n",
        "    2. Logs the top N interactions as MLflow metrics\n",
        "    3. Saves the full interaction table as CSV and logs as artifact\n",
        "\n",
        "    Args:\n",
        "        model: Trained sklearn-like model.\n",
        "        X: DataFrame of features.\n",
        "        feature_names: List of feature column names.\n",
        "        max_order: Maximum interaction order (default: 2).\n",
        "        top_n: Number of top interactions to log as metrics (default: 10).\n",
        "        budget: Evaluation budget for interaction approximation (default: 256).\n",
        "        n_samples: If provided, sample this many rows for computation.\n",
        "        output_path: Optional path for CSV output (default: \"shapiq_interactions.csv\").\n",
        "    \"\"\"\n",
        "    logger.info(\"Starting SHAP-IQ interaction logging\")\n",
        "    \n",
        "    # Compute interactions\n",
        "    df = compute_shapiq_interactions(\n",
        "        model, X, feature_names, max_order, budget, n_samples\n",
        "    )\n",
        "    \n",
        "    if df.empty:\n",
        "        logger.warning(\"No interactions computed - skipping logging\")\n",
        "        return\n",
        "    \n",
        "    # Aggregate: mean absolute value per combination across all samples\n",
        "    agg = (\n",
        "        df.groupby(['combination', 'feature_names', 'order'])['value']\n",
        "          .apply(lambda x: x.abs().mean())\n",
        "          .reset_index()\n",
        "          .sort_values('value', ascending=False)\n",
        "    )\n",
        "    \n",
        "    # Log summary statistics\n",
        "    mlflow.log_metric(\"shapiq_total_interactions\", len(df))\n",
        "    mlflow.log_metric(\"shapiq_unique_combinations\", len(agg))\n",
        "    mlflow.log_metric(\"shapiq_max_order\", max_order)\n",
        "    mlflow.log_metric(\"shapiq_samples_analyzed\", len(X) if n_samples is None else min(n_samples, len(X)))\n",
        "    \n",
        "    # Log top N interactions as metrics\n",
        "    logger.info(f\"Logging top {top_n} interactions as MLflow metrics\")\n",
        "    for idx, row in agg.head(top_n).iterrows():\n",
        "        combo = row['combination']\n",
        "        feature_combo = row['feature_names'] \n",
        "        value = row['value']\n",
        "        order = row['order']\n",
        "        \n",
        "        # Create metric name from feature names or indices\n",
        "        if feature_combo:\n",
        "            name = f\"shapiq_order{order}_{'_x_'.join(feature_combo)}\"\n",
        "        else:\n",
        "            name = f\"shapiq_order{order}_{'_'.join(map(str, combo))}\"\n",
        "        \n",
        "        # Sanitize metric name (MLflow has restrictions)\n",
        "        name = name.replace(' ', '_').replace('(', '').replace(')', '').replace(',', '_')[:250]\n",
        "        \n",
        "        mlflow.log_metric(name, float(value))\n",
        "    \n",
        "    # Log order-specific summaries\n",
        "    order_summary = df.groupby('order')['value'].agg(['count', 'mean', 'std']).fillna(0)\n",
        "    for order_val in order_summary.index:\n",
        "        mlflow.log_metric(f\"shapiq_order{order_val}_count\", order_summary.loc[order_val, 'count'])\n",
        "        mlflow.log_metric(f\"shapiq_order{order_val}_mean_abs\", abs(order_summary.loc[order_val, 'mean']))\n",
        "        if order_summary.loc[order_val, 'std'] > 0:\n",
        "            mlflow.log_metric(f\"shapiq_order{order_val}_std\", order_summary.loc[order_val, 'std'])\n",
        "    \n",
        "    # Save and log full DataFrame as artifact\n",
        "    output_file = output_path or \"shapiq_interactions.csv\"\n",
        "    \n",
        "    try:\n",
        "        # Add readable feature names to the full DataFrame\n",
        "        df_export = df.copy()\n",
        "        df_export['feature_names_str'] = df_export['feature_names'].apply(lambda x: ' x '.join(x) if x else 'baseline')\n",
        "        \n",
        "        df_export.to_csv(output_file, index=False)\n",
        "        mlflow.log_artifact(output_file)\n",
        "        logger.info(f\"Logged SHAP-IQ interactions artifact: {output_file}\")\n",
        "        \n",
        "        # Also create and log a summary file\n",
        "        summary_file = output_path.replace('.csv', '_summary.csv') if output_path else \"shapiq_interactions_summary.csv\"\n",
        "        agg_export = agg.copy()\n",
        "        agg_export['feature_names_str'] = agg_export['feature_names'].apply(lambda x: ' x '.join(x) if x else 'baseline')\n",
        "        agg_export.to_csv(summary_file, index=False)\n",
        "        mlflow.log_artifact(summary_file)\n",
        "        logger.info(f\"Logged SHAP-IQ summary artifact: {summary_file}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error saving SHAP-IQ artifacts: {e}\")\n",
        "    \n",
        "    logger.info(\"SHAP-IQ interaction logging completed\")\n",
        "\n",
        "\n",
        "def get_top_interactions(\n",
        "    shapiq_df: pd.DataFrame,\n",
        "    top_n: int = 10,\n",
        "    order: Optional[int] = None\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extract top interactions from a SHAP-IQ DataFrame.\n",
        "    \n",
        "    Args:\n",
        "        shapiq_df: DataFrame returned by compute_shapiq_interactions.\n",
        "        top_n: Number of top interactions to return.\n",
        "        order: If provided, filter to interactions of this order only.\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with top interactions, aggregated across samples.\n",
        "    \"\"\"\n",
        "    df = shapiq_df.copy()\n",
        "    \n",
        "    if order is not None:\n",
        "        df = df[df['order'] == order]\n",
        "    \n",
        "    if df.empty:\n",
        "        return df\n",
        "    \n",
        "    # Aggregate and sort by absolute mean value\n",
        "    agg = (\n",
        "        df.groupby(['combination', 'feature_names', 'order'])['value']\n",
        "          .agg(['mean', 'std', 'count'])\n",
        "          .reset_index()\n",
        "    )\n",
        "    agg['abs_mean'] = agg['mean'].abs()\n",
        "    agg = agg.sort_values('abs_mean', ascending=False)\n",
        "    \n",
        "    return agg.head(top_n)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/mlops/training.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/mlops/training.py\n",
        "\"\"\"Training utilities with MLflow integration.\"\"\"\n",
        "import mlflow\n",
        "from mlflow import sklearn  # type: ignore\n",
        "from mlflow import models  # type: ignore\n",
        "import optuna\n",
        "from optuna.integration.mlflow import MLflowCallback\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from typing import Optional, Tuple, List, Callable, cast, Any, Dict, TypeAlias\n",
        "from numpy.typing import NDArray\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils import Bunch\n",
        "\n",
        "from .config import RANDOM_STATE, TEST_SIZE\n",
        "from .experiment_utils import setup_mlflow_experiment\n",
        "from .logging import (\n",
        "    log_full_metrics,\n",
        "    log_confusion_matrix,\n",
        "    log_feature_importance,\n",
        "    log_dataset_info,\n",
        "    log_parameters\n",
        ")\n",
        "from .shapiq_utils import log_shapiq_interactions\n",
        "\n",
        "from src.mlops.cleanup import prune_experiment, prune_model_versions\n",
        "\n",
        "# Type aliases for complex types\n",
        "FloatArray: TypeAlias = NDArray[np.float64]\n",
        "IntArray: TypeAlias = NDArray[np.int64]\n",
        "DatasetTuple: TypeAlias = Tuple[FloatArray, FloatArray, IntArray, IntArray, List[str], List[str], StandardScaler]\n",
        "\n",
        "\n",
        "def load_and_prepare_iris_data(\n",
        "    test_size: float = TEST_SIZE,\n",
        "    random_state: int = RANDOM_STATE\n",
        ") -> DatasetTuple:\n",
        "    \"\"\"\n",
        "    Load and prepare the Iris dataset.\n",
        "    \n",
        "    Args:\n",
        "        test_size: Fraction of data to use for testing\n",
        "        random_state: Random state for reproducibility\n",
        "        \n",
        "    Returns:\n",
        "        Tuple of (X_train_scaled, X_test_scaled, y_train, y_test, \n",
        "                 feature_names, target_names, scaler)\n",
        "    \"\"\"\n",
        "    # Load dataset\n",
        "    iris: Any = load_iris()\n",
        "    X: NDArray[np.float64] = cast(NDArray[np.float64], iris.data)\n",
        "    y: NDArray[np.int64] = cast(NDArray[np.int64], iris.target)\n",
        "    feature_names: List[str] = list(iris.feature_names)\n",
        "    target_names: List[str] = list(iris.target_names)\n",
        "    \n",
        "    # Split data\n",
        "    X_train: NDArray[np.float64]\n",
        "    X_test: NDArray[np.float64]\n",
        "    y_train: NDArray[np.int64]\n",
        "    y_test: NDArray[np.int64]\n",
        "    X_train, X_test, y_train, y_test = cast(\n",
        "        Tuple[NDArray[np.float64], NDArray[np.float64], NDArray[np.int64], NDArray[np.int64]],\n",
        "        train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
        "    )\n",
        "    \n",
        "    # Scale features\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled: NDArray[np.float64] = cast(NDArray[np.float64], scaler.fit_transform(X_train))\n",
        "    X_test_scaled: NDArray[np.float64] = cast(NDArray[np.float64], scaler.transform(X_test))\n",
        "    \n",
        "    return (X_train_scaled, X_test_scaled, y_train, y_test,\n",
        "            feature_names, target_names, scaler)\n",
        "\n",
        "\n",
        "# === (A) LOGISTIC REGRESSION (training only, NO dashboard) ================\n",
        "def train_logistic_regression(\n",
        "    X_train, y_train, X_test, y_test, feature_names, target_names,\n",
        "    *, run_name: str = \"lr_baseline\", register: bool = True\n",
        ") -> str:\n",
        "    \"\"\"Train logistic regression model without dashboard integration.\"\"\"\n",
        "    setup_mlflow_experiment()\n",
        "    mlflow.sklearn.autolog(log_models=True)\n",
        "    \n",
        "    with mlflow.start_run(run_name=run_name) as run:\n",
        "        model = LogisticRegression(random_state=RANDOM_STATE, max_iter=1_000).fit(\n",
        "            X_train, y_train\n",
        "        )\n",
        "\n",
        "        y_pred = model.predict(X_test)\n",
        "        log_full_metrics(y_test, y_pred)\n",
        "        log_confusion_matrix(y_test, y_pred, class_names=target_names)\n",
        "\n",
        "        signature = mlflow.models.infer_signature(X_train, model.predict(X_train))\n",
        "        sklearn.log_model(\n",
        "            model, \"model\",\n",
        "            registered_model_name=\"iris_logreg\" if register else None,\n",
        "            signature=signature, input_example=X_test[:5],\n",
        "        )\n",
        "        \n",
        "        # SHAP-IQ: compute & log feature interaction values\n",
        "        X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "        log_shapiq_interactions(model, X_test_df, feature_names, max_order=2)\n",
        "        \n",
        "        return run.info.run_id\n",
        "\n",
        "\n",
        "def _create_rf_objective(X_train, y_train, X_test, y_test) -> Callable[[optuna.trial.Trial], float]:\n",
        "    \"\"\"Create Optuna objective function for Random Forest optimization.\"\"\"\n",
        "    def objective(trial: optuna.trial.Trial) -> float:\n",
        "        params = {\n",
        "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 10, 200),\n",
        "            \"max_depth\": trial.suggest_int(\"max_depth\", 2, 20),\n",
        "            \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
        "            \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10),\n",
        "            \"random_state\": RANDOM_STATE,\n",
        "        }\n",
        "        m = RandomForestClassifier(**params).fit(X_train, y_train)\n",
        "        return float(accuracy_score(y_test, m.predict(X_test)))\n",
        "    return objective\n",
        "\n",
        "\n",
        "# === (B) RANDOM-FOREST + Optuna (training only) ===========================\n",
        "def train_random_forest_optimized(\n",
        "    X_train, y_train, X_test, y_test, feature_names, target_names,\n",
        "    *, n_trials: int = 50, run_name: str = \"rf_optimized\", register: bool = True\n",
        ") -> str:\n",
        "    \"\"\"Train optimized Random Forest model without dashboard integration.\"\"\"\n",
        "    setup_mlflow_experiment()\n",
        "    mlflow.sklearn.autolog(disable=True)        # Optuna will log\n",
        "\n",
        "    with mlflow.start_run(run_name=run_name) as run:\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(_create_rf_objective(X_train, y_train, X_test, y_test), n_trials=n_trials,\n",
        "                       callbacks=[MLflowCallback(\n",
        "                           tracking_uri=mlflow.get_tracking_uri(),\n",
        "                           metric_name=\"accuracy\", mlflow_kwargs={\"nested\": True}\n",
        "                       )])\n",
        "\n",
        "        best = RandomForestClassifier(**study.best_params).fit(X_train, y_train)\n",
        "        y_pred = best.predict(X_test)\n",
        "        log_full_metrics(y_test, y_pred)\n",
        "        log_confusion_matrix(y_test, y_pred, class_names=target_names)\n",
        "        log_feature_importance(feature_names, best.feature_importances_)\n",
        "        mlflow.log_metric(\"best_accuracy\", study.best_value)\n",
        "\n",
        "        signature = mlflow.models.infer_signature(X_train, best.predict(X_train))\n",
        "        sklearn.log_model(\n",
        "            best, \"model\",\n",
        "            registered_model_name=\"iris_random_forest\" if register else None,\n",
        "            signature=signature, input_example=X_test[:5],\n",
        "        )\n",
        "        \n",
        "        # SHAP-IQ: compute & log feature interaction values\n",
        "        X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "        log_shapiq_interactions(best, X_test_df, feature_names, max_order=2)\n",
        "        \n",
        "        return run.info.run_id\n",
        "\n",
        "\n",
        "# === (C) ONE-STOP helper: train both models ===============================\n",
        "def run_all_trainings(*,\n",
        "    test_size: float = TEST_SIZE, random_state: int = RANDOM_STATE, n_trials: int = 50) -> None:\n",
        "    \"\"\"Train both logistic regression and random forest models.\"\"\"\n",
        "    X_tr, X_te, y_tr, y_te, feats, tgts, _ = load_and_prepare_iris_data(\n",
        "        test_size, random_state\n",
        "    )\n",
        "    train_logistic_regression(\n",
        "        X_tr, y_tr, X_te, y_te, feats, tgts, run_name=\"lr_baseline\"\n",
        "    )\n",
        "    train_random_forest_optimized(\n",
        "        X_tr, y_tr, X_te, y_te, feats, tgts,\n",
        "        n_trials=n_trials, run_name=\"rf_optimized\"\n",
        "    )\n",
        "    \n",
        "    # Add pruning after training finishes\n",
        "    prune_experiment(\"iris_classification\", metric=\"accuracy\", top_k=1)\n",
        "    prune_model_versions(\"iris_classifier\", metric=\"accuracy\", top_k=1)\n",
        "\n",
        "\n",
        "# === (D) Robust comparator ===============================================\n",
        "def compare_models(\n",
        "    experiment_name: Optional[str] = None,\n",
        "    metric_key: str = \"accuracy\",\n",
        "    maximize: bool = True,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Print the best run according to *metric_key* while gracefully\n",
        "    falling-back to common alternates when the preferred key is missing.\n",
        "    \"\"\"\n",
        "    from .experiment_utils import get_best_run\n",
        "\n",
        "    fallback_keys = [\"accuracy_score\", \"best_accuracy\"]\n",
        "    try:\n",
        "        best = get_best_run(experiment_name, metric_key, maximize)\n",
        "        rid = best[\"run_id\"]\n",
        "\n",
        "        # choose first key that exists\n",
        "        score = best.get(f\"metrics.{metric_key}\")\n",
        "        if score is None:\n",
        "            for alt in fallback_keys:\n",
        "                score = best.get(f\"metrics.{alt}\")\n",
        "                if score is not None:\n",
        "                    metric_key = alt\n",
        "                    break\n",
        "\n",
        "        model_type = best.get(\"params.model_type\", \"unknown\")\n",
        "        print(f\"🏆 Best run: {rid}\")\n",
        "        print(f\"📈 {metric_key}: {score if score is not None else 'N/A'}\")\n",
        "        print(f\"🔖 Model type: {model_type}\")\n",
        "    except Exception as err:\n",
        "        print(f\"❌ Error comparing models: {err}\")\n",
        "\n",
        "\n",
        "# Legacy compatibility functions (with dashboard support)\n",
        "train_logistic_regression_autolog = train_logistic_regression\n",
        "train_random_forest_with_optimization = train_random_forest_optimized\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_all_trainings()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/mlops/explainer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/mlops/explainer.py\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import socket\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Any, Sequence, Optional\n",
        "from contextlib import closing\n",
        "\n",
        "import mlflow\n",
        "import psutil  # lightweight; already added to pyproject deps\n",
        "from sklearn.utils.multiclass import type_of_target\n",
        "from explainerdashboard import (\n",
        "    ClassifierExplainer,\n",
        "    RegressionExplainer,\n",
        "    ExplainerDashboard,\n",
        ")\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "__all__ = [\"build_and_log_dashboard\", \"load_dashboard_yaml\", \"dashboard_best_run\", \"_first_free_port\", \"_port_details\"]\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "def _port_details(port: int) -> str:\n",
        "    \"\"\"\n",
        "    Return a one-line string with PID & cmdline of the process\n",
        "    listening on *port*, or '' if none / not discoverable.\n",
        "    \"\"\"\n",
        "    for c in psutil.net_connections(kind=\"tcp\"):\n",
        "        if c.status == psutil.CONN_LISTEN and c.laddr and c.laddr.port == port:\n",
        "            try:\n",
        "                p = psutil.Process(c.pid)\n",
        "                return f\"[PID {p.pid} – {p.name()}] cmd={p.cmdline()}\"\n",
        "            except psutil.Error:\n",
        "                return f\"[PID {c.pid}] (no detail)\"\n",
        "    return \"\"\n",
        "\n",
        "def _first_free_port(start: int = 8050, tries: int = 50) -> int:\n",
        "    \"\"\"Return first free TCP port ≥ *start* on localhost.\"\"\"\n",
        "    for port in range(start, start + tries):\n",
        "        try:\n",
        "            with closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n",
        "                s.settimeout(0.05)\n",
        "                s.bind((\"127.0.0.1\", port))\n",
        "                return port\n",
        "        except OSError:\n",
        "            # Port is in use, try next one\n",
        "            continue\n",
        "    raise RuntimeError(\"⚠️  No free ports found in range\")\n",
        "\n",
        "def _next_free_port(start: int = 8050, tries: int = 50) -> int:\n",
        "    \"\"\"Return the first free TCP port ≥ *start*. (Alias for backward compatibility)\"\"\"\n",
        "    return _first_free_port(start, tries)\n",
        "\n",
        "def _port_in_use(port: int) -> bool:\n",
        "    \"\"\"Check if a port is already in use on any interface.\"\"\"\n",
        "    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n",
        "        s.settimeout(0.05)\n",
        "        # Check both localhost and 0.0.0.0 to be thorough\n",
        "        try:\n",
        "            # First check localhost (127.0.0.1)\n",
        "            if s.connect_ex((\"127.0.0.1\", port)) == 0:\n",
        "                return True\n",
        "            # Also check if anything is bound to all interfaces\n",
        "            if s.connect_ex((\"0.0.0.0\", port)) == 0:\n",
        "                return True\n",
        "        except (socket.gaierror, OSError):\n",
        "            # If we can't connect, assume port is free\n",
        "            pass\n",
        "        return False\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "# -------------------------------------------------------------- #\n",
        "#  src/mlops/explainer.py (only this function changed)           #\n",
        "# -------------------------------------------------------------- #\n",
        "def build_and_log_dashboard(\n",
        "    model: Any,\n",
        "    X_test,\n",
        "    y_test,\n",
        "    *,\n",
        "    # ---- explainer kwargs (unchanged) -------------------------\n",
        "    cats: Optional[Sequence[str]] = None,\n",
        "    idxs: Optional[Sequence[Any]] = None,\n",
        "    descriptions: Optional[dict[str, str]] = None,\n",
        "    target: Optional[str] = None,\n",
        "    labels: Optional[Sequence[str]] = None,\n",
        "    X_background=None,\n",
        "    model_output: str = \"probability\",\n",
        "    shap: str = \"guess\",\n",
        "    shap_interaction: bool = True,\n",
        "    simple: bool = False,\n",
        "    mode: str = \"dash\",         # 🆕 safest default for docker\n",
        "    title: str = \"Model Explainer\",\n",
        "    # ---- infra -----------------------------------------------\n",
        "    run: mlflow.ActiveRun | None = None,\n",
        "    port: int | None = None,\n",
        "    serve: bool = False,\n",
        "    server_backend: str = \"waitress\",   # 🆕 waitress|gunicorn|jupyterdash\n",
        "    conflict_strategy: str = \"next\",\n",
        "    max_tries: int = 20,\n",
        "    save_yaml: bool = True,\n",
        "    output_dir: os.PathLike | str | None = None,\n",
        ") -> Path:\n",
        "    \"\"\"\n",
        "    Build + (optionally) serve the dashboard.\n",
        "\n",
        "    server_backend\n",
        "        'waitress'    – production WSGI server (binds 0.0.0.0)  \n",
        "        'gunicorn'    – spawn via subprocess (needs gunicorn installed)  \n",
        "        'jupyterdash' – fallback; use only for notebook demos\n",
        "    \"\"\"\n",
        "    # ------------ build explainer (unchanged) ------------------\n",
        "    problem = type_of_target(y_test)\n",
        "    ExplainerCls = RegressionExplainer if problem.startswith(\"continuous\") else ClassifierExplainer\n",
        "    expl_kwargs = dict(\n",
        "        cats=cats, idxs=idxs, descriptions=descriptions, target=target,\n",
        "        labels=labels, X_background=X_background, model_output=model_output, shap=shap,\n",
        "    )\n",
        "    expl_kwargs = {k: v for k, v in expl_kwargs.items() if v is not None}\n",
        "    explainer = ExplainerCls(model, X_test, y_test, **expl_kwargs)\n",
        "\n",
        "    dash = ExplainerDashboard(\n",
        "        explainer, title=title, shap_interaction=shap_interaction,\n",
        "        simple=simple, mode=mode,\n",
        "    )\n",
        "\n",
        "    out_dir = Path(output_dir or \".\"); out_dir.mkdir(parents=True, exist_ok=True)\n",
        "    html_path = out_dir / \"explainer_dashboard.html\"; dash.save_html(html_path); mlflow.log_artifact(str(html_path))\n",
        "    if save_yaml:\n",
        "        yaml = out_dir / \"dashboard.yaml\"; dash.to_yaml(yaml); mlflow.log_artifact(str(yaml))\n",
        "\n",
        "    # ------------ serve ----------------------------------------\n",
        "    if not serve:\n",
        "        return html_path\n",
        "\n",
        "    chosen = port or _first_free_port()\n",
        "    attempts = 0\n",
        "    while _port_in_use(chosen):\n",
        "        if conflict_strategy == \"raise\":\n",
        "            raise RuntimeError(f\"Port {chosen} in use {_port_details(chosen)}\")\n",
        "        if conflict_strategy == \"kill\":\n",
        "            pid = int((_port_details(chosen) or \"PID 0\").split()[1]); psutil.Process(pid).terminate()\n",
        "            break\n",
        "        attempts += 1\n",
        "        if attempts >= max_tries:\n",
        "            raise RuntimeError(f\"No free port after {max_tries} tries\")\n",
        "        chosen += 1\n",
        "\n",
        "    logging.info(\"🌐 Dashboard on http://0.0.0.0:%s via %s\", chosen, server_backend)\n",
        "\n",
        "    if server_backend == \"waitress\":\n",
        "        dash.run(chosen, host=\"0.0.0.0\", use_waitress=True, mode=\"dash\")\n",
        "    elif server_backend == \"gunicorn\":\n",
        "        import subprocess, shlex\n",
        "        cmd = f\"gunicorn -w 3 -b 0.0.0.0:{chosen} dashboard:app\"\n",
        "        subprocess.Popen(shlex.split(cmd), cwd=str(out_dir))\n",
        "    else:  # jupyterdash\n",
        "        dash.run(chosen, host=\"0.0.0.0\")\n",
        "\n",
        "    return html_path\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------------\n",
        "def load_dashboard_yaml(path: os.PathLike | str) -> ExplainerDashboard:\n",
        "    \"\"\"Reload a YAML config – unchanged but kept for public API.\"\"\"\n",
        "    return ExplainerDashboard.from_config(path) \n",
        "\n",
        "\n",
        "# ────────────────────────────────────────────────────────────────────────────\n",
        "def dashboard_best_run(metric: str = \"accuracy\",\n",
        "                       maximize: bool = True,\n",
        "                       *, port: int | None = None) -> None:\n",
        "    \"\"\"\n",
        "    Load the *best* run (by `metric`) from the active experiment and\n",
        "    launch an ExplainerDashboard **once** for that model.\n",
        "\n",
        "    Example\n",
        "    -------\n",
        "    >>> from mlops.explainer import dashboard_best_run\n",
        "    >>> dashboard_best_run(\"accuracy\")      # opens http://0.0.0.0:8050\n",
        "    \"\"\"\n",
        "    from .experiment_utils import get_best_run\n",
        "    from .model_registry  import load_model_from_run\n",
        "    from sklearn.datasets import load_iris\n",
        "    import pandas as pd\n",
        "\n",
        "    best = get_best_run(metric_key=metric, maximize=maximize)\n",
        "    run_id = best[\"run_id\"]\n",
        "    model  = load_model_from_run(run_id)\n",
        "\n",
        "    iris = load_iris()\n",
        "    X_df  = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "    build_and_log_dashboard(\n",
        "        model, X_df, iris.target,\n",
        "        labels=list(iris.target_names),\n",
        "        run=None, serve=True, port=port or 8050\n",
        "    )\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/mlops/utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/mlops/utils.py\n",
        "from pathlib import Path\n",
        "import os\n",
        "# Add near the top of utils.py\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import inspect\n",
        "\n",
        "def add_project_root_to_sys_path(levels_up: int = 2) -> Path:\n",
        "    \"\"\"\n",
        "    Ensure the repository root (default: two directories up) is on sys.path.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Path\n",
        "        The absolute Path object pointing to the directory inserted.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        here = Path(__file__).resolve()\n",
        "    except NameError:           # running in Jupyter / IPython\n",
        "        # Use the file of the *caller* if possible,\n",
        "        # otherwise fall back to the current working directory.\n",
        "        caller = inspect.stack()[1].filename\n",
        "        here = Path(caller).resolve() if caller != \"<stdin>\" else Path.cwd()\n",
        "\n",
        "    root = here.parents[levels_up]\n",
        "    sys.path.insert(0, str(root))\n",
        "    return root\n",
        "\n",
        "\n",
        "_added_src_flag: bool = False          # module-level cache\n",
        "\n",
        "def project_root() -> Path:\n",
        "    \"\"\"\n",
        "    Return the absolute path to the repo root *without* relying on __file__.\n",
        "\n",
        "    • If running from a .py file, use that file's parent/parent (…/src/..)\n",
        "    • If running interactively (no __file__), fall back to CWD.\n",
        "    \"\"\"\n",
        "    if \"__file__\" in globals():\n",
        "        return Path(__file__).resolve().parent.parent\n",
        "    return Path.cwd()\n",
        "\n",
        "def ensure_src_on_path(verbose: bool = True) -> None:\n",
        "    \"\"\"\n",
        "    Ensure <repo-root>/src is the *first* entry in sys.path exactly once.\n",
        "    The verbose flag prints the helper line the first time only.\n",
        "    \"\"\"\n",
        "    import sys\n",
        "    global _added_src_flag\n",
        "    root = project_root()\n",
        "    src_path = root / \"src\"\n",
        "\n",
        "    if str(src_path) not in sys.path:\n",
        "        sys.path.insert(0, str(src_path))\n",
        "        if verbose and not _added_src_flag:\n",
        "            print(f\"🔧 Added {src_path} to sys.path\")\n",
        "        _added_src_flag = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/mlops/experiment_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/mlops/experiment_utils.py\n",
        "\"\"\"MLflow experiment utilities.\"\"\"\n",
        "import os\n",
        "import pathlib\n",
        "import mlflow\n",
        "import mlflow.tracking\n",
        "from typing import Optional, Dict, Any\n",
        "import requests\n",
        "\n",
        "from src.mlops.config import EXPERIMENT_NAME, TRACKING_URI\n",
        "\n",
        "import re, shutil, logging\n",
        "\n",
        "_HEALTH_ENDPOINTS = (\"/health\", \"/version\")\n",
        "_hex32 = re.compile(r\"^[0-9a-f]{32}$\", re.I)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "def _ping_tracking_server(uri: str, timeout: float = 2.0) -> bool:\n",
        "    \"\"\"Return True iff an HTTP MLflow server is reachable at *uri*.\"\"\"\n",
        "    if not uri.startswith(\"http\"):\n",
        "        return False                        # file store – nothing to ping\n",
        "    try:\n",
        "        # Use new health endpoints\n",
        "        for ep in _HEALTH_ENDPOINTS:\n",
        "            response = requests.get(uri.rstrip(\"/\") + ep, timeout=timeout)\n",
        "            response.raise_for_status()\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "\n",
        "# ─────────────────────────── src/mlops/experiment_utils.py ──────────────────\n",
        "\n",
        "\n",
        "def _sanitize_mlruns_dir(root: pathlib.Path) -> None:\n",
        "    \"\"\"\n",
        "    Remove or archive directories inside *root* that cannot possibly be valid\n",
        "    MLflow experiments (file-store experiments MUST be numeric).\n",
        "    \"\"\"\n",
        "    for p in root.iterdir():\n",
        "        if p.is_dir() and _hex32.match(p.name) and not (p / \"meta.yaml\").exists():\n",
        "            logging.warning(\"🧹 Removing orphan MLflow dir %s\", p)\n",
        "            shutil.rmtree(p, ignore_errors=True)\n",
        "\n",
        "def _fallback_uri() -> str:\n",
        "    \"\"\"Local file-store *outside* the default ./mlruns to avoid collisions.\"\"\"\n",
        "    local = pathlib.Path.cwd() / \"mlruns_local\"\n",
        "    local.mkdir(exist_ok=True)\n",
        "    _sanitize_mlruns_dir(local)          # one-time clean-up\n",
        "    return f\"file:{local}\"\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "# Add this just below the imports at module top (once per module)\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------------------\n",
        "def setup_mlflow_experiment(experiment_name: Optional[str] = None) -> None:\n",
        "    \"\"\"\n",
        "    Resolve a reachable MLflow tracking URI and make sure the experiment exists.\n",
        "    Falls back to a local file store if the remote /health or /version ping fails.\n",
        "    \"\"\"\n",
        "    from .config import EXPERIMENT_NAME, TRACKING_URI\n",
        "\n",
        "    exp_name = experiment_name or EXPERIMENT_NAME\n",
        "    uri = TRACKING_URI\n",
        "\n",
        "    def _ping(u: str) -> bool:\n",
        "        if not u.startswith(\"http\"):\n",
        "            return False\n",
        "        try:\n",
        "            for ep in (\"/health\", \"/version\"):\n",
        "                r = requests.get(u.rstrip(\"/\") + ep, timeout=2)\n",
        "                r.raise_for_status()\n",
        "            return True\n",
        "        except requests.RequestException as exc:\n",
        "            logger.debug(\"MLflow server ping failed: %s\", exc)\n",
        "            return False\n",
        "\n",
        "    if not _ping(uri):\n",
        "        uri = _fallback_uri()\n",
        "        logger.warning(\"⚠️  MLflow server unreachable – using local store %s\", uri)\n",
        "\n",
        "    mlflow.set_tracking_uri(uri)\n",
        "\n",
        "    # guarantee the experiment exists\n",
        "    if mlflow.get_experiment_by_name(exp_name) is None:\n",
        "        mlflow.create_experiment(exp_name, artifact_location=f\"{uri}/artifacts\")\n",
        "\n",
        "    mlflow.set_experiment(exp_name)\n",
        "    logger.info(\"🗂  Using MLflow experiment '%s' @ %s\", exp_name, uri)\n",
        "\n",
        "\n",
        "def get_best_run(\n",
        "    experiment_name: Optional[str] = None,\n",
        "    metric_key: str = \"accuracy\",\n",
        "    maximize: bool = True,\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Return a *shallow* dict with run_id, metrics.*, and params.* keys\n",
        "    so downstream code can use predictable dotted paths.\n",
        "    \"\"\"\n",
        "    exp_name = experiment_name or EXPERIMENT_NAME\n",
        "    setup_mlflow_experiment(exp_name)\n",
        "\n",
        "    client = mlflow.tracking.MlflowClient()\n",
        "    exp = mlflow.get_experiment_by_name(exp_name)\n",
        "    if exp is None:\n",
        "        raise ValueError(f\"Experiment '{exp_name}' not found\")\n",
        "\n",
        "    order = \"DESC\" if maximize else \"ASC\"\n",
        "    run = client.search_runs(\n",
        "        [exp.experiment_id],\n",
        "        order_by=[f\"metrics.{metric_key} {order}\"],\n",
        "        max_results=1,\n",
        "    )[0]\n",
        "\n",
        "    # Build a *flat* mapping -------------------------------------------------\n",
        "    flat: Dict[str, Any] = {\"run_id\": run.info.run_id}\n",
        "\n",
        "    # Metrics\n",
        "    for k, v in run.data.metrics.items():\n",
        "        flat[f\"metrics.{k}\"] = v\n",
        "\n",
        "    # Params\n",
        "    for k, v in run.data.params.items():\n",
        "        flat[f\"params.{k}\"] = v\n",
        "\n",
        "    # Tags (optional but handy)\n",
        "    for k, v in run.data.tags.items():\n",
        "        flat[f\"tags.{k}\"] = v\n",
        "\n",
        "    return flat\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/mlops/cleanup.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/mlops/cleanup.py\n",
        "# src/mlops/cleanup.py\n",
        "from __future__ import annotations\n",
        "import logging, shutil, pathlib\n",
        "from typing import Sequence, Optional\n",
        "import mlflow\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "_logger = logging.getLogger(__name__)\n",
        "\n",
        "def _runs_by_metric(\n",
        "    experiment_id: str,\n",
        "    metric: str = \"accuracy\",\n",
        "    maximize: bool = True\n",
        ") -> Sequence[mlflow.entities.Run]:\n",
        "    client = MlflowClient()\n",
        "    order = \"DESC\" if maximize else \"ASC\"\n",
        "    return client.search_runs(\n",
        "        [experiment_id],\n",
        "        order_by=[f\"metrics.{metric} {order}\"],\n",
        "        max_results=10_000,  # enough for 99 % of cases\n",
        "    )\n",
        "\n",
        "def prune_experiment(\n",
        "    experiment_name: str,\n",
        "    metric: str = \"accuracy\",\n",
        "    top_k: int = 1,\n",
        "    ascending: bool = False\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Prune an experiment by keeping only the top K runs based on a metric.\n",
        "    \n",
        "    Args:\n",
        "        experiment_name: Name of the experiment to prune\n",
        "        metric: Metric to sort by\n",
        "        top_k: Number of top runs to keep\n",
        "        ascending: Whether to sort in ascending order (default: False)\n",
        "    \"\"\"\n",
        "    client = MlflowClient()\n",
        "    experiment = client.get_experiment_by_name(experiment_name)\n",
        "    \n",
        "    if experiment is None:\n",
        "        print(f\"Experiment {experiment_name} not found\")\n",
        "        return\n",
        "        \n",
        "    # Get all runs for the experiment\n",
        "    runs = client.search_runs(\n",
        "        experiment_ids=[experiment.experiment_id],\n",
        "        filter_string=\"\",\n",
        "        order_by=[f\"metrics.{metric} {'ASC' if ascending else 'DESC'}\"]\n",
        "    )\n",
        "    \n",
        "    # Keep top K runs\n",
        "    runs_to_delete = runs[top_k:]\n",
        "    \n",
        "    # Delete the rest\n",
        "    for run in runs_to_delete:\n",
        "        client.delete_run(run.info.run_id)\n",
        "        print(f\"Deleted run {run.info.run_id} with {metric}={run.data.metrics.get(metric)}\")\n",
        "\n",
        "def prune_model_versions(\n",
        "    model_name: str,\n",
        "    metric: str = \"accuracy\",\n",
        "    top_k: int = 1,\n",
        "    ascending: bool = False\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Prune model versions by keeping only the top K versions based on a metric.\n",
        "    \n",
        "    Args:\n",
        "        model_name: Name of the registered model to prune\n",
        "        metric: Metric to sort by\n",
        "        top_k: Number of top versions to keep\n",
        "        ascending: Whether to sort in ascending order (default: False)\n",
        "    \"\"\"\n",
        "    client = MlflowClient()\n",
        "    \n",
        "    try:\n",
        "        versions = client.search_model_versions(f\"name='{model_name}'\")\n",
        "    except Exception:\n",
        "        print(f\"Model {model_name} not found\")\n",
        "        return\n",
        "        \n",
        "    # Get metrics for each version\n",
        "    version_metrics = []\n",
        "    for version in versions:\n",
        "        try:\n",
        "            if version.run_id is not None:  # Handle None run_id\n",
        "                run = client.get_run(version.run_id)\n",
        "                metric_value = run.data.metrics.get(metric)\n",
        "                if metric_value is not None:\n",
        "                    version_metrics.append((version, metric_value))\n",
        "        except Exception:\n",
        "            continue\n",
        "            \n",
        "    # Sort by metric\n",
        "    version_metrics.sort(key=lambda x: x[1], reverse=not ascending)\n",
        "    \n",
        "    # Keep top K versions\n",
        "    versions_to_delete = version_metrics[top_k:]\n",
        "    \n",
        "    # Delete the rest\n",
        "    for version, metric_value in versions_to_delete:\n",
        "        client.delete_model_version(model_name, version.version)\n",
        "        print(f\"Deleted version {version.version} with {metric}={metric_value}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/examples/shapiq_demo.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/examples/shapiq_demo.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "SHAP-IQ Integration Demo\n",
        "\n",
        "This script demonstrates the new SHAP-IQ (Shapley Interaction) functionality\n",
        "integrated into the MLOps pipeline. It shows how Shapley interaction values\n",
        "are computed and logged alongside regular model metrics.\n",
        "\n",
        "Usage:\n",
        "    python src/examples/shapiq_demo.py\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import logging\n",
        "\n",
        "# ─── Path setup ─────────────────────────────────────────────────────────────\n",
        "from src.mlops.utils import add_project_root_to_sys_path\n",
        "PROJECT_ROOT = add_project_root_to_sys_path(levels_up=2)  # safe in both .py and interactive :contentReference[oaicite:8]{index=8}\n",
        "\n",
        "# ─── Imports ────────────────────────────────────────────────────────────────\n",
        "from src.mlops.training import (\n",
        "    load_and_prepare_iris_data,\n",
        "    train_logistic_regression,\n",
        "    train_random_forest_optimized\n",
        ")\n",
        "from src.mlops.shapiq_utils import (\n",
        "    compute_shapiq_interactions,\n",
        "    log_shapiq_interactions,\n",
        "    get_top_interactions\n",
        ")\n",
        "from src.mlops.experiment_utils import setup_mlflow_experiment, get_best_run\n",
        "import mlflow\n",
        "import pandas as pd\n",
        "\n",
        "# ─── Logging Setup ─────────────────────────────────────────────────────────\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "def demo_standalone_shapiq():\n",
        "    \"\"\"Demonstrate standalone SHAP-IQ computation without MLflow logging.\"\"\"\n",
        "    print(\"🔬 SHAP-IQ Standalone Demo\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Load data and train a simple model\n",
        "    X_train, X_test, y_train, y_test, feature_names, target_names, _ = load_and_prepare_iris_data()\n",
        "    \n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    model = RandomForestClassifier(n_estimators=20, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    print(f\"✓ Trained RandomForest on {len(X_train)} samples\")\n",
        "    print(f\"✓ Test accuracy: {model.score(X_test, y_test):.3f}\")\n",
        "    \n",
        "    # Compute SHAP-IQ interactions\n",
        "    X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "    print(f\"\\n🧮 Computing SHAP-IQ interactions...\")\n",
        "    \n",
        "    shapiq_df = compute_shapiq_interactions(\n",
        "        model, \n",
        "        X_test_df.head(10),  # Use subset for demo\n",
        "        feature_names, \n",
        "        max_order=2,\n",
        "        budget=128\n",
        "    )\n",
        "    \n",
        "    if not shapiq_df.empty:\n",
        "        print(f\"✓ Computed {len(shapiq_df)} interaction values\")\n",
        "        \n",
        "        # Show top interactions\n",
        "        top_interactions = get_top_interactions(shapiq_df, top_n=5)\n",
        "        print(f\"\\n🏆 Top 5 Feature Interactions:\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        for idx, row in top_interactions.iterrows():\n",
        "            feature_combo = ' × '.join(row['feature_names'])\n",
        "            if not feature_combo:\n",
        "                feature_combo = \"baseline\"\n",
        "            print(f\"  {feature_combo:30} | Order {row['order']} | {row['abs_mean']:.4f}\")\n",
        "        \n",
        "        # Show order breakdown\n",
        "        order_counts = shapiq_df['order'].value_counts().sort_index()\n",
        "        print(f\"\\n📊 Interaction Order Breakdown:\")\n",
        "        for order, count in order_counts.items():\n",
        "            if order == 0:\n",
        "                print(f\"  Order {order} (main effects):     {count:4d} values\")\n",
        "            elif order == 1:\n",
        "                print(f\"  Order {order} (individual):       {count:4d} values\")\n",
        "            elif order == 2:\n",
        "                print(f\"  Order {order} (pairwise):         {count:4d} values\")\n",
        "            else:\n",
        "                print(f\"  Order {order} (higher-order):     {count:4d} values\")\n",
        "    else:\n",
        "        print(\"⚠️  No interactions computed (this can happen with simple models/data)\")\n",
        "\n",
        "\n",
        "def demo_integrated_training():\n",
        "    \"\"\"Demonstrate SHAP-IQ integration in the training pipeline.\"\"\"\n",
        "    print(\"\\n\\n🚀 SHAP-IQ Integrated Training Demo\") \n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Setup MLflow experiment\n",
        "    setup_mlflow_experiment(\"shapiq_demo\")\n",
        "    \n",
        "    # Load data\n",
        "    X_train, X_test, y_train, y_test, feature_names, target_names, _ = load_and_prepare_iris_data()\n",
        "    print(f\"✓ Loaded Iris dataset: {len(X_train)} train, {len(X_test)} test samples\")\n",
        "    \n",
        "    # Train model with SHAP-IQ integration\n",
        "    print(f\"\\n🤖 Training Logistic Regression with SHAP-IQ...\")\n",
        "    lr_run_id = train_logistic_regression(\n",
        "        X_train, y_train, X_test, y_test, \n",
        "        feature_names, target_names,\n",
        "        run_name=\"lr_with_shapiq\"\n",
        "    )\n",
        "    print(f\"✓ Logistic Regression complete: {lr_run_id[:8]}\")\n",
        "    \n",
        "    print(f\"\\n🌲 Training Random Forest with SHAP-IQ...\")\n",
        "    rf_run_id = train_random_forest_optimized(\n",
        "        X_train, y_train, X_test, y_test,\n",
        "        feature_names, target_names,\n",
        "        n_trials=10,  # Reduced for demo\n",
        "        run_name=\"rf_with_shapiq\"\n",
        "    )\n",
        "    print(f\"✓ Random Forest complete: {rf_run_id[:8]}\")\n",
        "    \n",
        "    # Show logged SHAP-IQ metrics\n",
        "    print(f\"\\n📊 SHAP-IQ Metrics from MLflow:\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    try:\n",
        "        # Get the latest run (Random Forest)\n",
        "        with mlflow.start_run(run_id=rf_run_id):\n",
        "            run_data = mlflow.get_run(rf_run_id)\n",
        "            metrics = run_data.data.metrics\n",
        "            \n",
        "            # Filter SHAP-IQ metrics\n",
        "            shapiq_metrics = {k: v for k, v in metrics.items() if k.startswith('shapiq_')}\n",
        "            \n",
        "            if shapiq_metrics:\n",
        "                print(f\"Found {len(shapiq_metrics)} SHAP-IQ metrics:\")\n",
        "                for metric, value in sorted(shapiq_metrics.items()):\n",
        "                    if 'order' in metric and 'count' not in metric:\n",
        "                        print(f\"  {metric:35} = {value:.6f}\")\n",
        "                    elif 'total' in metric or 'unique' in metric or 'max' in metric:\n",
        "                        print(f\"  {metric:35} = {int(value)}\")\n",
        "            else:\n",
        "                print(\"  No SHAP-IQ metrics found (may take longer to compute)\")\n",
        "                \n",
        "    except Exception as e:\n",
        "        print(f\"  Error retrieving metrics: {e}\")\n",
        "    \n",
        "    # Compare models\n",
        "    print(f\"\\n🏆 Comparing Models:\")\n",
        "    print(\"-\" * 30)\n",
        "    try:\n",
        "        best_run = get_best_run(\"accuracy\", maximize=True)\n",
        "        run_id = best_run[\"run_id\"]\n",
        "        accuracy = best_run.get(\"metrics.accuracy\", \"N/A\")\n",
        "        print(f\"Best model: {run_id[:8]} (accuracy: {accuracy})\")\n",
        "        \n",
        "        # Check if SHAP-IQ metrics are available for best model\n",
        "        shapiq_count = best_run.get(\"metrics.shapiq_total_interactions\")\n",
        "        if shapiq_count:\n",
        "            print(f\"SHAP-IQ interactions: {int(shapiq_count)} computed\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error comparing models: {e}\")\n",
        "\n",
        "\n",
        "def demo_manual_shapiq_logging():\n",
        "    \"\"\"Demonstrate manual SHAP-IQ logging outside of training.\"\"\"\n",
        "    print(f\"\\n\\n🔧 Manual SHAP-IQ Logging Demo\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    # Load data and train model\n",
        "    X_train, X_test, y_train, y_test, feature_names, target_names, _ = load_and_prepare_iris_data()\n",
        "    \n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Manual MLflow run with SHAP-IQ logging\n",
        "    setup_mlflow_experiment(\"shapiq_demo\") \n",
        "    \n",
        "    with mlflow.start_run(run_name=\"manual_shapiq_demo\"):\n",
        "        # Log basic metrics\n",
        "        accuracy = model.score(X_test, y_test)\n",
        "        mlflow.log_metric(\"accuracy\", accuracy)\n",
        "        \n",
        "        # Log SHAP-IQ interactions\n",
        "        X_test_df = pd.DataFrame(X_test, columns=feature_names)\n",
        "        print(\"Computing and logging SHAP-IQ interactions...\")\n",
        "        \n",
        "        log_shapiq_interactions(\n",
        "            model, \n",
        "            X_test_df,\n",
        "            feature_names,\n",
        "            max_order=2,\n",
        "            top_n=5,\n",
        "            budget=64,\n",
        "            n_samples=15  # Sample for faster computation\n",
        "        )\n",
        "        \n",
        "        current_run = mlflow.active_run()\n",
        "        print(f\"✓ SHAP-IQ logged to run: {current_run.info.run_id[:8]}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"Run all SHAP-IQ demos.\"\"\"\n",
        "    print(\"🌟 SHAP-IQ Integration Demonstration\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"This demo shows how Shapley interactions are computed and logged\")\n",
        "    print(\"in the MLOps pipeline to understand feature interactions.\")\n",
        "    print()\n",
        "    \n",
        "    try:\n",
        "        # Demo 1: Standalone computation\n",
        "        demo_standalone_shapiq()\n",
        "        \n",
        "        # Demo 2: Integrated training\n",
        "        demo_integrated_training()\n",
        "        \n",
        "        # Demo 3: Manual logging\n",
        "        demo_manual_shapiq_logging()\n",
        "        \n",
        "        print(f\"\\n\\n🎉 SHAP-IQ Demo Complete!\")\n",
        "        print(\"=\" * 60)\n",
        "        print(\"✓ Standalone SHAP-IQ computation\")\n",
        "        print(\"✓ Integrated training with automatic SHAP-IQ logging\") \n",
        "        print(\"✓ Manual SHAP-IQ logging\")\n",
        "        print()\n",
        "        print(\"🔍 Check MLflow UI to see logged SHAP-IQ metrics and artifacts:\")\n",
        "        print(\"   - Metrics: shapiq_order1_*, shapiq_order2_*, etc.\")\n",
        "        print(\"   - Artifacts: shapiq_interactions.csv, shapiq_interactions_summary.csv\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        logger.error(f\"Demo failed: {e}\")\n",
        "        print(f\"\\n❌ Demo failed: {e}\")\n",
        "        print(\"This might be due to SHAP-IQ dependency issues or data problems.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main() "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/examples/select_best_and_dashboard.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/examples/select_best_and_dashboard.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Select best model and launch dashboard (training is done elsewhere).\n",
        "\n",
        "Usage:\n",
        "    python src/examples/select_best_and_dashboard.py\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from src.mlops.utils import add_project_root_to_sys_path\n",
        "PROJECT_ROOT = add_project_root_to_sys_path()\n",
        "\n",
        "from src.mlops.experiment_utils import get_best_run\n",
        "from src.mlops.model_registry import load_model_from_run\n",
        "from src.mlops.explainer import dashboard_best_run\n",
        "\n",
        "# Configuration variables\n",
        "METRIC = \"accuracy\"  # Metric to optimize (e.g., 'accuracy', 'f1')\n",
        "PORT = 8050           # Port for the dashboard\n",
        "MAXIMIZE = True       # Whether to maximize (True) or minimize (False) the metric\n",
        "\n",
        "def main() -> None:\n",
        "    print(f\"🔍 Searching MLflow runs by {METRIC}…\")\n",
        "\n",
        "    # Retrieve the best run based on the specified metric\n",
        "    best = get_best_run(metric_key=METRIC, maximize=MAXIMIZE)\n",
        "    run_id = best[\"run_id\"]\n",
        "    score = best.get(f\"metrics.{METRIC}\", \"N/A\")\n",
        "\n",
        "    print(f\"🏆 Best run: {run_id[:8]} — {METRIC}: {score}\")\n",
        "\n",
        "    # Load the model from the run registry\n",
        "    model = load_model_from_run(run_id)\n",
        "    if model is None:\n",
        "        raise RuntimeError(\"Model could not be loaded from registry\")\n",
        "\n",
        "    print(\"✓ Model loaded – launching dashboard\")\n",
        "    # Launch the explainer dashboard for the best model\n",
        "    dashboard_best_run(METRIC, maximize=MAXIMIZE, port=PORT)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Running all training pipelines from /workspace\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/06/27 11:23:29 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
            "2025/06/27 11:23:31 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/06/27 11:23:32 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
            "Registered model 'iris_logreg' already exists. Creating a new version of this model...\n",
            "2025/06/27 11:23:33 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: iris_logreg, version 7\n",
            "Created version '7' of model 'iris_logreg'.\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/games/imputer/marginal_imputer.py:103: UserWarning: The sample size is larger than the number of data points in the background set. Reducing the sample size to the number of background samples.\n",
            "  self.init_background(self.data)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "[I 2025-06-27 11:23:56,930] A new study created in memory with name: no-name-b038c189-d61f-4194-afd6-18bd76e9cb5d\n",
            "/workspace/src/mlops/training.py:138: ExperimentalWarning: MLflowCallback is experimental (supported from v1.4.0). The interface can change in the future.\n",
            "  callbacks=[MLflowCallback(\n",
            "[I 2025-06-27 11:23:56,952] Trial 0 finished with value: 1.0 and parameters: {'n_estimators': 29, 'max_depth': 13, 'min_samples_split': 7, 'min_samples_leaf': 8}. Best is trial 0 with value: 1.0.\n",
            "2025/06/27 11:23:56 INFO mlflow.tracking.fluent: Experiment with name 'no-name-b038c189-d61f-4194-afd6-18bd76e9cb5d' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run lr_baseline at: http://mlflow:5000/#/experiments/1/runs/5a63ecab71694314862cfddb6068b808\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-27 11:23:57,152] Trial 1 finished with value: 1.0 and parameters: {'n_estimators': 112, 'max_depth': 15, 'min_samples_split': 14, 'min_samples_leaf': 10}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run 0 at: http://mlflow:5000/#/experiments/9/runs/8317aac7400b406cb537ef808872b15a\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/9\n",
            "🏃 View run 1 at: http://mlflow:5000/#/experiments/9/runs/b0cb5c7ff50040349ee21f1e761da021\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-27 11:23:57,350] Trial 2 finished with value: 1.0 and parameters: {'n_estimators': 180, 'max_depth': 11, 'min_samples_split': 15, 'min_samples_leaf': 4}. Best is trial 0 with value: 1.0.\n",
            "[I 2025-06-27 11:23:57,514] Trial 3 finished with value: 1.0 and parameters: {'n_estimators': 115, 'max_depth': 16, 'min_samples_split': 2, 'min_samples_leaf': 1}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run 2 at: http://mlflow:5000/#/experiments/9/runs/2f08f175765d4580919e75e8e0980a58\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/9\n",
            "🏃 View run 3 at: http://mlflow:5000/#/experiments/9/runs/44d930193fe14e64a310989f9cb0ea47\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-27 11:23:57,676] Trial 4 finished with value: 1.0 and parameters: {'n_estimators': 99, 'max_depth': 13, 'min_samples_split': 15, 'min_samples_leaf': 2}. Best is trial 0 with value: 1.0.\n",
            "[I 2025-06-27 11:23:57,804] Trial 5 finished with value: 1.0 and parameters: {'n_estimators': 19, 'max_depth': 13, 'min_samples_split': 20, 'min_samples_leaf': 3}. Best is trial 0 with value: 1.0.\n",
            "[I 2025-06-27 11:23:57,979] Trial 6 finished with value: 1.0 and parameters: {'n_estimators': 123, 'max_depth': 17, 'min_samples_split': 2, 'min_samples_leaf': 6}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run 4 at: http://mlflow:5000/#/experiments/9/runs/ca325db16b064077b504d87f8be4daea\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/9\n",
            "🏃 View run 5 at: http://mlflow:5000/#/experiments/9/runs/e7b1c7737dda4374819a591a76c6c579\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-27 11:23:58,132] Trial 7 finished with value: 1.0 and parameters: {'n_estimators': 85, 'max_depth': 3, 'min_samples_split': 7, 'min_samples_leaf': 7}. Best is trial 0 with value: 1.0.\n",
            "[I 2025-06-27 11:23:58,258] Trial 8 finished with value: 1.0 and parameters: {'n_estimators': 48, 'max_depth': 8, 'min_samples_split': 11, 'min_samples_leaf': 10}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run 6 at: http://mlflow:5000/#/experiments/9/runs/8be3f6782bc740009c1e7504303cace9\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/9\n",
            "🏃 View run 7 at: http://mlflow:5000/#/experiments/9/runs/0942085d484d45ddbd47326f2618e740\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-27 11:23:58,428] Trial 9 finished with value: 1.0 and parameters: {'n_estimators': 131, 'max_depth': 17, 'min_samples_split': 15, 'min_samples_leaf': 2}. Best is trial 0 with value: 1.0.\n",
            "[I 2025-06-27 11:23:58,552] Trial 10 finished with value: 1.0 and parameters: {'n_estimators': 17, 'max_depth': 7, 'min_samples_split': 6, 'min_samples_leaf': 8}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run 8 at: http://mlflow:5000/#/experiments/9/runs/f12d1422170a4a49b27b0ff68d68470c\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/9\n",
            "🏃 View run 9 at: http://mlflow:5000/#/experiments/9/runs/8f9f5fefa0014062a66dae5c4ad02de8\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-27 11:23:58,693] Trial 11 finished with value: 1.0 and parameters: {'n_estimators': 65, 'max_depth': 20, 'min_samples_split': 10, 'min_samples_leaf': 10}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run 10 at: http://mlflow:5000/#/experiments/9/runs/a422039a5bed456aa3aba0664cd938c7\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/9\n",
            "🏃 View run 11 at: http://mlflow:5000/#/experiments/9/runs/263160dc4caa44c289ccb183b5008b7f\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-27 11:23:58,872] Trial 12 finished with value: 1.0 and parameters: {'n_estimators': 148, 'max_depth': 11, 'min_samples_split': 8, 'min_samples_leaf': 8}. Best is trial 0 with value: 1.0.\n",
            "[I 2025-06-27 11:23:59,066] Trial 13 finished with value: 1.0 and parameters: {'n_estimators': 162, 'max_depth': 14, 'min_samples_split': 19, 'min_samples_leaf': 9}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run 12 at: http://mlflow:5000/#/experiments/9/runs/9c989fef27314942ad42e72f70540f1d\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/9\n",
            "🏃 View run 13 at: http://mlflow:5000/#/experiments/9/runs/085eab5710e441178a8995aa40cb5d3e\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-27 11:23:59,224] Trial 14 finished with value: 1.0 and parameters: {'n_estimators': 56, 'max_depth': 8, 'min_samples_split': 13, 'min_samples_leaf': 6}. Best is trial 0 with value: 1.0.\n",
            "[I 2025-06-27 11:23:59,429] Trial 15 finished with value: 1.0 and parameters: {'n_estimators': 196, 'max_depth': 20, 'min_samples_split': 6, 'min_samples_leaf': 8}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run 14 at: http://mlflow:5000/#/experiments/9/runs/15b58c72f845420da209185c67dce34d\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/9\n",
            "🏃 View run 15 at: http://mlflow:5000/#/experiments/9/runs/4f9b4cb7a6b24e4a846096ec3ab9406c\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-27 11:23:59,591] Trial 16 finished with value: 1.0 and parameters: {'n_estimators': 81, 'max_depth': 15, 'min_samples_split': 17, 'min_samples_leaf': 10}. Best is trial 0 with value: 1.0.\n",
            "[I 2025-06-27 11:23:59,732] Trial 17 finished with value: 1.0 and parameters: {'n_estimators': 38, 'max_depth': 10, 'min_samples_split': 9, 'min_samples_leaf': 5}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run 16 at: http://mlflow:5000/#/experiments/9/runs/b970ce9d862f4e5c88e885f77634c189\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/9\n",
            "🏃 View run 17 at: http://mlflow:5000/#/experiments/9/runs/1b19c821e9044150be1d458d07b32262\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-27 11:23:59,905] Trial 18 finished with value: 1.0 and parameters: {'n_estimators': 91, 'max_depth': 5, 'min_samples_split': 12, 'min_samples_leaf': 9}. Best is trial 0 with value: 1.0.\n",
            "[I 2025-06-27 11:24:00,086] Trial 19 finished with value: 1.0 and parameters: {'n_estimators': 142, 'max_depth': 19, 'min_samples_split': 4, 'min_samples_leaf': 7}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run 18 at: http://mlflow:5000/#/experiments/9/runs/1b485183a34048e3abde450ce80edc68\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/9\n",
            "🏃 View run 19 at: http://mlflow:5000/#/experiments/9/runs/3b93499a8ee54d62abe9be3e34b89922\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/06/27 11:24:00 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/06/27 11:24:01 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
            "Registered model 'iris_random_forest' already exists. Creating a new version of this model...\n",
            "2025/06/27 11:24:02 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: iris_random_forest, version 8\n",
            "Created version '8' of model 'iris_random_forest'.\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/games/imputer/marginal_imputer.py:103: UserWarning: The sample size is larger than the number of data points in the background set. Reducing the sample size to the number of background samples.\n",
            "  self.init_background(self.data)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run rf_optimized at: http://mlflow:5000/#/experiments/1/runs/93927db5ec8a42499a53e0e55b7f1f1d\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/1\n",
            "Deleted run 5a63ecab71694314862cfddb6068b808 with accuracy=1.0\n",
            "Deleted run b32db67770ef4cf8a012a491da62258c with accuracy=1.0\n",
            "✅ Training complete!\n"
          ]
        }
      ],
      "source": [
        "# %%writefile src/scripts/run_training.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Simple training runner script.\n",
        "\n",
        "Run with:\n",
        "    python src/scripts/run_training.py\n",
        "    # or inside Jupyter:\n",
        "    %run src/scripts/run_training.py\n",
        "\"\"\"\n",
        "\n",
        "from src.mlops.utils import add_project_root_to_sys_path\n",
        "\n",
        "# Ensure src/ is importable in both script and notebook contexts\n",
        "PROJECT_ROOT = add_project_root_to_sys_path()\n",
        "\n",
        "from src.mlops.training import run_all_trainings\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    print(\"🚀 Running all training pipelines from\", PROJECT_ROOT)\n",
        "    run_all_trainings(n_trials=20)\n",
        "    print(\"✅ Training complete!\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔧 Added /workspace/src/src to sys.path\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:src.mlops.experiment_utils:🗂  Using MLflow experiment 'iris_classification' @ http://mlflow:5000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🌸 Iris Classification with MLflow\n",
            "==================================================\n",
            "✓ Training samples: 120 | Test: 30\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/06/27 11:24:05 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
            "2025/06/27 11:24:06 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/06/27 11:24:07 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
            "Registered model 'iris_logreg' already exists. Creating a new version of this model...\n",
            "2025/06/27 11:24:08 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: iris_logreg, version 8\n",
            "Created version '8' of model 'iris_logreg'.\n",
            "INFO:src.mlops.shapiq_utils:Starting SHAP-IQ interaction logging\n",
            "INFO:src.mlops.shapiq_utils:Computing SHAP-IQ (max_order=2, budget=256, n_samples=None)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/games/imputer/marginal_imputer.py:103: UserWarning: The sample size is larger than the number of data points in the background set. Reducing the sample size to the number of background samples.\n",
            "  self.init_background(self.data)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "INFO:src.mlops.shapiq_utils:✓ 330 interaction rows computed\n",
            "INFO:src.mlops.shapiq_utils:Logging top 10 interactions as MLflow metrics\n",
            "INFO:src.mlops.shapiq_utils:Logged SHAP-IQ interactions artifact: shapiq_interactions.csv\n",
            "INFO:src.mlops.shapiq_utils:Logged SHAP-IQ summary artifact: shapiq_interactions_summary.csv\n",
            "INFO:src.mlops.shapiq_utils:SHAP-IQ interaction logging completed\n",
            "INFO:src.mlops.experiment_utils:🗂  Using MLflow experiment 'iris_classification' @ http://mlflow:5000\n",
            "[I 2025-06-27 11:24:31,675] A new study created in memory with name: no-name-718f2096-9809-416d-a383-caab41c60099\n",
            "/workspace/src/mlops/training.py:138: ExperimentalWarning: MLflowCallback is experimental (supported from v1.4.0). The interface can change in the future.\n",
            "  callbacks=[MLflowCallback(\n",
            "[I 2025-06-27 11:24:31,724] Trial 0 finished with value: 1.0 and parameters: {'n_estimators': 118, 'max_depth': 7, 'min_samples_split': 12, 'min_samples_leaf': 2}. Best is trial 0 with value: 1.0.\n",
            "2025/06/27 11:24:31 INFO mlflow.tracking.fluent: Experiment with name 'no-name-718f2096-9809-416d-a383-caab41c60099' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run lr_baseline at: http://mlflow:5000/#/experiments/1/runs/eeb9842ad90a477498af8f705a0b4a01\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/1\n",
            "✓ Logistic run eeb9842a\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-27 11:24:31,939] Trial 1 finished with value: 1.0 and parameters: {'n_estimators': 155, 'max_depth': 17, 'min_samples_split': 7, 'min_samples_leaf': 10}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run 0 at: http://mlflow:5000/#/experiments/10/runs/66d99e18bee749c08a23c2ffeae5a888\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/10\n",
            "🏃 View run 1 at: http://mlflow:5000/#/experiments/10/runs/b38a1de5961148d18b32a4d92608465c\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-27 11:24:32,126] Trial 2 finished with value: 1.0 and parameters: {'n_estimators': 170, 'max_depth': 7, 'min_samples_split': 5, 'min_samples_leaf': 4}. Best is trial 0 with value: 1.0.\n",
            "[I 2025-06-27 11:24:32,322] Trial 3 finished with value: 1.0 and parameters: {'n_estimators': 135, 'max_depth': 5, 'min_samples_split': 9, 'min_samples_leaf': 2}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run 2 at: http://mlflow:5000/#/experiments/10/runs/eae32df58a4248a7a1b894d208a54002\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/10\n",
            "🏃 View run 3 at: http://mlflow:5000/#/experiments/10/runs/f2c2c8fbe89c4b329a72c0a36cf4a1ae\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-27 11:24:32,482] Trial 4 finished with value: 1.0 and parameters: {'n_estimators': 56, 'max_depth': 9, 'min_samples_split': 18, 'min_samples_leaf': 10}. Best is trial 0 with value: 1.0.\n",
            "[I 2025-06-27 11:24:32,680] Trial 5 finished with value: 1.0 and parameters: {'n_estimators': 153, 'max_depth': 4, 'min_samples_split': 19, 'min_samples_leaf': 2}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run 4 at: http://mlflow:5000/#/experiments/10/runs/47df5c9f89fa4ec49d391b16f3dc78f4\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/10\n",
            "🏃 View run 5 at: http://mlflow:5000/#/experiments/10/runs/a5f02341f13544b1843f3f6e75d4e7f7\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-27 11:24:32,882] Trial 6 finished with value: 1.0 and parameters: {'n_estimators': 186, 'max_depth': 17, 'min_samples_split': 20, 'min_samples_leaf': 7}. Best is trial 0 with value: 1.0.\n",
            "[I 2025-06-27 11:24:33,067] Trial 7 finished with value: 1.0 and parameters: {'n_estimators': 166, 'max_depth': 3, 'min_samples_split': 5, 'min_samples_leaf': 3}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run 6 at: http://mlflow:5000/#/experiments/10/runs/cad04e48367f42d5a301087dbb6378be\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/10\n",
            "🏃 View run 7 at: http://mlflow:5000/#/experiments/10/runs/b0ac0421c6964778aa430c00bc6dccca\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-27 11:24:33,244] Trial 8 finished with value: 1.0 and parameters: {'n_estimators': 150, 'max_depth': 16, 'min_samples_split': 11, 'min_samples_leaf': 6}. Best is trial 0 with value: 1.0.\n",
            "[I 2025-06-27 11:24:33,406] Trial 9 finished with value: 1.0 and parameters: {'n_estimators': 126, 'max_depth': 2, 'min_samples_split': 6, 'min_samples_leaf': 6}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run 8 at: http://mlflow:5000/#/experiments/10/runs/91494dfa96964fc69e60a20de1a55a71\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/10\n",
            "🏃 View run 9 at: http://mlflow:5000/#/experiments/10/runs/f3bdb930115849e6afca9f5dc312d519\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-27 11:24:33,569] Trial 10 finished with value: 1.0 and parameters: {'n_estimators': 75, 'max_depth': 12, 'min_samples_split': 14, 'min_samples_leaf': 1}. Best is trial 0 with value: 1.0.\n",
            "[I 2025-06-27 11:24:33,747] Trial 11 finished with value: 1.0 and parameters: {'n_estimators': 96, 'max_depth': 20, 'min_samples_split': 13, 'min_samples_leaf': 9}. Best is trial 0 with value: 1.0.\n",
            "[I 2025-06-27 11:24:33,889] Trial 12 finished with value: 1.0 and parameters: {'n_estimators': 19, 'max_depth': 13, 'min_samples_split': 9, 'min_samples_leaf': 8}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run 10 at: http://mlflow:5000/#/experiments/10/runs/6f2f8da4b10f483580aad181d33dd399\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/10\n",
            "🏃 View run 11 at: http://mlflow:5000/#/experiments/10/runs/ad114db4b7e340f58e834c1167794624\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-27 11:24:34,092] Trial 13 finished with value: 1.0 and parameters: {'n_estimators': 110, 'max_depth': 10, 'min_samples_split': 2, 'min_samples_leaf': 4}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run 12 at: http://mlflow:5000/#/experiments/10/runs/7c1f060415344e6fa4a877b11ac697bb\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/10\n",
            "🏃 View run 13 at: http://mlflow:5000/#/experiments/10/runs/dc16332629ad452fa0e38024d5599051\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-27 11:24:34,299] Trial 14 finished with value: 1.0 and parameters: {'n_estimators': 190, 'max_depth': 15, 'min_samples_split': 15, 'min_samples_leaf': 10}. Best is trial 0 with value: 1.0.\n",
            "[I 2025-06-27 11:24:34,495] Trial 15 finished with value: 1.0 and parameters: {'n_estimators': 109, 'max_depth': 20, 'min_samples_split': 9, 'min_samples_leaf': 5}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run 14 at: http://mlflow:5000/#/experiments/10/runs/17169d1675d546dc9bc859635324b92c\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/10\n",
            "🏃 View run 15 at: http://mlflow:5000/#/experiments/10/runs/201a4531885e43749166bade2a69e6f3\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-27 11:24:34,660] Trial 16 finished with value: 1.0 and parameters: {'n_estimators': 78, 'max_depth': 7, 'min_samples_split': 16, 'min_samples_leaf': 8}. Best is trial 0 with value: 1.0.\n",
            "[I 2025-06-27 11:24:34,863] Trial 17 finished with value: 1.0 and parameters: {'n_estimators': 132, 'max_depth': 14, 'min_samples_split': 12, 'min_samples_leaf': 1}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run 16 at: http://mlflow:5000/#/experiments/10/runs/4d4de90248914cdd8d0dacd75fb8df90\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/10\n",
            "🏃 View run 17 at: http://mlflow:5000/#/experiments/10/runs/83bc3282e2114d65ad67f692ea548f2a\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-06-27 11:24:35,014] Trial 18 finished with value: 1.0 and parameters: {'n_estimators': 30, 'max_depth': 8, 'min_samples_split': 7, 'min_samples_leaf': 5}. Best is trial 0 with value: 1.0.\n",
            "[I 2025-06-27 11:24:35,185] Trial 19 finished with value: 1.0 and parameters: {'n_estimators': 88, 'max_depth': 18, 'min_samples_split': 3, 'min_samples_leaf': 7}. Best is trial 0 with value: 1.0.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run 18 at: http://mlflow:5000/#/experiments/10/runs/7bef05a3a139438082f409c3f523f56e\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/10\n",
            "🏃 View run 19 at: http://mlflow:5000/#/experiments/10/runs/be76227ff3824e529de1f9bafa49c6fa\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025/06/27 11:24:35 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
            "2025/06/27 11:24:37 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
            "Registered model 'iris_random_forest' already exists. Creating a new version of this model...\n",
            "2025/06/27 11:24:38 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: iris_random_forest, version 9\n",
            "Created version '9' of model 'iris_random_forest'.\n",
            "INFO:src.mlops.shapiq_utils:Starting SHAP-IQ interaction logging\n",
            "INFO:src.mlops.shapiq_utils:Computing SHAP-IQ (max_order=2, budget=256, n_samples=None)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/games/imputer/marginal_imputer.py:103: UserWarning: The sample size is larger than the number of data points in the background set. Reducing the sample size to the number of background samples.\n",
            "  self.init_background(self.data)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "/app/.venv/lib/python3.10/site-packages/shapiq/approximator/regression/base.py:152: UserWarning: Not all budget is required due to the border-trick.\n",
            "  self._sampler.sample(budget)\n",
            "INFO:src.mlops.shapiq_utils:✓ 330 interaction rows computed\n",
            "INFO:src.mlops.shapiq_utils:Logging top 10 interactions as MLflow metrics\n",
            "INFO:src.mlops.shapiq_utils:Logged SHAP-IQ interactions artifact: shapiq_interactions.csv\n",
            "INFO:src.mlops.shapiq_utils:Logged SHAP-IQ summary artifact: shapiq_interactions_summary.csv\n",
            "INFO:src.mlops.shapiq_utils:SHAP-IQ interaction logging completed\n",
            "INFO:src.mlops.experiment_utils:🗂  Using MLflow experiment 'iris_classification' @ http://mlflow:5000\n",
            "INFO:src.mlops.experiment_utils:🗂  Using MLflow experiment 'iris_classification' @ http://mlflow:5000\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🏃 View run rf_optimized at: http://mlflow:5000/#/experiments/1/runs/e3a78eec36d0430f939f2b0e2f3722fc\n",
            "🧪 View experiment at: http://mlflow:5000/#/experiments/1\n",
            "✓ RF run e3a78eec\n",
            "🏆 Best run: e3a78eec36d0430f939f2b0e2f3722fc\n",
            "📈 accuracy: 1.0\n",
            "🔖 Model type: unknown\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d1b6e7d06e964a77b0c7fe8e76d64701",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7b4f39d04f14afc9cd74655c2f2829f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading artifacts:   0%|          | 0/7 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model from run e3a78eec36d0430f939f2b0e2f3722fc\n",
            "🏆 Best model accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# %%writefile src/examples/iris_classification_example.py\n",
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Iris Classification Example (argparse-free, notebook-safe).\n",
        "\n",
        "Configuration:\n",
        "    • export EXPLAINER_DASHBOARD=1   # launch dashboard\n",
        "    • export EXPLAINER_PORT=8150     # optional port override\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import os\n",
        "import logging\n",
        "\n",
        "from src.mlops.utils import ensure_src_on_path\n",
        "ensure_src_on_path()\n",
        "\n",
        "from src.mlops.training import (\n",
        "    load_and_prepare_iris_data,\n",
        "    train_logistic_regression,\n",
        "    train_random_forest_optimized,\n",
        "    compare_models,\n",
        ")\n",
        "from src.mlops.model_registry import load_model_from_run\n",
        "from src.mlops.experiment_utils import get_best_run\n",
        "\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\n",
        "def _bool_env(var: str, default: bool = False) -> bool:\n",
        "    v = os.getenv(var)\n",
        "    return default if v is None else v.lower() in {\"1\", \"true\", \"yes\"}\n",
        "\n",
        "\n",
        "def main(*, dashboard: bool = False, dashboard_port: int | None = None) -> None:\n",
        "    print(\"🌸 Iris Classification with MLflow\\n\" + \"=\" * 50)\n",
        "\n",
        "    # 1 Load data ------------------------------------------------------------\n",
        "    X_train, X_test, y_train, y_test, feat_names, tgt_names, _ = (\n",
        "        load_and_prepare_iris_data()\n",
        "    )\n",
        "    print(f\"✓ Training samples: {len(X_train)} | Test: {len(X_test)}\")\n",
        "\n",
        "    # 2 Logistic Regression --------------------------------------------------\n",
        "    lr_run = train_logistic_regression(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        feat_names,\n",
        "        tgt_names,\n",
        "        run_name=\"lr_baseline\",\n",
        "        register=True,\n",
        "    )\n",
        "    print(f\"✓ Logistic run {lr_run[:8]}\")\n",
        "\n",
        "    # 3 Random Forest + Optuna ----------------------------------------------\n",
        "    rf_run = train_random_forest_optimized(\n",
        "        X_train,\n",
        "        y_train,\n",
        "        X_test,\n",
        "        y_test,\n",
        "        feat_names,\n",
        "        tgt_names,\n",
        "        n_trials=20,\n",
        "        run_name=\"rf_optimized\",\n",
        "        register=True,\n",
        "    )\n",
        "    print(f\"✓ RF run {rf_run[:8]}\")\n",
        "\n",
        "    # 4 Compare & test best --------------------------------------------------\n",
        "    compare_models()\n",
        "    best = get_best_run()\n",
        "    mdl = load_model_from_run(best[\"run_id\"])\n",
        "    if mdl is not None:\n",
        "        acc = (mdl.predict(X_test) == y_test).mean()\n",
        "        print(f\"🏆 Best model accuracy: {acc:.4f}\")\n",
        "    else:\n",
        "        print(\"❌ Could not load best model\")\n",
        "\n",
        "    if dashboard:\n",
        "        port = dashboard_port or int(os.getenv(\"EXPLAINER_PORT\", \"8050\"))\n",
        "        print(f\"\\n🚀 ExplainerDashboard running on http://localhost:{port}\")\n",
        "        # Import and run dashboard for best model\n",
        "        from src.mlops.explainer import dashboard_best_run\n",
        "        dashboard_best_run(\"accuracy\", port=port)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main(\n",
        "        dashboard=_bool_env(\"EXPLAINER_DASHBOARD\", False),\n",
        "        dashboard_port=int(os.getenv(\"EXPLAINER_PORT\", \"8050\")),\n",
        "    ) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting tests/test_mlflow_integration.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tests/test_mlflow_integration.py\n",
        "\n",
        "\"\"\"Tests for MLflow integration modules.\"\"\"\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\n",
        "\n",
        "from mlops.experiment import setup_mlflow_experiment\n",
        "from mlops.training import (\n",
        "    load_and_prepare_iris_data, \n",
        "    train_logistic_regression\n",
        ")\n",
        "from mlops.model_registry import load_model_from_run\n",
        "\n",
        "\n",
        "def test_data_loading():\n",
        "    \"\"\"Test that data loading works correctly.\"\"\"\n",
        "    data = load_and_prepare_iris_data()\n",
        "    X_train, X_test, y_train, y_test, feature_names, target_names, scaler = data\n",
        "    \n",
        "    assert len(X_train) > 0\n",
        "    assert len(X_test) > 0\n",
        "    assert len(feature_names) == 4\n",
        "    assert len(target_names) == 3\n",
        "    assert X_train.shape[1] == 4  # 4 features\n",
        "\n",
        "\n",
        "def test_experiment_setup():\n",
        "    \"\"\"Test that MLflow experiment setup works.\"\"\"\n",
        "    # This should not raise an exception\n",
        "    setup_mlflow_experiment(\"test_experiment\")\n",
        "    \n",
        "\n",
        "def test_model_training_and_loading():\n",
        "    \"\"\"Test end-to-end model training and loading.\"\"\"\n",
        "    # Load data\n",
        "    data = load_and_prepare_iris_data()\n",
        "    X_train, X_test, y_train, y_test, feature_names, target_names, scaler = data\n",
        "    \n",
        "    # Train a simple model\n",
        "    run_id = train_logistic_regression(\n",
        "        X_train, y_train, X_test, y_test,\n",
        "        feature_names, target_names,\n",
        "        run_name=\"test_lr\",\n",
        "        register=False  # Don't register for tests\n",
        "    )\n",
        "    \n",
        "    assert run_id is not None\n",
        "    assert len(run_id) > 0\n",
        "    \n",
        "    # Load the model back\n",
        "    model = load_model_from_run(run_id, \"model\")\n",
        "    \n",
        "    # Test prediction\n",
        "    predictions = model.predict(X_test)\n",
        "    assert len(predictions) == len(y_test)\n",
        "    \n",
        "    # Check accuracy is reasonable (should be > 0.8 for iris)\n",
        "    accuracy = (predictions == y_test).mean()\n",
        "    assert accuracy > 0.8\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run tests\n",
        "    test_data_loading()\n",
        "    print(\"✓ Data loading test passed\")\n",
        "    \n",
        "    test_experiment_setup()\n",
        "    print(\"✓ Experiment setup test passed\")\n",
        "    \n",
        "    test_model_training_and_loading()\n",
        "    print(\"✓ Model training and loading test passed\")\n",
        "    \n",
        "    print(\"\\nAll tests passed! 🎉\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting tests/test_explainer.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tests/test_explainer.py\n",
        "import sys\n",
        "import os\n",
        "import pytest\n",
        "from pathlib import Path\n",
        "\n",
        "# Add src to path\n",
        "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
        "\n",
        "def test_yaml_roundtrip(tmp_path):\n",
        "    \"\"\"Test that a dashboard can be saved to YAML and reloaded.\"\"\"\n",
        "    from src.mlops.explainer import build_and_log_dashboard, load_dashboard_yaml\n",
        "    from sklearn.datasets import load_iris\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    import mlflow\n",
        "    import pandas as pd\n",
        "\n",
        "    iris = load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "    X_df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "    model = LogisticRegression(max_iter=1000).fit(X, y)\n",
        "    with mlflow.start_run():\n",
        "        yaml_path = build_and_log_dashboard(\n",
        "            model, X_df, y,\n",
        "            serve=False,\n",
        "            save_yaml=True,\n",
        "            output_dir=tmp_path\n",
        "        )\n",
        "        # Reload\n",
        "        dash = load_dashboard_yaml(yaml_path)\n",
        "        assert dash.explainer.model.__class__.__name__ == \"LogisticRegression\"\n",
        "\n",
        "\n",
        "def test_build_dashboard(tmp_path):\n",
        "    \"\"\"Test that a dashboard can be built and saved.\"\"\"\n",
        "    from src.mlops.explainer import build_and_log_dashboard\n",
        "    from sklearn.datasets import load_iris\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    import mlflow\n",
        "    import pandas as pd\n",
        "\n",
        "    iris = load_iris()\n",
        "    X, y = iris.data, iris.target\n",
        "    X_df = pd.DataFrame(X, columns=iris.feature_names)\n",
        "    model = LogisticRegression(max_iter=1000).fit(X, y)\n",
        "    with mlflow.start_run():\n",
        "        html = build_and_log_dashboard(\n",
        "            model, X_df, y,\n",
        "            serve=False,\n",
        "            save_yaml=False,\n",
        "            output_dir=tmp_path\n",
        "        )\n",
        "        assert html.exists() and html.suffix == \".html\" \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting tests/test_shapiq_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile tests/test_shapiq_utils.py\n",
        "\"\"\"\n",
        "Tests for SHAP-IQ utilities module.\n",
        "\"\"\"\n",
        "\n",
        "import sys\n",
        "import os\n",
        "import pytest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tempfile\n",
        "from unittest.mock import patch, MagicMock\n",
        "\n",
        "# Add src to path\n",
        "sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "from mlops.shapiq_utils import (\n",
        "    compute_shapiq_interactions,\n",
        "    log_shapiq_interactions,\n",
        "    get_top_interactions\n",
        ")\n",
        "\n",
        "\n",
        "@pytest.fixture\n",
        "def sample_data():\n",
        "    \"\"\"Create sample classification data for testing.\"\"\"\n",
        "    X, y = make_classification(\n",
        "        n_samples=50,\n",
        "        n_features=4,\n",
        "        n_informative=3,\n",
        "        n_redundant=1,\n",
        "        random_state=42\n",
        "    )\n",
        "    feature_names = [f\"feature_{i}\" for i in range(4)]\n",
        "    X_df = pd.DataFrame(X, columns=feature_names)\n",
        "    return X_df, y, feature_names\n",
        "\n",
        "\n",
        "@pytest.fixture\n",
        "def trained_model(sample_data):\n",
        "    \"\"\"Create a trained model for testing.\"\"\"\n",
        "    X_df, y, _ = sample_data\n",
        "    model = RandomForestClassifier(n_estimators=10, random_state=42)\n",
        "    model.fit(X_df, y)\n",
        "    return model\n",
        "\n",
        "\n",
        "def test_compute_shapiq_interactions_basic(sample_data, trained_model):\n",
        "    \"\"\"Test basic functionality of compute_shapiq_interactions.\"\"\"\n",
        "    X_df, _, feature_names = sample_data\n",
        "    \n",
        "    # Test with small sample to speed up test\n",
        "    result_df = compute_shapiq_interactions(\n",
        "        trained_model, \n",
        "        X_df.head(5),  # Use only 5 samples for testing\n",
        "        feature_names, \n",
        "        max_order=2, \n",
        "        budget=64  # Small budget for fast testing\n",
        "    )\n",
        "    \n",
        "    # Check structure\n",
        "    expected_columns = ['sample_idx', 'combination', 'value', 'order', 'feature_names']\n",
        "    assert all(col in result_df.columns for col in expected_columns)\n",
        "    \n",
        "    # Check data types\n",
        "    assert result_df['sample_idx'].dtype in [np.int64, int]\n",
        "    assert result_df['value'].dtype in [np.float64, float]\n",
        "    assert result_df['order'].dtype in [np.int64, int]\n",
        "    \n",
        "    # Check that we have interactions of different orders\n",
        "    if not result_df.empty:\n",
        "        orders = result_df['order'].unique()\n",
        "        assert len(orders) > 0\n",
        "        assert all(order <= 2 for order in orders)  # max_order=2\n",
        "\n",
        "\n",
        "def test_compute_shapiq_interactions_with_sampling(sample_data, trained_model):\n",
        "    \"\"\"Test compute_shapiq_interactions with n_samples parameter.\"\"\"\n",
        "    X_df, _, feature_names = sample_data\n",
        "    \n",
        "    result_df = compute_shapiq_interactions(\n",
        "        trained_model, \n",
        "        X_df, \n",
        "        feature_names, \n",
        "        max_order=1,  # Simple interactions only\n",
        "        budget=32,\n",
        "        n_samples=3   # Sample only 3 rows\n",
        "    )\n",
        "    \n",
        "    if not result_df.empty:\n",
        "        # Should have at most 3 different sample indices\n",
        "        unique_samples = result_df['sample_idx'].nunique()\n",
        "        assert unique_samples <= 3\n",
        "\n",
        "\n",
        "def test_compute_shapiq_interactions_empty_result():\n",
        "    \"\"\"Test handling of edge cases that might result in empty results.\"\"\"\n",
        "    # Create trivial data that might not generate interactions\n",
        "    X = pd.DataFrame([[1, 1], [1, 1]], columns=['a', 'b'])\n",
        "    y = [0, 0]\n",
        "    \n",
        "    model = LogisticRegression()\n",
        "    model.fit(X, y)\n",
        "    \n",
        "    result_df = compute_shapiq_interactions(\n",
        "        model, X, ['a', 'b'], max_order=1, budget=16\n",
        "    )\n",
        "    \n",
        "    # Should return a DataFrame with correct structure even if empty\n",
        "    expected_columns = ['sample_idx', 'combination', 'value', 'order', 'feature_names']\n",
        "    assert all(col in result_df.columns for col in expected_columns)\n",
        "\n",
        "\n",
        "@patch('mlflow.log_metric')\n",
        "@patch('mlflow.log_artifact')\n",
        "def test_log_shapiq_interactions(mock_log_artifact, mock_log_metric, sample_data, trained_model):\n",
        "    \"\"\"Test log_shapiq_interactions with mocked MLflow calls.\"\"\"\n",
        "    X_df, _, feature_names = sample_data\n",
        "    \n",
        "    with tempfile.TemporaryDirectory() as tmpdir:\n",
        "        output_path = os.path.join(tmpdir, \"test_interactions.csv\")\n",
        "        \n",
        "        # Call the function\n",
        "        log_shapiq_interactions(\n",
        "            trained_model,\n",
        "            X_df.head(5),  # Small sample for testing\n",
        "            feature_names,\n",
        "            max_order=2,\n",
        "            top_n=3,\n",
        "            budget=32,\n",
        "            output_path=output_path\n",
        "        )\n",
        "        \n",
        "        # Check that MLflow functions were called\n",
        "        assert mock_log_metric.called\n",
        "        assert mock_log_artifact.called\n",
        "        \n",
        "        # Check some expected metric calls\n",
        "        metric_calls = [call[0][0] for call in mock_log_metric.call_args_list]\n",
        "        expected_metrics = [\n",
        "            \"shapiq_total_interactions\",\n",
        "            \"shapiq_unique_combinations\", \n",
        "            \"shapiq_max_order\",\n",
        "            \"shapiq_samples_analyzed\"\n",
        "        ]\n",
        "        \n",
        "        for expected in expected_metrics:\n",
        "            assert any(expected in call for call in metric_calls), f\"Expected metric {expected} not found\"\n",
        "\n",
        "\n",
        "def test_get_top_interactions(sample_data, trained_model):\n",
        "    \"\"\"Test get_top_interactions utility function.\"\"\"\n",
        "    X_df, _, feature_names = sample_data\n",
        "    \n",
        "    # First compute interactions\n",
        "    shapiq_df = compute_shapiq_interactions(\n",
        "        trained_model, \n",
        "        X_df.head(10), \n",
        "        feature_names, \n",
        "        max_order=2, \n",
        "        budget=64\n",
        "    )\n",
        "    \n",
        "    if not shapiq_df.empty:\n",
        "        # Test getting top interactions\n",
        "        top_interactions = get_top_interactions(shapiq_df, top_n=5)\n",
        "        assert len(top_interactions) <= 5\n",
        "        \n",
        "        # Check structure\n",
        "        expected_columns = ['combination', 'feature_names', 'order', 'mean', 'std', 'count', 'abs_mean']\n",
        "        assert all(col in top_interactions.columns for col in expected_columns)\n",
        "        \n",
        "        # Test filtering by order\n",
        "        if len(shapiq_df['order'].unique()) > 1:\n",
        "            order_filtered = get_top_interactions(shapiq_df, top_n=3, order=1)\n",
        "            if not order_filtered.empty:\n",
        "                assert all(order_filtered['order'] == 1)\n",
        "\n",
        "\n",
        "def test_compute_shapiq_interactions_error_handling():\n",
        "    \"\"\"Test error handling in compute_shapiq_interactions.\"\"\"\n",
        "    # Create data that might cause issues\n",
        "    X = pd.DataFrame([[np.nan, 1], [2, np.nan]], columns=['a', 'b'])\n",
        "    y = [0, 1]\n",
        "    \n",
        "    model = LogisticRegression()\n",
        "    \n",
        "    # This should handle errors gracefully and return empty DataFrame\n",
        "    try:\n",
        "        model.fit([[1, 1], [2, 2]], [0, 1])  # Fit with clean data\n",
        "        result_df = compute_shapiq_interactions(model, X, ['a', 'b'], max_order=1, budget=16)\n",
        "        \n",
        "        # Should return DataFrame with expected structure even on error\n",
        "        expected_columns = ['sample_idx', 'combination', 'value', 'order', 'feature_names']\n",
        "        assert all(col in result_df.columns for col in expected_columns)\n",
        "        \n",
        "    except Exception:\n",
        "        # If an exception occurs, that's also acceptable for this edge case\n",
        "        pass\n",
        "\n",
        "\n",
        "@patch('mlflow.log_metric')\n",
        "@patch('mlflow.log_artifact')\n",
        "def test_log_shapiq_interactions_empty_result(mock_log_artifact, mock_log_metric):\n",
        "    \"\"\"Test log_shapiq_interactions when no interactions are computed.\"\"\"\n",
        "    # Mock compute_shapiq_interactions to return empty DataFrame\n",
        "    with patch('mlops.shapiq_utils.compute_shapiq_interactions') as mock_compute:\n",
        "        mock_compute.return_value = pd.DataFrame(columns=['sample_idx', 'combination', 'value', 'order', 'feature_names'])\n",
        "        \n",
        "        # This should handle empty results gracefully\n",
        "        log_shapiq_interactions(\n",
        "            MagicMock(),  # Mock model\n",
        "            pd.DataFrame([[1, 2]], columns=['a', 'b']),\n",
        "            ['a', 'b'],\n",
        "            max_order=1\n",
        "        )\n",
        "        \n",
        "        # Should not log metrics or artifacts for empty results\n",
        "        assert not mock_log_metric.called\n",
        "        assert not mock_log_artifact.called\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    pytest.main([__file__]) "
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
