"""
Model persistence utilities for NFL Kicker Analysis.

This module provides functions to save and load fitted models (Bayesian or scikit-learn)
to/from disk using joblib, avoiding Streamlit caching issues with un-picklable objects.
"""

from __future__ import annotations
import json
import hashlib
import datetime as _dt
from pathlib import Path
from typing import Any, Optional, Union, cast, Dict
import joblib  # fast, compressed persistence
import mlflow
import shutil
from mlflow.tracking import MlflowClient
from mlflow.pyfunc.model import PythonModel, PythonModelContext
from mlflow.models.signature import ModelSignature
from mlflow.types.schema import Schema
from mlflow.exceptions import MlflowException
from mlflow import sklearn as mlflow_sklearn
from src.nfl_kicker_analysis.config import config
# Ensure experiments directory or tracking server is initialized,
# and bind to a named experiment (creates it if missing).
mlflow.set_experiment(config.MLFLOW_EXPERIMENT_NAME)
from mlflow.models import infer_signature
from mlflow.pyfunc import load_model as mlflow_load_model
import pandas as pd
import numpy as np
import cloudpickle

DEFAULT_DIR = Path("/workspace/models")
MLRUNS_DIR = Path("/workspace/mlruns")

def cleanup_all_models():
    """
    Clean up all existing models and MLflow runs.
    """
    # ‚îÄ‚îÄ‚îÄ Helper for rmtree errors ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    def _on_rm_error(func, path, exc_info):
        err = exc_info[1]
        if isinstance(err, OSError) and err.errno == 16:
            print(f"‚ö†Ô∏è  Could not remove {path!r} (resource busy), skipping.")
        else:
            raise

    # ‚îÄ‚îÄ‚îÄ Remove local models directory ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    if DEFAULT_DIR.exists():
        shutil.rmtree(DEFAULT_DIR, onerror=_on_rm_error)
    DEFAULT_DIR.mkdir(parents=True, exist_ok=True)

    # ‚îÄ‚îÄ‚îÄ Remove MLflow runs directory ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    if MLRUNS_DIR.exists():
        try:
            shutil.rmtree(MLRUNS_DIR, onerror=_on_rm_error)
        except Exception as e:
            print(f"‚ö†Ô∏è  Unexpected error removing mlruns: {e}")
    MLRUNS_DIR.mkdir(parents=True, exist_ok=True)
    (MLRUNS_DIR / ".trash").mkdir(parents=True, exist_ok=True)

    # ‚îÄ‚îÄ‚îÄ Delete registered models from MLflow registry ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    registry_dir = MLRUNS_DIR / "models"
    if registry_dir.exists():
        client = MlflowClient()
        for rm in client.search_registered_models():
            client.delete_registered_model(rm.name)
    else:
        print(f"‚ÑπÔ∏è  MLflow registry folder {registry_dir!r} not found; skipping registry cleanup.")


def get_best_metrics(name: str) -> dict[str, float] | None:
    """
    Get the best metrics for `name`:
      1. Try MLflow registry (preferred).
      2. If not found or MLflow error, scan local version folders for metrics.json.
      3. Otherwise return None.
    """
    client = MlflowClient()

    # 1) MLflow lookup
    try:
        versions = client.get_latest_versions(name)
        if versions:
            run_id = versions[0].run_id
            if run_id:
                run = client.get_run(run_id)
                return dict(run.data.metrics)
    except Exception:
        pass

    # 2) Local fallback: look inside version subdirectories
    model_dir = DEFAULT_DIR / name
    if model_dir.exists():
        # list timestamped subfolders
        version_dirs = sorted(
            [d for d in model_dir.iterdir() if d.is_dir()]
        )
        if version_dirs:
            latest = version_dirs[-1]
            meta_path = latest / "metrics.json"
            if meta_path.exists():
                try:
                    return json.load(meta_path.open("r"))
                except Exception:
                    pass

    # 3) Nothing found
    return None



def _timestamp() -> str:
    """Helper for version folders - returns current timestamp as string."""
    return _dt.datetime.now().strftime("%Y%m%d_%H%M%S")

def _hash_dict(d: dict) -> str:
    """Create stable 8-char SHA-1 hash of dict for cache keys."""
    raw = json.dumps(d, sort_keys=True).encode()
    return hashlib.sha1(raw).hexdigest()[:8]

def get_model_metadata(
    name: str,
    version: str | None = "latest",
    base_dir: Path = DEFAULT_DIR
) -> dict[str, Any]:
    """
    Load metadata for a saved model without loading the model itself.
    
    Args:
        name: Model name
        version: Specific version or "latest"
        base_dir: Base directory for model storage
        
    Returns:
        Metadata dictionary
    """
    model_dir = base_dir / name
    
    if version == "latest":
        version_dirs = sorted(model_dir.iterdir(), reverse=True)
        if not version_dirs:
            raise FileNotFoundError(f"No saved model found for '{name}'")
        model_dir = version_dirs[0]
    else:
        if version is None:
            raise ValueError("Version cannot be None when not 'latest'")
        model_dir = model_dir / version

    meta_path = model_dir / "meta.json"
    if not meta_path.exists():
        return {}
        
    with meta_path.open("r") as fp:
        return json.load(fp)

def list_registered_models() -> dict[str, list[str]]:
    """
    Return a mapping of all MLflow-registered model names ‚Üí list of version strings.
    """
    client = MlflowClient()
    out: dict[str, list[str]] = {}
    # search_registered_models is the correct method in recent MLflow releases
    for rm in client.search_registered_models():
        name = rm.name
        # get_latest_versions still works
        versions = client.get_latest_versions(name)
        out[name] = [v.version for v in versions]
    return out

def list_saved_models(base_dir: Path = DEFAULT_DIR) -> dict[str, list[str]]:
    """
    List all saved filesystem models and their version subdirectories.
    """
    if not base_dir.exists():
        return {}
    models = {}
    for d in base_dir.iterdir():
        if d.is_dir():
            versions = [v.name for v in d.iterdir() if v.is_dir()]
            if versions:
                models[d.name] = sorted(versions, reverse=True)
    return models

def _save_leaderboard(df: pd.DataFrame):
    """
    Persist the dynamic Bayesian leaderboard to disk.
    """
    out = df.reset_index().rename(columns={"index": "player_id"})
    out.to_csv(config.LEADERBOARD_FILE, index=False)

class _WrappedModel(PythonModel):
    def __init__(self, model):
        super().__init__()
        self._model = model

    def predict(
        self,
        context: PythonModelContext,
        model_input: pd.DataFrame
    ) -> pd.DataFrame:
        """
        context: MLflow-provided context (unused here)
        model_input: pandas DataFrame of features
        returns: pandas DataFrame or Series of predictions
        """
        # Delegate to the underlying model
        return pd.DataFrame(self._model.predict(model_input))

from mlflow.exceptions import MlflowException
from mlflow.tracking import MlflowClient

from yaml.representer import RepresenterError

def save_model(
    model: Any,
    name: str,
    metrics: dict[str, float],
    meta: dict | None = None,
    register: bool = True,
    stage: str = "Staging"
) -> str | None:
    """
    Persist model to MLflow and register it if its metrics improve.
    Catches YAML serialization errors during stage transition.
    """
    if "accuracy" not in metrics:
        raise ValueError("Metrics must include 'accuracy'")

    best = get_best_metrics(name)
    if best and metrics["accuracy"] <= best.get("accuracy", 0):
        print(f"‚ö†Ô∏è New accuracy ({metrics['accuracy']:.4f}) ‚â§ best ({best['accuracy']:.4f}); skipping save.")
        return None

    try:
        with mlflow.start_run(nested=True) as run:
            mlflow.log_metrics(metrics)
            kwargs = {
                "artifact_path": name,
                "registered_model_name": None,
                "metadata": meta or {},
            }
            if hasattr(model, "predict") and "sklearn" in type(model).__module__:
                mlflow_sklearn.log_model(sk_model=model, **kwargs)
            else:
                mlflow.pyfunc.log_model(
                    python_model=_WrappedModel(model),
                    **kwargs
                )
            artifact_uri = f"runs:/{run.info.run_id}/{name}"
            print(f"‚úÖ Logged model to {artifact_uri}")

            if register:
                result = mlflow.register_model(
                    model_uri=artifact_uri,
                    name=name
                )
                version = result.version
                print(f"‚úÖ Registered model '{name}' as version {version}")

                client = MlflowClient()
                try:
                    client.transition_model_version_stage(
                        name, version, stage,
                        archive_existing_versions=False
                    )
                    uri = f"models:/{name}/{stage}"
                    print(f"‚úÖ Transitioned version {version} to stage '{stage}'")
                    return uri
                except RepresenterError as rep_err:
                    print(f"‚ö†Ô∏è Could not serialize model-version metadata: {rep_err}; skipping stage transition.")
                    return f"models:/{name}/{version}"

            return artifact_uri

    except MlflowException as e:
        # Fallback to local filesystem
        ts = _timestamp()
        version_dir = DEFAULT_DIR / name / ts
        version_dir.mkdir(parents=True, exist_ok=True)

        # Save metrics.json
        metrics_path = version_dir / "metrics.json"
        with open(metrics_path, "w") as f:
            json.dump(metrics, f)

        path = version_dir / "model.joblib"
        joblib.dump(model, path)
        print(f"‚ö†Ô∏è MLflow error ({e}); saved locally to {path}")
        return str(path)

# ‚îÄ‚îÄ New imports ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
from mlflow.sklearn import load_model as sklearn_load_model
from mlflow.pyfunc import load_model as pyfunc_load_model
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def load_model(
    name: str,
    version: str | None = "latest",
    base_dir: Path = DEFAULT_DIR
) -> Any:
    """
    Load a saved model:
      1) Try the sklearn flavor (full API w/ predict_proba).
      2) Fallback to the pyfunc flavor (generic predict-only).
      3) Fallback to local filesystem (joblib/cloudpickle).
    """
    model_uri = f"models:/{name}/{version or 'latest'}"

    # 1Ô∏è‚É£ sklearn flavor
    try:
        model = sklearn_load_model(model_uri)
        print(f"üîÑ Loaded sklearn model '{name}' from '{model_uri}'")
        return model
    except Exception as e:
        print(f"‚ö†Ô∏è sklearn flavor load failed ({e}); trying pyfunc...")

    # 2Ô∏è‚É£ pyfunc flavor
    try:
        model = pyfunc_load_model(model_uri)
        print(f"üîÑ Loaded pyfunc model '{name}' from '{model_uri}'")
        return model
    except Exception as e:
        print(f"‚ö†Ô∏è pyfunc flavor load failed ({e}); trying local filesystem...")

    # 3Ô∏è‚É£ Filesystem fallback
    model_root = base_dir / name
    if not model_root.exists():
        raise FileNotFoundError(f"No model directory for '{name}' at {model_root}")

    # [‚Ä¶existing local-loading logic‚Ä¶]
    versions = sorted(d for d in model_root.iterdir() if d.is_dir())
    if not versions:
        raise FileNotFoundError(f"No versions for '{name}' in {model_root}")
    model_dir = versions[-1]

    joblib_path = model_dir / "model.joblib"
    if joblib_path.exists():
        return joblib.load(joblib_path)

    pkl_path = model_dir / "model.pkl"
    if pkl_path.exists():
        import cloudpickle
        with open(pkl_path, "rb") as f:
            return cloudpickle.load(f)

    raise FileNotFoundError(f"No model file in {model_dir}. Expected 'model.joblib' or 'model.pkl'.")


from mlflow.tracking import MlflowClient

def get_best_model_info(name: str) -> tuple[str, float | None]:
    """
    Return the latest registered version and its accuracy for `name`.
    """
    client = MlflowClient()
    versions = client.get_latest_versions(name)
    if not versions:
        raise ValueError(f"No registered versions found for '{name}'")
    latest_ver = versions[0].version
    metrics = get_best_metrics(name)
    acc = metrics.get("accuracy") if metrics else None
    return latest_ver, acc



if __name__ == "__main__":
    # Clean up existing models first
    cleanup_all_models()
    print("üßπ Cleaned up existing models and MLflow runs")
    # # Re-initialize MLflow now that mlruns/ is fresh
    # mlflow.set_experiment(config.MLFLOW_EXPERIMENT_NAME)

    import numpy as np
    import pymc as pm

    # 1Ô∏è‚É£ Dummy test with metrics
    class Dummy:
        def __init__(self, x):
            self.x = x
            
        def predict(self, X):
            return np.array([self.x] * len(X))

    dummy = Dummy(42)
    uri = save_model(dummy, name="dummy_model", metrics=config.metrics)
    
    # Try saving a worse model - should be rejected
    dummy2 = Dummy(43)
    worse_metrics = {"accuracy": 0.80, "f1": 0.78}
    uri2 = save_model(dummy2, name="dummy_model", metrics=worse_metrics)
    assert uri2 is None, "Worse model should have been rejected"
    
    # Try saving a better model - should be accepted
    dummy3 = Dummy(44)
    better_metrics = {"accuracy": 0.90, "f1": 0.88}
    uri3 = save_model(dummy3, name="dummy_model", metrics=better_metrics)
    assert uri3 is not None, "Better model should have been accepted"
    
    loaded_dummy = load_model("dummy_model")
    #assert isinstance(loaded_dummy, Dummy)
    print("‚úÖ Dummy save/load with metrics passed!")

    # 2Ô∏è‚É£ LogisticRegression test with metrics
    from sklearn.linear_model import LogisticRegression
    X, y = [[0,0],[1,1],[1,0],[0,1]], [0,1,1,0]
    model = LogisticRegression().fit(X, y)
    metrics = {"accuracy": 0.75, "f1": 0.73}
    save_model(model, name="logreg_test", metrics=metrics)
    reloaded = load_model("logreg_test")
    assert (reloaded.predict(X) == model.predict(X)).all()
    print("‚úÖ LogisticRegression save/load with metrics passed!")

    # 3Ô∏è‚É£ RandomForestClassifier test with metrics
    from sklearn.ensemble import RandomForestClassifier
    rf = RandomForestClassifier(n_estimators=10, random_state=0).fit(X, y)
    metrics = {"accuracy": 0.85, "f1": 0.83}
    save_model(rf, name="rf_test", metrics=metrics)
    rf2 = load_model("rf_test")
    assert (rf2.predict(X) == rf.predict(X)).all()
    print("‚úÖ RandomForestClassifier save/load with metrics passed!")

