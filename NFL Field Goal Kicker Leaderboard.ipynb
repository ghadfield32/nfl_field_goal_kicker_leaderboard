{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NFL Field Goal Kicker Analysis\n",
        "\n",
        "Exploratory data analysis and modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/nfl_kicker_analysis/eda.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/nfl_kicker_analysis/eda.py\n",
        "\"\"\"\n",
        "NFL Kicker Field‑Goal EDA Utilities \n",
        "\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import Sequence, Tuple\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "\n",
        "# ───────────────────── configuration ────────────────────────────\n",
        "logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s │ %(levelname)s │ %(message)s\",\n",
        "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
        ")\n",
        "\n",
        "plt.rcParams.update({\n",
        "    \"figure.figsize\": (12, 7),\n",
        "    \"axes.spines.top\": False,\n",
        "    \"axes.spines.right\": False,\n",
        "})\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "_FIELD_GOAL_RESULT_SUCCESS = \"Made\"\n",
        "_PRESEASON_FLAG = \"Pre\"\n",
        "\n",
        "# ──────────────────────── core helpers ──────────────────────────\n",
        "\n",
        "def load_and_merge(\n",
        "    kickers_path: Path | str,\n",
        "    attempts_path: Path | str,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Load raw CSVs and merge on player_id.\n",
        "\n",
        "    - Parses 'birthdate' in kickers.csv.\n",
        "    - Parses 'game_date' in field_goal_attempts.csv.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Combined DataFrame with parsed dates.\n",
        "    \"\"\"\n",
        "    # 1. Inspect columns for debugging\n",
        "    logger.info(\"Inspecting kickers file columns: %s\", pd.read_csv(kickers_path, nrows=0).columns.tolist())\n",
        "    logger.info(\"Inspecting attempts file columns: %s\", pd.read_csv(attempts_path, nrows=0).columns.tolist())\n",
        "\n",
        "    # 2. Read with correct parse_dates per file\n",
        "    logger.info(\"Reading kickers.csv with parse_dates=['birthdate']\")\n",
        "    kickers = pd.read_csv(kickers_path, parse_dates=[\"birthdate\"])\n",
        "\n",
        "    logger.info(\"Reading field_goal_attempts.csv with parse_dates=['game_date']\")\n",
        "    attempts = pd.read_csv(attempts_path, parse_dates=[\"game_date\"])\n",
        "\n",
        "    # 3. Merge\n",
        "    df = attempts.merge(kickers, on=\"player_id\", how=\"left\", validate=\"many_to_one\")\n",
        "\n",
        "    missing = df[\"player_name\"].isna().sum()\n",
        "    if missing:\n",
        "        logger.warning(\"%d attempts missing kicker metadata\", missing)\n",
        "    logger.info(\"Merged shape: %s rows × %s cols\", *df.shape)\n",
        "\n",
        "    print(f\"Merged dataset shape: {df.shape}\")\n",
        "    print(f\"Total field goal attempts: {len(df):,}\")\n",
        "    missing = df[\"player_name\"].isna().sum()\n",
        "    print(f\"Attempts with missing kicker info: {missing}\")\n",
        "    print(\"\\nFirst 5 rows of merged dataset:\")\n",
        "    print(df.head(5))\n",
        "    return df\n",
        "\n",
        "\n",
        "def basic_overview(df: pd.DataFrame) -> None:\n",
        "    print(\"\\n─ BASIC OVERVIEW ─\")\n",
        "    print(\"Data types:\")\n",
        "    print(df.dtypes)\n",
        "    print(\"\\nFull info:\")\n",
        "    print(df.info())\n",
        "    dupes = df.duplicated().sum()\n",
        "    print(f\"\\nDuplicate rows: {dupes}\")\n",
        "    print(f\"\\nUnique seasons: {sorted(df['season'].unique())}\")\n",
        "    print(f\"Season types: {df['season_type'].unique()}\")\n",
        "    print(f\"Field goal results: {df['field_goal_result'].unique()}\")\n",
        "    print(f\"Unique kickers: {df['player_name'].nunique()}\")\n",
        "    # date range/span\n",
        "    if \"game_date\" in df.columns:\n",
        "        rng = df[\"game_date\"]\n",
        "        if not np.issubdtype(rng.dtype, np.datetime64):\n",
        "            rng = pd.to_datetime(rng)\n",
        "        print(f\"\\nDate range: {rng.min()} to {rng.max()}\")\n",
        "        print(f\"Span: {rng.max() - rng.min()}\")\n",
        "\n",
        "\n",
        "def basic_overview(df: pd.DataFrame) -> None:\n",
        "    \"\"\"Print high‑level schema info mirroring the notebook's *Section 2*.\"\"\"\n",
        "    print(\"\\n─ BASIC OVERVIEW ─\")\n",
        "    print(df.dtypes)\n",
        "    print(\"\\nUnique kickers:\", df[\"player_name\"].nunique())\n",
        "    print(\"Seasons:\", sorted(df[\"season\"].unique()))\n",
        "    dupe = df.duplicated().sum()\n",
        "    if dupe:\n",
        "        logger.warning(\"%d duplicate rows detected\", dupe)\n",
        "\n",
        "\n",
        "def prepare_dataset(\n",
        "    df: pd.DataFrame,\n",
        "    *,\n",
        "    include_preseason: bool = False,\n",
        "    max_distance: int | None = 63,\n",
        "    add_age_feature: bool = True,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Clean/filter & engineer variables exactly as the notebook does.\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Filter out preseason unless explicitly requested\n",
        "    if not include_preseason:\n",
        "        df = df[df[\"season_type\"] != _PRESEASON_FLAG]\n",
        "\n",
        "    # Binary target\n",
        "    df[\"success\"] = (df[\"field_goal_result\"] == _FIELD_GOAL_RESULT_SUCCESS).astype(int)\n",
        "\n",
        "    # Remove extreme distances\n",
        "    if max_distance is not None:\n",
        "        df = df[df[\"attempt_yards\"] <= max_distance]\n",
        "\n",
        "    # Feature engineering\n",
        "    df[\"distance_squared\"] = df[\"attempt_yards\"].pow(2)\n",
        "    df[\"is_long_attempt\"] = (df[\"attempt_yards\"] >= 50).astype(int)\n",
        "\n",
        "    # Sort for cumulative counts\n",
        "    df = df.sort_values([\"player_id\", \"game_date\"])\n",
        "    df[\"kicker_attempt_number\"] = df.groupby(\"player_id\").cumcount() + 1\n",
        "\n",
        "    # Age at attempt\n",
        "    if add_age_feature and {\"birthdate\", \"game_date\"}.issubset(df.columns):\n",
        "        df[\"age_at_attempt\"] = (\n",
        "            (df[\"game_date\"] - df[\"birthdate\"]).dt.days / 365.25\n",
        "        ).round(2)\n",
        "\n",
        "    logger.info(\"Prepared tidy dataset → %s rows\", len(df))\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "\n",
        "# ─────────────────────── analytical helpers ────────────────────\n",
        "def outcome_summary(\n",
        "    df: pd.DataFrame,\n",
        "    savefig: Path | None = None,\n",
        ") -> Tuple[pd.Series, plt.Figure]:\n",
        "    \"\"\"Outcome counts + pie/bar figure (adds binary-distribution prints).\"\"\"\n",
        "    counts = df[\"field_goal_result\"].value_counts()\n",
        "    success_rate = (df[\"success\"]).mean()\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    counts.plot.pie(ax=ax1, autopct=\"%1.1f%%\", startangle=90)\n",
        "    ax1.set_ylabel(\"\")\n",
        "    ax1.set_title(\"Regular-Season Field-Goal Outcomes\")\n",
        "\n",
        "    season_success = df.groupby(\"season_type\")[\"success\"].mean()\n",
        "    sns.barplot(\n",
        "        x=season_success.index,\n",
        "        y=season_success.values,\n",
        "        palette=[\"lightblue\", \"orange\"],\n",
        "        ax=ax2,\n",
        "    )\n",
        "    ax2.set_title(\"Success Rate by Season Type\")\n",
        "    ax2.set_ylabel(\"Success Rate\")\n",
        "    ax2.set_xlabel(\"\")\n",
        "    ax2.set_ylim(0.7, 0.9)\n",
        "    for i, v in enumerate(season_success.values):\n",
        "        ax2.text(i, v + 0.01, f\"{v:.1%}\", ha=\"center\", va=\"bottom\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if savefig:\n",
        "        fig.savefig(savefig, dpi=150, bbox_inches=\"tight\")\n",
        "\n",
        "    # 🆕 notebook-style console echoes\n",
        "    print(f\"\\nBinary target distribution — Success (Made): {df['success'].sum():,}\"\n",
        "          f\" ({success_rate:.1%}) | Failure: {(1-success_rate):.1%}\")\n",
        "\n",
        "    return counts, fig\n",
        "\n",
        "\n",
        "def distance_analysis(\n",
        "    df: pd.DataFrame,\n",
        "    *,\n",
        "    min_attempts: int = 3,\n",
        "    savefig: Path | None = None,\n",
        ") -> Tuple[pd.DataFrame, plt.Figure]:\n",
        "    \"\"\"Histogram + scatter + box-plot + printed distance buckets.\"\"\"\n",
        "    summary = (\n",
        "        df.groupby(\"attempt_yards\")[\"success\"]\n",
        "        .agg(success_rate=\"mean\", attempts=\"size\")\n",
        "        .query(\"attempts >= @min_attempts\")\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # Define distance buckets exactly as the notebook\n",
        "    buckets = [\n",
        "        (18, 29, \"Short (18-29)\"),\n",
        "        (30, 39, \"Medium-Short (30-39)\"),\n",
        "        (40, 49, \"Medium (40-49)\"),\n",
        "        (50, 59, \"Long (50-59)\"),\n",
        "        (60, 75, \"Extreme (60+)\"),\n",
        "    ]\n",
        "\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(12, 14))\n",
        "\n",
        "    # A) Histogram\n",
        "    sns.histplot(df[\"attempt_yards\"], bins=30, edgecolor=\"black\",\n",
        "                 color=\"skyblue\", ax=ax1)\n",
        "    ax1.set_title(\"Distribution of Field-Goal Attempt Distances\")\n",
        "    ax1.set_xlabel(\"Distance (yards)\")\n",
        "\n",
        "    # B) Scatter + quadratic trend\n",
        "    sizes = summary[\"attempts\"] / 2\n",
        "    ax2.scatter(summary[\"attempt_yards\"], summary[\"success_rate\"],\n",
        "                s=sizes, alpha=0.6, color=\"darkblue\")\n",
        "    z = np.polyfit(summary[\"attempt_yards\"], summary[\"success_rate\"], 2)\n",
        "    ax2.plot(\n",
        "        np.unique(summary[\"attempt_yards\"]),\n",
        "        np.poly1d(z)(np.unique(summary[\"attempt_yards\"])),\n",
        "        \"r--\", linewidth=2,\n",
        "    )\n",
        "    ax2.set_title(\"Success Rate vs Distance (bubble = attempts)\")\n",
        "    ax2.set_xlabel(\"Distance (yards)\")\n",
        "    ax2.set_ylabel(\"Success Rate\")\n",
        "    ax2.set_ylim(0, 1.05)\n",
        "\n",
        "    # C) Box-plot by outcome\n",
        "    sns.boxplot(\n",
        "        x=\"field_goal_result\",\n",
        "        y=\"attempt_yards\",\n",
        "        data=df,\n",
        "        ax=ax3,\n",
        "        palette=\"Set2\",\n",
        "    )\n",
        "    ax3.set_title(\"Distance Distribution by Outcome\")\n",
        "    ax3.set_xlabel(\"\")\n",
        "    ax3.set_ylabel(\"Distance (yards)\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if savefig:\n",
        "        fig.savefig(savefig, dpi=150, bbox_inches=\"tight\")\n",
        "\n",
        "    # 🆕 Print bucketed success rates\n",
        "    print(\"\\nSuccess rates by distance range:\")\n",
        "    for lo, hi, label in buckets:\n",
        "        mask = (df[\"attempt_yards\"] >= lo) & (df[\"attempt_yards\"] <= hi)\n",
        "        if mask.any():\n",
        "            rate = df.loc[mask, \"success\"].mean()\n",
        "            print(f\"{label}: {rate:.1%} ({mask.sum():,} attempts)\")\n",
        "\n",
        "    return summary, fig\n",
        "\n",
        "\n",
        "def temporal_analysis(\n",
        "    df: pd.DataFrame,\n",
        "    *,\n",
        "    savefig: Path | None = None,\n",
        ") -> Tuple[pd.DataFrame, plt.Figure]:\n",
        "    \"\"\"Season trend, week quartiles, age histogram, age-group prints.\"\"\"\n",
        "    season_df = (\n",
        "        df.groupby(\"season\")\n",
        "        .agg(success_rate=(\"success\", \"mean\"),\n",
        "             total_attempts=(\"success\", \"size\"),\n",
        "             avg_distance=(\"attempt_yards\", \"mean\"))\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    # Ensure age feature exists\n",
        "    if \"age_at_attempt\" not in df.columns:\n",
        "        if {\"birthdate\", \"game_date\"}.issubset(df.columns):\n",
        "            df = df.copy()\n",
        "            df[\"age_at_attempt\"] = (df[\"game_date\"] - df[\"birthdate\"]).dt.days / 365.25\n",
        "\n",
        "    # 🆕 week-level success print\n",
        "    week_trends = df.groupby(\"week\")[\"success\"].mean()\n",
        "    print(\"\\nSuccess rate by season-quarter weeks:\")\n",
        "    quarters = {\n",
        "        \"Weeks 1-4\": week_trends.loc[1:4].mean(),\n",
        "        \"Weeks 5-8\": week_trends.loc[5:8].mean(),\n",
        "        \"Weeks 9-12\": week_trends.loc[9:12].mean(),\n",
        "        \"Weeks 13-16\": week_trends.loc[13:16].mean(),\n",
        "    }\n",
        "    for k, v in quarters.items():\n",
        "        print(f\"{k}: {v:.1%}\")\n",
        "\n",
        "    # 🆕 age-group print\n",
        "    age_bins = [(0, 25), (25, 30), (30, 35), (35, 45)]\n",
        "    print(\"\\nSuccess rate by age group:\")\n",
        "    for lo, hi in age_bins:\n",
        "        grp = df[(df[\"age_at_attempt\"] >= lo) & (df[\"age_at_attempt\"] < hi)]\n",
        "        if grp.empty:\n",
        "            continue\n",
        "        print(f\"{lo:>2}-{hi:<2}: {grp['success'].mean():.1%} \"\n",
        "              f\"({len(grp):,} attempts, avg {grp['attempt_yards'].mean():.1f} yds)\")\n",
        "\n",
        "    # -------- figures (unchanged layout) --------\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "    ax1.plot(season_df[\"season\"], season_df[\"success_rate\"],\n",
        "             marker=\"o\", linewidth=2)\n",
        "    ax1.set_title(\"Field-Goal Success Rate by Season\")\n",
        "    ax1.set_ylabel(\"Success Rate\")\n",
        "\n",
        "    ax2.plot(season_df[\"season\"], season_df[\"avg_distance\"],\n",
        "             marker=\"s\", color=\"orange\", linewidth=2)\n",
        "    ax2.set_title(\"Average Distance by Season\")\n",
        "    ax2.set_ylabel(\"Distance (yards)\")\n",
        "\n",
        "    sns.histplot(df[\"age_at_attempt\"].dropna(), bins=20, edgecolor=\"black\",\n",
        "                 color=\"green\", ax=ax3)\n",
        "    ax3.axvline(df[\"age_at_attempt\"].mean(), color=\"red\", linestyle=\"--\", label=\"Mean\")\n",
        "    ax3.set_title(\"Distribution of Kicker Ages at Attempt\")\n",
        "    ax3.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if savefig:\n",
        "        fig.savefig(savefig, dpi=150, bbox_inches=\"tight\")\n",
        "\n",
        "    return season_df, fig\n",
        "\n",
        "\n",
        "\n",
        "def kicker_performance_analysis(\n",
        "    df: pd.DataFrame,\n",
        "    *,\n",
        "    min_attempts: int = 20,\n",
        "    savefig: Path | None = None,\n",
        ") -> Tuple[pd.DataFrame, plt.Figure]:\n",
        "    \"\"\"Per‑kicker stats + four‑plot dashboard (Section 5 visuals).\"\"\"\n",
        "    stats_df = (\n",
        "        df.groupby([\"player_name\", \"player_id\"])\n",
        "        .agg(\n",
        "            total_attempts=(\"success\", \"size\"),\n",
        "            made=(\"success\", \"sum\"),\n",
        "            success_rate=(\"success\", \"mean\"),\n",
        "            avg_distance=(\"attempt_yards\", \"mean\"),\n",
        "            min_distance=(\"attempt_yards\", \"min\"),\n",
        "            max_distance=(\"attempt_yards\", \"max\"),\n",
        "        )\n",
        "        .reset_index()\n",
        "    )\n",
        "\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "    sns.histplot(stats_df[\"total_attempts\"], bins=20, edgecolor=\"black\", ax=axes[0, 0], color=\"lightgreen\")\n",
        "    axes[0, 0].axvline(stats_df[\"total_attempts\"].median(), color=\"red\", linestyle=\"--\", label=\"Median\")\n",
        "    axes[0, 0].set_title(\"Distribution of Attempts per Kicker\")\n",
        "    axes[0, 0].legend()\n",
        "\n",
        "    experienced = stats_df.query(\"total_attempts >= @min_attempts\")\n",
        "    sns.histplot(experienced[\"success_rate\"], bins=15, edgecolor=\"black\", ax=axes[0, 1], color=\"lightcoral\")\n",
        "    axes[0, 1].axvline(experienced[\"success_rate\"].median(), color=\"red\", linestyle=\"--\", label=\"Median\")\n",
        "    axes[0, 1].set_title(f\"Success Rate Distribution (≥{min_attempts} attempts)\")\n",
        "    axes[0, 1].legend()\n",
        "\n",
        "    axes[1, 0].scatter(stats_df[\"total_attempts\"], stats_df[\"success_rate\"], alpha=0.6, color=\"purple\")\n",
        "    z = np.polyfit(stats_df[\"total_attempts\"], stats_df[\"success_rate\"], 1)\n",
        "    axes[1, 0].plot(stats_df[\"total_attempts\"], np.poly1d(z)(stats_df[\"total_attempts\"]), \"r--\")\n",
        "    axes[1, 0].set_title(\"Success Rate vs Total Attempts\")\n",
        "    axes[1, 0].set_xlabel(\"Total Attempts\")\n",
        "    axes[1, 0].set_ylabel(\"Success Rate\")\n",
        "\n",
        "    bubble = experienced[\"total_attempts\"] / 5\n",
        "    axes[1, 1].scatter(experienced[\"avg_distance\"], experienced[\"success_rate\"], s=bubble, alpha=0.6, color=\"orange\")\n",
        "    axes[1, 1].set_title(\"Success Rate vs Average Distance (bubble = attempts)\")\n",
        "    axes[1, 1].set_xlabel(\"Average Attempt Distance (yards)\")\n",
        "    axes[1, 1].set_ylabel(\"Success Rate\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if savefig:\n",
        "        fig.savefig(savefig, dpi=150, bbox_inches=\"tight\")\n",
        "    return stats_df, fig\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def feature_engineering(df: pd.DataFrame, savefig: Path | None = None) -> plt.Figure:\n",
        "    \"\"\"Correlation heatmap of engineered numeric variables (Section 7 visuals).\"\"\"\n",
        "    numeric_cols = [\n",
        "        \"attempt_yards\",\n",
        "        \"distance_squared\",\n",
        "        \"season\",\n",
        "        \"week\",\n",
        "        \"age_at_attempt\",\n",
        "        \"kicker_attempt_number\",\n",
        "        \"success\",\n",
        "    ]\n",
        "    corr = df[numeric_cols].corr()\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\", ax=ax)\n",
        "    ax.set_title(\"Feature Correlation Matrix\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    if savefig:\n",
        "        fig.savefig(savefig, dpi=150, bbox_inches=\"tight\")\n",
        "    return fig\n",
        "\n",
        "# ───────────────────────── sanity guards ─────────────────────────\n",
        "# ───────────────────────── utils ──────────────────────────\n",
        "def _loess_smooth(x: np.ndarray, y: np.ndarray, frac: float = 0.25) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Lightweight LOESS smoother used only for sanity checking.\n",
        "    Falls back to a centred 3-point rolling mean if statsmodels isn’t present.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from statsmodels.nonparametric.smoothers_lowess import lowess\n",
        "        return lowess(y, x, frac=frac, return_sorted=False)\n",
        "    except ImportError:\n",
        "        return pd.Series(y).rolling(3, center=True, min_periods=1).mean().values\n",
        "\n",
        "\n",
        "# ─────────────────────── sanity guards ────────────────────\n",
        "def quick_sanity_checks(\n",
        "    df: pd.DataFrame,\n",
        "    *,\n",
        "    tol: float = 0.04,\n",
        "    min_count: int = 5,\n",
        "    check_monotonic: bool = False,\n",
        "    verbose: bool = False,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Fast data-quality assertions.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    tol : float\n",
        "        Max allowed jump in success rate between **smoothed** adjacent yardages.\n",
        "    min_count : int\n",
        "        Minimum attempt count to include a yardage in the check.\n",
        "    check_monotonic : bool\n",
        "        If True, raise on monotonicity violations; otherwise only log warnings.\n",
        "    verbose : bool\n",
        "        Print full list of violations.\n",
        "    \"\"\"\n",
        "    # 1. Duplicates\n",
        "    if df.duplicated().any():\n",
        "        raise AssertionError(\"Duplicate rows detected\")\n",
        "\n",
        "    # 2. Missing kicker names\n",
        "    n_missing = df[\"player_name\"].isna().sum()\n",
        "    if n_missing:\n",
        "        raise AssertionError(f\"Missing player_name in {n_missing} rows\")\n",
        "\n",
        "    # 3. Distance-success monotonicity  (optional)\n",
        "    grp = df.groupby(\"attempt_yards\")\n",
        "    counts = grp.size()\n",
        "    rates  = grp[\"success\"].mean()\n",
        "    rates  = rates[counts >= min_count].sort_index()\n",
        "    if rates.empty:\n",
        "        logger.warning(\"Monotonicity check skipped – no yardage meets min_count=%d\", min_count)\n",
        "        return\n",
        "\n",
        "    smooth = _loess_smooth(rates.index.values, rates.values)\n",
        "    deltas = np.diff(smooth)\n",
        "    bad_idx = np.where(np.abs(deltas) > tol)[0]  \n",
        "    if bad_idx.size:\n",
        "        yards = rates.index.values[1:][bad_idx]\n",
        "        if verbose or (check_monotonic and bad_idx.size < 20):\n",
        "            for y, d in zip(yards, deltas[bad_idx]):\n",
        "                logger.warning(\"Δ success@%dy = %+0.3f  (n=%d)\", y, d, counts[y])\n",
        "        msg = f\"Distance-success curve violations at {len(yards)} yardages; tol={tol:.2%}\"\n",
        "        if check_monotonic:\n",
        "            raise AssertionError(msg)\n",
        "        else:\n",
        "            logger.warning(msg + \"  – continuing because check_monotonic=False\")\n",
        "    else:\n",
        "        logger.info(\"Success-distance curve looks monotonic within ±%.1f %%\", tol*100)\n",
        "\n",
        "\n",
        "def player_activity_checks(\n",
        "    df: pd.DataFrame,\n",
        "    kickers_csv: Path | str,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Additional checks:\n",
        "      - Metadata kickers with zero attempts\n",
        "      - Player-seasons with attempts but zero makes\n",
        "      - Players with <2 seasons\n",
        "      - Players by years since last appearance\n",
        "    \"\"\"\n",
        "    kickers = pd.read_csv(kickers_csv, usecols=['player_id','player_name'])\n",
        "    all_ids = set(kickers['player_id'])\n",
        "    df_ids = set(df['player_id'].unique())\n",
        "    zero_attempts = sorted(all_ids - df_ids)\n",
        "    print(f\"\\nKickers with zero attempts: {len(zero_attempts)}\")\n",
        "    if zero_attempts:\n",
        "        names = kickers[kickers['player_id'].isin(zero_attempts)]['player_name'].tolist()\n",
        "        print(names)\n",
        "    ps = df.groupby(['player_id','player_name','season']).agg(\n",
        "        attempts=('success','size'), makes=('success','sum')\n",
        "    ).reset_index()\n",
        "    zero_makes = ps[(ps.attempts>0)&(ps.makes==0)]\n",
        "    print(f\"\\nPlayer-seasons with zero makes: {len(zero_makes)}\")\n",
        "    print(zero_makes[['player_name','season','attempts']].to_string(index=False))\n",
        "    season_counts = df.groupby(['player_id','player_name'])['season'].nunique().reset_index(name='season_count')\n",
        "    lt2 = season_counts[season_counts.season_count<2]\n",
        "    print(f\"\\nPlayers with <2 seasons: {len(lt2)}\")\n",
        "    print(lt2[['player_name','season_count']].to_string(index=False))\n",
        "    current_season = int(df['season'].max())\n",
        "    last = df.groupby(['player_id','player_name'])['season'].max().reset_index(name='last_season')\n",
        "    last['gap'] = current_season - last['last_season']\n",
        "    last['gap_group'] = last['gap'].apply(lambda g: str(g) if g<4 else '4+')\n",
        "    gap_counts = last['gap_group'].value_counts().sort_index(key=lambda x: [int(v.rstrip('+')) for v in x])\n",
        "    print(\"\\nPlayers by years since last appearance:\")\n",
        "    for grp,cnt in gap_counts.items():\n",
        "        print(f\"{grp} years: {cnt}\")\n",
        "\n",
        "\n",
        "# ───────────────────── orchestrator API ─────────────────────\n",
        "def run_full_eda(\n",
        "    kickers_csv: Path | str,\n",
        "    attempts_csv: Path | str,\n",
        "    *,\n",
        "    output_dir: Path | str = \"figures\",\n",
        "    include_preseason: bool = False,\n",
        "    max_distance: int | None = 63,\n",
        "    check_monotonic: bool = False,\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"Single convenience entry – replicates the entire notebook flow.\"\"\"\n",
        "    output_dir = Path(output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # 1) Load & merge raw data\n",
        "    print(\"── Section 1 Load & Merge ──\")\n",
        "    df_raw = load_and_merge(kickers_csv, attempts_csv)\n",
        "\n",
        "    # 2) Data‐quality & basic info\n",
        "    print(\"── Section 2 Data Quality & Basic Info ──\")\n",
        "    basic_overview(df_raw)\n",
        "\n",
        "    # 3) Prepare & engineer the dataset (adds 'success', drops Pre, filters distance, etc.)\n",
        "    print(\"── Section 3 Prepare Dataset ──\")\n",
        "    df = prepare_dataset(\n",
        "        df_raw,\n",
        "        include_preseason=include_preseason,\n",
        "        max_distance=max_distance,\n",
        "    )\n",
        "\n",
        "    print(\"── Section 3.1 Player Activity Checks ──\")\n",
        "    player_activity_checks(df, kickers_csv)\n",
        "    \n",
        "    # 4) Outcome analysis (now that df has 'success')\n",
        "    print(\"── Section 4 Outcome Analysis ──\")\n",
        "    outcome_summary(df, output_dir / \"outcomes.png\")\n",
        "\n",
        "    # 5) Distance‐success analysis\n",
        "    print(\"── Section 5 Distance Analysis ──\")\n",
        "    distance_analysis(df, savefig=output_dir / \"distance.png\")\n",
        "\n",
        "    # 6) Kicker performance dashboard\n",
        "    print(\"── Section 6 Kicker Performance ──\")\n",
        "    kicker_performance_analysis(df, savefig=output_dir / \"kicker_dashboard.png\")\n",
        "\n",
        "    # 7) Temporal factors\n",
        "    print(\"── Section 7 Temporal Factors ──\")\n",
        "    temporal_analysis(df, savefig=output_dir / \"temporal.png\")\n",
        "\n",
        "    # 8) Feature correlation\n",
        "    print(\"── Section 8 Feature Engineering ──\")\n",
        "    feature_engineering(df, savefig=output_dir / \"correlation.png\")\n",
        "\n",
        "    # 9) Final sanity checks\n",
        "    print(\"── Section 9 Sanity Checks ──\")\n",
        "    quick_sanity_checks(df, check_monotonic=check_monotonic)\n",
        "\n",
        "    logger.info(\"All figures saved in %s\", output_dir.resolve())\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "# ─────────────────────────── CLI demo ───────────────────────────\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Paths configurable via env or CLI args as needed\n",
        "    KICKERS = Path(\"data/raw/kickers.csv\")\n",
        "    ATTEMPTS = Path(\"data/raw/field_goal_attempts.csv\")\n",
        "\n",
        "    # End‑to‑end run replicating the notebook defaults\n",
        "    df_model = run_full_eda(KICKERS, ATTEMPTS)\n",
        "\n",
        "    out = Path(\"data/processed/field_goal_modeling_data.csv\")\n",
        "    out.parent.mkdir(parents=True, exist_ok=True)\n",
        "    df_model.to_csv(out, index=False)\n",
        "    logger.info(\"Processed dataset saved → %s (%s rows)\", out, len(df_model))\n",
        "\n",
        "\n",
        "    print(\"=\"*80)\n",
        "    print(\"COMPREHENSIVE EDA SUMMARY AND MODELING RECOMMENDATIONS\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    print(f\"\"\"\n",
        "     PROBLEM DEFINITION\n",
        "    • Binary classification problem: Predict field goal success (Made vs Missed/Blocked)\n",
        "    • Target distribution: {df_model['success'].mean():.1%} success rate (manageable imbalance)\n",
        "    • Dataset: {len(df_model):,} regular season field goal attempts (2010-2018)\n",
        "\n",
        "     KEY FINDINGS\n",
        "\n",
        "    1. DISTANCE IS THE DOMINANT FACTOR\n",
        "    • Strong negative correlation with success (-0.685)\n",
        "    • Non-linear relationship: ~99% success at 18-20 yards → ~0% at 60+ yards\n",
        "    • Success drops sharply after 50 yards (long range threshold)\n",
        "\n",
        "    2. KICKER DIFFERENCES ARE SIGNIFICANT\n",
        "    • {df_model['player_name'].nunique()} unique kickers with vastly different performance levels\n",
        "    • Raw success rates range from ~60% to ~95% among experienced kickers\n",
        "    • Sample sizes vary dramatically: 1 to 300+ attempts per kicker\n",
        "    • Clear evidence for kicker-specific modeling\n",
        "\n",
        "    3. TEMPORAL PATTERNS ARE MINIMAL\n",
        "    • Success rates stable across seasons (83-86%)\n",
        "    • No major trends in attempt difficulty over time\n",
        "    • Week and age effects are minor compared to distance and kicker skill\n",
        "\n",
        "    4. DATA QUALITY IS EXCELLENT\n",
        "    • No missing values in key variables\n",
        "    • Clean, well-structured data ready for modeling\n",
        "    • Minimal outliers (removed extreme distances >63 yards)\n",
        "\n",
        "     RECOMMENDED MODELING APPROACH\n",
        "\n",
        "    PRIMARY MODEL: Hierarchical Bayesian Logistic Regression\n",
        "    ✓ Handles varying sample sizes per kicker elegantly\n",
        "    ✓ Provides uncertainty quantification for ratings\n",
        "    ✓ Natural pooling of information across kickers\n",
        "    ✓ Logistic function matches distance-success relationship\n",
        "\n",
        "    MODEL SPECIFICATION:\n",
        "    success_ij ~ Bernoulli(p_ij)\n",
        "    logit(p_ij) = α_j + β * distance_ij\n",
        "    α_j ~ Normal(μ_α, σ_α)  [kicker random effects]\n",
        "\n",
        "    ALTERNATIVE MODELS for comparison:\n",
        "    • Regularized logistic regression (Ridge/Lasso)\n",
        "    • Random Forest (for non-linear interactions)\n",
        "    • XGBoost (gradient boosting)\n",
        "\n",
        "     FEATURE ENGINEERING RECOMMENDATIONS\n",
        "\n",
        "    ESSENTIAL FEATURES:\n",
        "    • attempt_yards (primary predictor)\n",
        "    • player_name/player_id (kicker identity)\n",
        "\n",
        "    POTENTIAL ENHANCEMENTS:\n",
        "    • distance_squared (for non-linearity)\n",
        "    • is_long_attempt (50+ yard flag)\n",
        "    • kicker_attempt_number (experience effect)\n",
        "    • season trends (if needed)\n",
        "\n",
        "    EVALUATION STRATEGY\n",
        "\n",
        "    METRICS:\n",
        "    • Brier Score (calibration of probabilities)\n",
        "    • Log Loss (proper scoring rule)\n",
        "    • AUC-ROC (discrimination ability)\n",
        "    • Custom: Expected Points Added per attempt\n",
        "\n",
        "    VALIDATION:\n",
        "    • Time-based split (train: 2010-2017, test: 2018)\n",
        "    • Cross-validation with kicker groups\n",
        "    • Out-of-sample kicker prediction (new kickers)\n",
        "\n",
        "     EXPECTED OUTCOMES\n",
        "\n",
        "    The model will enable:\n",
        "    • Accurate field goal success probability prediction\n",
        "    • Fair kicker evaluation accounting for attempt difficulty\n",
        "    • Expected points calculation for strategic decisions\n",
        "    • Identification of clutch vs. situational performance\n",
        "\n",
        "    \"\"\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Configuration Module\n",
        "\n",
        "Central configuration for the entire package.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/nfl_kicker_analysis/data/feature_schema.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/nfl_kicker_analysis/data/feature_schema.py\n",
        "\"\"\"\n",
        "FeatureSchema – canonical column lists for preprocessing & modelling.\n",
        "\"\"\"\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class FeatureSchema:\n",
        "    \"\"\"Container class listing every column by semantic type.\"\"\"\n",
        "    numerical: List[str] = field(default_factory=list)\n",
        "    binary:    List[str] = field(default_factory=list)      # 0/1 ints\n",
        "    ordinal:   List[str] = field(default_factory=list)      # ordered integers\n",
        "    nominal:   List[str] = field(default_factory=list)      # unordered cats / high-card\n",
        "    target:    str        = \"success\"\n",
        "\n",
        "    # ───── convenience helpers ────────────────────────────────────\n",
        "    @property\n",
        "    def model_features(self) -> List[str]:\n",
        "        \"\"\"All predictors in modelling order.\"\"\"\n",
        "        return self.numerical + self.binary + self.ordinal + self.nominal\n",
        "\n",
        "    def assert_in_dataframe(self, df) -> None:\n",
        "        \"\"\"Raise if any declared column is missing from df.columns.\"\"\"\n",
        "        missing = [c for c in self.model_features + [self.target]\n",
        "                   if c not in df.columns]\n",
        "        if missing:\n",
        "            raise ValueError(f\"FeatureSchema mismatch – missing cols: {missing}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the feature schema\n",
        "    schema = FeatureSchema()\n",
        "    # sample features\n",
        "    FeatureSchema.numerical = ['distance', 'weather_condition', 'time_of_day']\n",
        "    FeatureSchema.binary = ['is_home', 'is_playoff']\n",
        "    FeatureSchema.ordinal = ['weather_temperature']\n",
        "    FeatureSchema.nominal = ['weather_type', 'team_name']\n",
        "    FeatureSchema.target = 'success'\n",
        "    \n",
        "    print(schema.model_features)\n",
        "    print(schema.assert_in_dataframe(pd.DataFrame()))\n",
        "    print(\"******* FeatureSchema tests passed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/nfl_kicker_analysis/__init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/nfl_kicker_analysis/__init__.py\n",
        "# This file is empty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/nfl_kicker_analysis/config.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/nfl_kicker_analysis/config.py\n",
        "\"\"\"\n",
        "Configuration module for NFL Kicker Analysis package.\n",
        "Contains all constants, paths, and configuration parameters.\n",
        "\"\"\"\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Tuple\n",
        "import os\n",
        "\n",
        "class Config:\n",
        "    \"\"\"Main configuration class for the NFL Kicker Analysis package.\"\"\"\n",
        "    MLFLOW_EXPERIMENT_NAME=\"nfl_kicker_analysis\"\n",
        "    \n",
        "    # Base paths - use relative paths that work in both local and cloud environments\n",
        "    # Get the project root by going up from this config file location\n",
        "    _CONFIG_DIR = Path(__file__).parent.parent.parent  # Go up to project root\n",
        "    PROJECT_ROOT = _CONFIG_DIR.resolve()\n",
        "    \n",
        "    DATA_DIR = PROJECT_ROOT / \"data\"\n",
        "    RAW_DATA_DIR = DATA_DIR / \"raw\"\n",
        "    PROCESSED_DATA_DIR = DATA_DIR / \"processed\"\n",
        "    OUTPUT_DIR = PROJECT_ROOT / \"output\"\n",
        "    MODELS_DIR = PROJECT_ROOT / \"models\"  # Bayesian models directory\n",
        "    MODEL_DIR = MODELS_DIR / \"bayesian\"   # Specific directory for Bayesian models\n",
        "    POINT_ESTIMATE_DIR = MODELS_DIR / \"mlruns\" / \"models\"  # Point estimate models go here\n",
        "    \n",
        "    \n",
        "    # Raw data files\n",
        "    KICKERS_FILE = RAW_DATA_DIR / \"kickers.csv\"\n",
        "    ATTEMPTS_FILE = RAW_DATA_DIR / \"field_goal_attempts.csv\"\n",
        "    \n",
        "    # Processed data files\n",
        "    MODELING_DATA_FILE = PROCESSED_DATA_DIR / \"field_goal_modeling_data.csv\"\n",
        "    LEADERBOARD_FILE = OUTPUT_DIR / \"leaderboard.csv\"\n",
        "    MODEL_DATA_FILE: Path = OUTPUT_DIR / \"bayesian_features.csv\"\n",
        "    \n",
        "    # Analysis parameters\n",
        "    MIN_DISTANCE = 20\n",
        "    min_distance = 20\n",
        "    MAX_DISTANCE = 60\n",
        "    MIN_KICKER_ATTEMPTS = 10  # Changed from 8 to 5 to match Method B\n",
        "    \n",
        "    # Distance profile for EPA calculation\n",
        "    DISTANCE_PROFILE = [20, 25, 30, 35, 40, 45, 50, 55, 60]\n",
        "    DISTANCE_WEIGHTS = [0.05, 0.10, 0.20, 0.20, 0.20, 0.15, 0.08, 0.02, 0.01]\n",
        "    \n",
        "    # Distance ranges for analysis\n",
        "    DISTANCE_RANGES = [\n",
        "        (18, 29, \"Short (18-29 yards)\"),\n",
        "        (30, 39, \"Medium-Short (30-39 yards)\"),\n",
        "        (40, 49, \"Medium (40-49 yards)\"),\n",
        "        (50, 59, \"Long (50-59 yards)\"),\n",
        "        (60, 75, \"Extreme (60+ yards)\")\n",
        "    ]\n",
        "    \n",
        "    # Model parameters\n",
        "    BAYESIAN_MCMC_SAMPLES = 2_000\n",
        "    BAYESIAN_TUNE = 1_000\n",
        "    BAYESIAN_CHAINS = 2\n",
        "    \n",
        "    # Rating thresholds\n",
        "    ELITE_THRESHOLD = 0.15\n",
        "    STRUGGLING_THRESHOLD = -0.20\n",
        "    \n",
        "    # Visualization settings\n",
        "    FIGURE_SIZE = (12, 8)\n",
        "    DPI = 100\n",
        "    \n",
        "    # Season types to include\n",
        "    SEASON_TYPES = ['Reg', 'Post']  # Include both regular season and playoffs\n",
        "    \n",
        "    # ─── Feature flags ───────────────────────────────────────────\n",
        "    FILTER_RETIRED_INJURED = True   # keep everyone by default\n",
        "    metrics = {\"accuracy\": 0.85, \"f1\": 0.82}\n",
        "    \n",
        "    # 16 game seasons\n",
        "    PRE_2021_SEASON_GAMES = 16\n",
        "    POST_2021_SEASON_GAMES = 17\n",
        "    YEAR_GAMES_INCREASED = 2021\n",
        "    \n",
        "    # ── NEW: maximum gap (in games) since last kick for Bayesian defaults\n",
        "    MAX_GAMES_SINCE_LAST_KICK: int | None = None\n",
        "\n",
        "    # How many Optuna trials for each tree-based model\n",
        "    OPTUNA_TRIALS: Dict[str, int] = {\n",
        "        \"simple_logistic\": 50,\n",
        "        \"ridge_logistic\": 50,\n",
        "        \"random_forest\": 50,\n",
        "        \"xgboost\": 50,\n",
        "        \"catboost\": 50\n",
        "    }\n",
        "\n",
        "    @classmethod\n",
        "    def ensure_directories(cls):\n",
        "        \"\"\"Create all required directories if they don't exist.\"\"\"\n",
        "        for dir_path in [cls.RAW_DATA_DIR, cls.PROCESSED_DATA_DIR, \n",
        "                        cls.OUTPUT_DIR, cls.MODELS_DIR]:\n",
        "            dir_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Create global config instance\n",
        "config = Config()\n",
        "\n",
        "# ───────────────────────── Feature catalogue ─────────────────────────\n",
        "# Single source of truth for column roles – centralized from all modules\n",
        "FEATURE_LISTS: Dict[str, List[str]] = {\n",
        "    \"numerical\": [\n",
        "        \"attempt_yards\", \"age_at_attempt\", \"distance_squared\",\n",
        "        \"career_length_years\", \"season_progress\", \"exp_100\", \n",
        "        \"distance_zscore\", \"distance_percentile\",\n",
        "        \"seasons_of_experience\", \"career_year\",\n",
        "        \"age_c\", \"age_c2\", \n",
        "        \"importance\", \"days_since_last_kick\",\n",
        "        \"age_dist_interact\", \"exp_dist_interact\", \n",
        "    ],\n",
        "    \"ordinal\": [\"season\", \"week\", \"month\", \"day_of_year\"],\n",
        "    \"nominal\": [\n",
        "        \"kicker_id\", \"kicker_idx\",\n",
        "        \"is_long_attempt\", \"is_very_long_attempt\",\n",
        "        \"is_rookie_attempt\", \"distance_category\", \"experience_category\",\n",
        "        \"is_early_season\", \"is_late_season\", \"is_playoffs\",\n",
        "    ],\n",
        "    \"y_variable\": [\"success\"],\n",
        "}\n",
        "\n",
        "# Attach FEATURE_LISTS onto the config instance for ease of use\n",
        "config.FEATURE_LISTS = FEATURE_LISTS\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the configuration\n",
        "    print(\"NFL Kicker Analysis Configuration\")\n",
        "    print(\"=\" * 40)\n",
        "    print(f\"Data directory: {config.DATA_DIR}\")\n",
        "    print(f\"Min distance: {config.MIN_DISTANCE}\")\n",
        "    print(f\"Max distance: {config.MAX_DISTANCE}\")\n",
        "    print(f\"Distance profile: {config.DISTANCE_PROFILE}\")\n",
        "    print(f\"Elite threshold: {config.ELITE_THRESHOLD}\")\n",
        "    \n",
        "    # Test directory creation\n",
        "    config.ensure_directories()\n",
        "    print(\"******* Configuration loaded and directories created!\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Loading Module\n",
        "\n",
        "Handles loading and merging of raw kicker data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/nfl_kicker_analysis/data/loader.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/nfl_kicker_analysis/data/loader.py\n",
        "\"\"\"\n",
        "Data loading module for NFL kicker analysis.\n",
        "Handles loading and merging of raw datasets.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from typing import Optional, Tuple\n",
        "import warnings\n",
        "from pandas.tseries.offsets import DateOffset\n",
        "\n",
        "from src.nfl_kicker_analysis.config import config\n",
        "\n",
        "class DataLoader:\n",
        "    \"\"\"Handles loading and initial merging of kicker datasets.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the data loader.\"\"\"\n",
        "        self.kickers_df = None\n",
        "        self.attempts_df = None\n",
        "        self.merged_df = None\n",
        "    \n",
        "    def load_kickers(self, filepath: Optional[Path] = None) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load kicker metadata.\n",
        "        \n",
        "        Args:\n",
        "            filepath: Optional path to kickers CSV file\n",
        "            \n",
        "        Returns:\n",
        "            DataFrame with kicker information\n",
        "        \"\"\"\n",
        "        if filepath is None:\n",
        "            filepath = config.KICKERS_FILE\n",
        "            \n",
        "        try:\n",
        "            self.kickers_df = pd.read_csv(filepath)\n",
        "            print(f\"******* Loaded {len(self.kickers_df)} kickers from {filepath}\")\n",
        "            return self.kickers_df\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"Kickers data file not found: {filepath}\")\n",
        "    \n",
        "    def load_attempts(self, filepath: Optional[Path] = None) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load field goal attempts data.\n",
        "        \n",
        "        Args:\n",
        "            filepath: Optional path to attempts CSV file\n",
        "            \n",
        "        Returns:\n",
        "            DataFrame with field goal attempt information\n",
        "        \"\"\"\n",
        "        if filepath is None:\n",
        "            filepath = config.ATTEMPTS_FILE\n",
        "            \n",
        "        try:\n",
        "            self.attempts_df = pd.read_csv(filepath)\n",
        "            print(f\"******* Loaded {len(self.attempts_df)} field goal attempts from {filepath}\")\n",
        "            return self.attempts_df\n",
        "        except FileNotFoundError:\n",
        "            raise FileNotFoundError(f\"Attempts data file not found: {filepath}\")\n",
        "    \n",
        "    def merge_datasets(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Merge kickers and attempts datasets and drop preseason games.\n",
        "        \n",
        "        Returns:\n",
        "            Merged DataFrame with kicker names attached to attempts, excluding preseason\n",
        "        \"\"\"\n",
        "        if self.kickers_df is None:\n",
        "            self.load_kickers()\n",
        "        if self.attempts_df is None:\n",
        "            self.load_attempts()\n",
        "            \n",
        "        # Merge on player_id\n",
        "        self.merged_df = pd.merge(\n",
        "            self.attempts_df,\n",
        "            self.kickers_df,\n",
        "            on='player_id',\n",
        "            how='left'\n",
        "        )\n",
        "        \n",
        "        # ── 3️⃣ Binary success target & drop kickers with zero makes ever ──\n",
        "        self.merged_df['success'] = (self.merged_df['field_goal_result'] == 'Made').astype(int)\n",
        "        makes = self.merged_df.groupby('player_id')['success'].sum()\n",
        "        zero_ids = makes[makes == 0].index\n",
        "        if len(zero_ids):\n",
        "            print(f\"🗑️  Removing {len(zero_ids)} kickers with zero makes ever\")\n",
        "            self.merged_df = self.merged_df.loc[~self.merged_df['player_id'].isin(zero_ids)].copy()\n",
        "            \n",
        "        # Drop preseason attempts if present\n",
        "        if 'season_type' in self.merged_df.columns:\n",
        "            preseason_mask = self.merged_df['season_type'] == 'Pre'\n",
        "            num_preseason = preseason_mask.sum()\n",
        "            if num_preseason > 0:\n",
        "                print(f\"🗑️  Filtered out {num_preseason} preseason attempts\")\n",
        "            # keep everything that is not Pre-season\n",
        "            self.merged_df = self.merged_df.loc[~preseason_mask].copy()\n",
        "\n",
        "        # ── 4️⃣ Filter out players with no games in the last 2 years ─────────────────\n",
        "        # ensure game_date is datetime\n",
        "        self.merged_df['game_date'] = pd.to_datetime(self.merged_df['game_date'])\n",
        "        max_date = self.merged_df['game_date'].max()\n",
        "        cutoff = max_date - DateOffset(years=2)\n",
        "\n",
        "        # players with any game in the last 2 years\n",
        "        recent = set(self.merged_df.loc[self.merged_df['game_date'] >= cutoff, 'player_id'])\n",
        "        # all players\n",
        "        all_ids = set(self.merged_df['player_id'])\n",
        "        # players with no recent games\n",
        "        inactive = all_ids - recent\n",
        "\n",
        "        # ── 5️⃣ For those inactive players, require ≥10 games outside their last 16 ─────────────────\n",
        "        keep_ids = set(recent)\n",
        "        for pid in inactive:\n",
        "            # get this player's games sorted newest→oldest\n",
        "            pg = (self.merged_df[self.merged_df['player_id'] == pid]\n",
        "                  .sort_values('game_date', ascending=False))\n",
        "            # drop their 16 most‐recent games\n",
        "            older = pg.iloc[16:] if len(pg) > 16 else pg.iloc[0:0]\n",
        "            if len(older) >= 10:\n",
        "                keep_ids.add(pid)\n",
        "\n",
        "        # apply the combined filter\n",
        "        before = len(self.merged_df)\n",
        "        self.merged_df = self.merged_df[self.merged_df['player_id'].isin(keep_ids)].copy()\n",
        "        after = len(self.merged_df)\n",
        "        print(f\"🗂️  Filtered out {before - after} attempts due to recency/experience rules\")\n",
        "        \n",
        "\n",
        "        # Validate merge\n",
        "        missing_kickers = self.merged_df['player_name'].isnull().sum()\n",
        "        if missing_kickers > 0:\n",
        "            warnings.warn(f\"Found {missing_kickers} attempts with missing kicker info\")\n",
        "\n",
        "        print(f\"******* Merged dataset: {self.merged_df.shape[0]} attempts, {self.merged_df.shape[1]} columns\")\n",
        "        return self.merged_df\n",
        "\n",
        "    \n",
        "    def load_complete_dataset(self) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Load and merge complete dataset in one call.\n",
        "        \n",
        "        Returns:\n",
        "            Complete merged DataFrame\n",
        "        \"\"\"\n",
        "        self.load_kickers()\n",
        "        self.load_attempts()\n",
        "        return self.merge_datasets()\n",
        "    \n",
        "    def get_data_summary(self) -> dict:\n",
        "        \"\"\"\n",
        "        Get summary statistics of loaded data.\n",
        "        \n",
        "        Returns:\n",
        "            Dictionary with data summary information\n",
        "        \"\"\"\n",
        "        if self.merged_df is None:\n",
        "            raise ValueError(\"No data loaded. Call load_complete_dataset() first.\")\n",
        "            \n",
        "        summary = {\n",
        "            'total_attempts': len(self.merged_df),\n",
        "            'unique_kickers': self.merged_df['player_name'].nunique(),\n",
        "            'unique_seasons': sorted(self.merged_df['season'].unique()),\n",
        "            'season_types': self.merged_df['season_type'].unique().tolist(),\n",
        "            'outcome_counts': self.merged_df['field_goal_result'].value_counts().to_dict(),\n",
        "            'date_range': (\n",
        "                self.merged_df['game_date'].min(),\n",
        "                self.merged_df['game_date'].max()\n",
        "            ),\n",
        "            'distance_range': (\n",
        "                self.merged_df['attempt_yards'].min(),\n",
        "                self.merged_df['attempt_yards'].max()\n",
        "            )\n",
        "        }\n",
        "        return summary\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test the data loader\n",
        "    print(\"Testing DataLoader...\")\n",
        "    \n",
        "    loader = DataLoader()\n",
        "    \n",
        "    try:\n",
        "        # Load complete dataset\n",
        "        df = loader.load_complete_dataset()\n",
        "        print(\"---------------head-----------------\")\n",
        "        print(df.head())\n",
        "        print(\"---------------columns-----------------\")\n",
        "        print(df.columns)\n",
        "        \n",
        "        # Print summary\n",
        "        summary = loader.get_data_summary()\n",
        "        print(\"\\nData Summary:\")\n",
        "        print(summary)\n",
        "        print(f\"Total attempts: {summary['total_attempts']:,}\")\n",
        "        print(f\"Unique kickers: {summary['unique_kickers']}\")\n",
        "        print(f\"season_types: {summary['season_types']}\")\n",
        "        print(f\"Seasons: {summary['unique_seasons']}\")\n",
        "        print(f\"Outcomes: {summary['outcome_counts']}\")\n",
        "        \n",
        "        print(\"******* DataLoader tests passed!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"------------- Error testing DataLoader: {e}\")\n",
        "        print(\"Note: This is expected if data files are not present.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/nfl_kicker_analysis/data/feature_engineering.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/nfl_kicker_analysis/data/feature_engineering.py\n",
        "\"\"\"\n",
        "Feature engineering module for NFL kicker analysis.\n",
        "Contains all feature creation functions that can be used to build features for modeling.\n",
        "\"\"\"\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Dict, Union, List\n",
        "from pandas.tseries.offsets import DateOffset\n",
        "\n",
        "# Required columns for EPA calculation\n",
        "REQUIRED_EPA_COLS = {\"age_at_attempt\", \"exp_100\", \"player_status\"}\n",
        "\n",
        "def ensure_epa_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Guarantee that df contains every column in REQUIRED_EPA_COLS.\n",
        "    If any are missing, run the minimal FeatureEngineer step that\n",
        "    creates just that column and merge the result back.\n",
        "    Never fills with dummy values.\n",
        "    \"\"\"\n",
        "    eng = FeatureEngineer()          # lightweight, stateless\n",
        "    missing = REQUIRED_EPA_COLS.difference(df.columns)\n",
        "\n",
        "    if \"age_at_attempt\" in missing:\n",
        "        df = eng.create_date_features(df)\n",
        "        missing -= {\"age_at_attempt\"}\n",
        "\n",
        "    if {\"exp_100\"}.issubset(missing):           # experience needs ordering\n",
        "        df = eng.create_experience_features(df)\n",
        "        missing -= {\"exp_100\"}\n",
        "\n",
        "    if \"player_status\" in missing:\n",
        "        df = eng.create_player_status_features(df)\n",
        "        missing -= {\"player_status\"}\n",
        "\n",
        "    if missing:      # still not resolved ➜ stop early\n",
        "        raise KeyError(f\"Unable to generate columns: {sorted(missing)}\")\n",
        "\n",
        "    return df\n",
        "\n",
        "class FeatureEngineer:\n",
        "    \"\"\"Handles all feature engineering operations.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the feature engineer.\"\"\"\n",
        "        self.kicker_mapping = None\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # Target / basic temporal features\n",
        "    # ---------------------------------------------------------------------\n",
        "    def create_target_variable(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create binary success target variable.\"\"\"\n",
        "        df = df.copy()\n",
        "        df[\"success\"] = (df[\"field_goal_result\"] == \"Made\").astype(int)\n",
        "        print(f\"******* Created target variable: {df['success'].mean():.1%} success rate\")\n",
        "        return df\n",
        "\n",
        "    def create_date_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Create date-related features including centered age variables.\n",
        "        \n",
        "        Adds:\n",
        "        • age_at_attempt     – raw age in years\n",
        "        • age_c              – centred (30 yrs) & scaled (÷10) age\n",
        "        • age_c2             – quadratic term (for simple aging curve)\n",
        "        \"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        # Ensure datetime dtypes\n",
        "        df[\"game_date\"] = pd.to_datetime(df[\"game_date\"])\n",
        "        df[\"birthdate\"] = pd.to_datetime(df[\"birthdate\"])\n",
        "\n",
        "        # Age at attempt\n",
        "        df[\"age_at_attempt\"] = (df[\"game_date\"] - df[\"birthdate\"]).dt.days / 365.25\n",
        "        \n",
        "        # Centered & scaled age (Gelman scaling)\n",
        "        df[\"age_c\"]  = (df[\"age_at_attempt\"] - 30.0) / 10.0\n",
        "        df[\"age_c2\"] = df[\"age_c\"] ** 2\n",
        "        \n",
        "        # Seasonal features\n",
        "        df[\"day_of_year\"] = df[\"game_date\"].dt.dayofyear\n",
        "        df[\"month\"] = df[\"game_date\"].dt.month\n",
        "        print(\"******* Created date features (age_c, age_c2, day_of_year, month)\")\n",
        "        return df\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # OPTIONAL – continuous spline basis for age (k = 3 knots)\n",
        "    # ------------------------------------------------------------------\n",
        "    def create_age_spline_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Add natural-cubic-spline basis age_spline_1…k (centered & scaled age).\n",
        "        Use when nonlinear aging curve is desired.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            import patsy as ps\n",
        "        except ImportError:\n",
        "            print(\"⚠️  patsy not available - skipping age spline features\")\n",
        "            return df\n",
        "            \n",
        "        df = df.copy()\n",
        "        # Design matrix returns ndarray with intercept, drop it\n",
        "        spline = ps.dmatrix(\"bs(age_c, df=3, degree=3, include_intercept=False)\",\n",
        "                            df, return_type=\"dataframe\")\n",
        "        # Rename columns nicely\n",
        "        spline.columns = [f\"age_spline_{i+1}\" for i in range(spline.shape[1])]\n",
        "        df = pd.concat([df, spline], axis=1)\n",
        "        print(\"******* Created age spline features\")\n",
        "        return df\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Identifier mapping\n",
        "    # ------------------------------------------------------------------\n",
        "    def create_kicker_mapping(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Adds two columns:\n",
        "          • kicker_id   – raw player_id from the source (GSIS / Stathead)\n",
        "          • kicker_idx  – zero-based contiguous index used ONLY for matrix ops\n",
        "        The mapping is cached so that train/test splits share the same indices.\n",
        "        Also preserves player_name for human-readable analysis.\n",
        "        \"\"\"\n",
        "        df = df.copy()\n",
        "\n",
        "        df[\"kicker_id\"] = df[\"player_id\"].astype(int)        # ← raw, never mutates\n",
        "        if self.kicker_mapping is None:\n",
        "            unique = df[\"kicker_id\"].unique()\n",
        "            self.kicker_mapping = {pid: i for i, pid in enumerate(sorted(unique))}\n",
        "        df[\"kicker_idx\"] = df[\"kicker_id\"].map(self.kicker_mapping).astype(int)\n",
        "\n",
        "        # Ensure player_name is preserved for name-based operations\n",
        "        # This enables the hybrid approach recommended in the roadmap\n",
        "        if \"player_name\" not in df.columns:\n",
        "            print(\"⚠️  WARNING: player_name column not found. Name-based operations may not work.\")\n",
        "\n",
        "        print(f\"******* Created kicker mapping for {len(self.kicker_mapping)} unique kickers \"\n",
        "              f\"(raw_id→idx) with player names preserved\")\n",
        "        return df\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Distance related features\n",
        "    # ------------------------------------------------------------------\n",
        "    def create_distance_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Create distance-based engineered features, with safe log transform.\"\"\"\n",
        "        df = df.copy()\n",
        "        df[\"distance_squared\"] = df[\"attempt_yards\"] ** 2\n",
        "        df[\"distance_cubed\"]   = df[\"attempt_yards\"] ** 3\n",
        "        # Use log1p to handle zero-yard attempts without -inf :contentReference[oaicite:13]{index=13}\n",
        "        df[\"log_distance\"]     = np.log1p(df[\"attempt_yards\"])\n",
        "        df[\"is_long_attempt\"]  = (df[\"attempt_yards\"] >= 50).astype(int)\n",
        "        df[\"is_very_long_attempt\"] = (df[\"attempt_yards\"] >= 55).astype(int)\n",
        "        q1, q2, q3 = df[\"attempt_yards\"].quantile([0.25, 0.5, 0.75])\n",
        "        df[\"distance_category\"] = df[\"attempt_yards\"].apply(\n",
        "            lambda dist: \"Short\" if dist < q1\n",
        "                        else \"Medium-Short\" if dist < q2\n",
        "                        else \"Medium\" if dist < q3\n",
        "                        else \"Long\"\n",
        "        )\n",
        "        df[\"distance_from_sweet_spot\"] = (df[\"attempt_yards\"] - 35).abs()\n",
        "        print(\"******* Created distance features (poly, log, quantile categories, flags)\")\n",
        "        return df\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Experience features\n",
        "    # ------------------------------------------------------------------\n",
        "    def create_experience_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Adds cumulative-experience variables.\n",
        "\n",
        "        Columns created\n",
        "        ---------------\n",
        "        kicker_attempt_number : 1-indexed count (already used elsewhere)\n",
        "        experience_in_kicks   : 0-indexed before current kick\n",
        "        exp_100               : experience_in_kicks / 100  (for stable priors)\n",
        "        is_rookie_attempt     : 1 if first ≤20 attempts\n",
        "        experience_category   : Rookie / Developing / Veteran (10th & 25th pct)\n",
        "        career_length_years   : yrs since first attempt\n",
        "        \"\"\"\n",
        "        df = df.sort_values([\"player_id\", \"game_date\"]).copy()\n",
        "\n",
        "        # cumulative counts\n",
        "        df[\"kicker_attempt_number\"] = df.groupby(\"player_id\").cumcount() + 1\n",
        "        df[\"experience_in_kicks\"]   = df[\"kicker_attempt_number\"] - 1\n",
        "        df[\"exp_100\"]               = df[\"experience_in_kicks\"] / 100.0\n",
        "\n",
        "        # career length (years)\n",
        "        first_dates = df.groupby(\"player_id\")[\"game_date\"].transform(\"min\")\n",
        "        df[\"career_length_years\"] = (df[\"game_date\"] - first_dates).dt.days / 365.25\n",
        "\n",
        "        # simple buckets\n",
        "        df[\"is_rookie_attempt\"] = (df[\"kicker_attempt_number\"] <= 20).astype(int)\n",
        "        p10, p25 = df[\"kicker_attempt_number\"].quantile([0.1, 0.25])\n",
        "        df[\"experience_category\"] = df[\"kicker_attempt_number\"].apply(\n",
        "            lambda n: \"Rookie\" if n <= p10\n",
        "            else (\"Developing\" if n <= p25 else \"Veteran\")\n",
        "        )\n",
        "\n",
        "        print(\"******* Created experience features (exp_100 added)\")\n",
        "        return df\n",
        "    \n",
        "    # ------------------------------------------------------------------\n",
        "    # Situational features\n",
        "    # ------------------------------------------------------------------\n",
        "    def create_situational_features(\n",
        "            self,\n",
        "            df: pd.DataFrame,\n",
        "            *,\n",
        "            weight_cfg: dict | None = None\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Adds season / clutch flags *and* an 'importance' weight.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        weight_cfg : dict, optional\n",
        "            Keys: 'late', 'clutch', 'playoff'.\n",
        "            Defaults = {'late': 1, 'clutch': 2, 'playoff': 4}.\n",
        "        \"\"\"\n",
        "        w = {'late': 1, 'clutch': 2, 'playoff': 4}\n",
        "        if weight_cfg:\n",
        "            w.update(weight_cfg)\n",
        "\n",
        "        df = df.copy()\n",
        "        df[\"is_early_season\"] = (df[\"week\"] <= 4).astype(int)\n",
        "        df[\"is_late_season\"]  = (df[\"week\"] >= 14).astype(int)\n",
        "        df[\"is_playoffs\"]     = (df[\"week\"] >= 17).astype(int)\n",
        "        df[\"season_progress\"] = df[\"week\"] / 16.0\n",
        "\n",
        "        # # Optional clutch (leave at 0 if context cols absent)\n",
        "        # req = [\"quarter\", \"game_seconds_remaining\", \"score_differential\"]\n",
        "        # missing = [c for c in req if c not in df.columns]\n",
        "        # if missing:\n",
        "        #     df[\"is_clutch\"] = 0\n",
        "        # else:\n",
        "        #     df[\"is_clutch\"] = (\n",
        "        #         (df[\"quarter\"] >= 4) &\n",
        "        #         (df[\"game_seconds_remaining\"] <= 120) &\n",
        "        #         (df[\"score_differential\"].abs() <= 3)\n",
        "        #     ).astype(int)\n",
        "\n",
        "        # --- NEW flexible weighting ------------------------------------------\n",
        "        df[\"importance\"] = (\n",
        "            1\n",
        "            + w['late']     * df[\"is_late_season\"]\n",
        "            # + w['clutch']   * df[\"is_clutch\"]\n",
        "            + w['playoff']  * df[\"is_playoffs\"]\n",
        "        )\n",
        "        return df\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Rolling performance history\n",
        "    # ------------------------------------------------------------------\n",
        "    def create_performance_history_features(self, df: pd.DataFrame, window_size: int = 10) -> pd.DataFrame:\n",
        "        \"\"\"Rolling success rates, similar-distance success, and current streaks.\"\"\"\n",
        "        df = df.sort_values([\"player_id\", \"game_date\"]).copy()\n",
        "\n",
        "        # Rolling mean of success\n",
        "        df[\"rolling_success_rate\"] = (\n",
        "            df.groupby(\"player_id\")[\"success\"]\n",
        "            .transform(lambda s: s.rolling(window_size, min_periods=1).mean())\n",
        "        )\n",
        "\n",
        "        overall = df[\"success\"].mean()\n",
        "\n",
        "        def similar_rate(sub):\n",
        "            \"\"\"For each attempt, compute mean success rate of prior attempts within ±5 yards.\"\"\"\n",
        "            vals = []\n",
        "            for i in range(len(sub)):\n",
        "                prev = sub.iloc[:i]\n",
        "                mask = (prev[\"attempt_yards\"] - sub.iloc[i][\"attempt_yards\"]).abs() <= 5\n",
        "                vals.append(prev.loc[mask, \"success\"].mean() if mask.any() else overall)\n",
        "            return pd.Series(vals, index=sub.index)\n",
        "\n",
        "        # Minimal deprecation fix: exclude grouping column before apply\n",
        "        sim = (\n",
        "            df.groupby(\"player_id\", group_keys=False)\n",
        "            .apply(similar_rate, include_groups=False)\n",
        "            .reset_index(level=0, drop=True)\n",
        "        )\n",
        "        df[\"rolling_similar_distance_success\"] = sim\n",
        "\n",
        "        # Current streak length\n",
        "        df[\"current_streak\"] = (\n",
        "            df.groupby(\"player_id\")[\"success\"]\n",
        "            .transform(lambda x: x.groupby((x != x.shift()).cumsum()).cumcount() + 1)\n",
        "        )\n",
        "\n",
        "        print(\"******* Created performance history features (rolling & streaks)\")\n",
        "        return df\n",
        "\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Statistical interaction features\n",
        "    # ------------------------------------------------------------------\n",
        "    def create_statistical_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        df = df.copy()\n",
        "        df[\"distance_zscore\"] = (\n",
        "            (df[\"attempt_yards\"] - df[\"attempt_yards\"].mean()) /\n",
        "            df[\"attempt_yards\"].std()\n",
        "        )\n",
        "        df[\"distance_percentile\"] = df[\"attempt_yards\"].rank(pct=True)\n",
        "        \n",
        "        # Cleaner interaction names using centered age\n",
        "        df[\"age_dist_interact\"] = df[\"age_c\"] * df[\"attempt_yards\"]\n",
        "        df[\"exp_dist_interact\"] = df[\"exp_100\"] * df[\"attempt_yards\"]\n",
        "        \n",
        "        print(\"******* Created statistical interaction features (age × distance, exp × distance)\")\n",
        "        return df\n",
        "\n",
        "    def create_player_status_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Create player status categorization based on recent activity relative to dataset's latest date.\n",
        "        \n",
        "        Categories:\n",
        "        - Retired/Injured: 2+ years since last kick (730+ days)\n",
        "        - Not Playing/Potentially Playable: 1-2 years since last kick (365-729 days)  \n",
        "        - Playable: Less than 1 year since last kick (0-364 days)\n",
        "        \"\"\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        # Ensure datetime format\n",
        "        df[\"game_date\"] = pd.to_datetime(df[\"game_date\"])\n",
        "        \n",
        "        # Find the latest date in the dataset (reference point)\n",
        "        latest_date = df[\"game_date\"].max()\n",
        "        \n",
        "        # Calculate last kick date for each player\n",
        "        last_kick_by_player = df.groupby(\"player_id\")[\"game_date\"].max().reset_index()\n",
        "        last_kick_by_player.columns = [\"player_id\", \"last_kick_date\"]\n",
        "        \n",
        "        # Calculate days since last kick\n",
        "        last_kick_by_player[\"days_since_last_kick\"] = (\n",
        "            latest_date - last_kick_by_player[\"last_kick_date\"]\n",
        "        ).dt.days\n",
        "        \n",
        "        # Categorize player status\n",
        "        def categorize_status(days_since_last):\n",
        "            if days_since_last >= 730:  # 2+ years\n",
        "                return \"Retired/Injured\"\n",
        "            elif days_since_last >= 365:  # 1-2 years\n",
        "                return \"Not Playing/Potentially Playable\"\n",
        "            else:  # < 1 year\n",
        "                return \"Playable\"\n",
        "        \n",
        "        last_kick_by_player[\"player_status\"] = last_kick_by_player[\"days_since_last_kick\"].apply(categorize_status)\n",
        "        \n",
        "        # Merge back to main dataframe\n",
        "        df = df.merge(\n",
        "            last_kick_by_player[[\"player_id\", \"player_status\", \"days_since_last_kick\"]], \n",
        "            on=\"player_id\", \n",
        "            how=\"left\"\n",
        "        )\n",
        "        \n",
        "        # Add summary statistics\n",
        "        status_counts = last_kick_by_player[\"player_status\"].value_counts()\n",
        "        print(\"******* Created player status features based on recent activity\")\n",
        "        print(f\"   Reference date: {latest_date.strftime('%Y-%m-%d')}\")\n",
        "        print(f\"   Playable: {status_counts.get('Playable', 0)} players (<1 year)\")\n",
        "        print(f\"   Not Playing/Potentially Playable: {status_counts.get('Not Playing/Potentially Playable', 0)} players (1-2 years)\")\n",
        "        print(f\"   Retired/Injured: {status_counts.get('Retired/Injured', 0)} players (2+ years)\")\n",
        "        \n",
        "        return df\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Orchestration: build *all* features\n",
        "    # ------------------------------------------------------------------\n",
        "    def create_all_features(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        include_performance_history: bool = True,\n",
        "        performance_window: int = 10,\n",
        "        include_player_status: bool = True\n",
        "    ) -> pd.DataFrame:\n",
        "        print(\"Creating all features...\")\n",
        "        df = (\n",
        "            df.pipe(self.create_target_variable)\n",
        "              .pipe(self.create_date_features)\n",
        "              .pipe(self.create_kicker_mapping)\n",
        "              .pipe(self.create_distance_features)\n",
        "              .pipe(self.create_experience_features)\n",
        "              .pipe(self.create_situational_features)\n",
        "              .pipe(self.create_statistical_features)\n",
        "        )\n",
        "        if include_performance_history:\n",
        "            df = self.create_performance_history_features(df, performance_window)\n",
        "        if include_player_status:\n",
        "            df = self.create_player_status_features(df)\n",
        "        print(f\"******* All features created! Dataset shape: {df.shape}\")\n",
        "        return df\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Dynamic feature catalogue helper\n",
        "    # ------------------------------------------------------------------\n",
        "    def get_available_features(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        include_unique: bool = True,\n",
        "        max_unique_values: int = 20\n",
        "    ) -> Dict[str, Union[List[str], Dict[str, List[Union[str, int, float]]]]]:\n",
        "        \"\"\"Return feature categories -> features (optionally with uniques) after engineering.\"\"\"\n",
        "        base_catalog = {\n",
        "            \"target\": [\"success\"],\n",
        "            \"basic\": [\"attempt_yards\", \"age_at_attempt\", \"kicker_attempt_number\", \"importance\"],\n",
        "            \"distance\": [\n",
        "                \"distance_squared\", \"distance_cubed\", \"log_distance\",\n",
        "                \"distance_from_sweet_spot\", \"distance_zscore\", \"distance_percentile\"\n",
        "            ],\n",
        "            \"distance_flags\": [\"is_long_attempt\", \"is_very_long_attempt\"],\n",
        "            \"distance_categories\": [\"distance_category\"],\n",
        "            \"temporal\": [\"day_of_year\", \"month\", \"week\", \"season\", \"season_progress\"],\n",
        "            \"situational\": [\"is_early_season\", \"is_late_season\", \"is_playoffs\"],\n",
        "            \"experience\": [\"career_length_years\", \"is_rookie_attempt\", \"experience_category\"],\n",
        "            \"performance_history\": [\n",
        "                \"rolling_success_rate\", \"rolling_similar_distance_success\", \"current_streak\"\n",
        "            ],\n",
        "            \"interactions\": [\"age_distance_interaction\", \"experience_distance_interaction\"],\n",
        "            \"identifiers\": [\"kicker_id\", \"kicker_idx\", \"player_name\"]\n",
        "        }\n",
        "        catalog: Dict[str, Union[List[str], Dict[str, List[Union[str, int, float]]]]] = {}\n",
        "        for cat, feats in base_catalog.items():\n",
        "            present = [f for f in feats if f in df.columns]\n",
        "            if include_unique:\n",
        "                detail: Dict[str, List[Union[str, int, float]]] = {}\n",
        "                for f in present:\n",
        "                    if df[f].dtype == \"object\" or df[f].nunique() <= max_unique_values:\n",
        "                        detail[f] = sorted(df[f].dropna().unique().tolist())\n",
        "                    else:\n",
        "                        detail[f] = []\n",
        "                catalog[cat] = detail\n",
        "            else:\n",
        "                catalog[cat] = present\n",
        "        return catalog\n",
        "\n",
        "# Function to create a summary DataFrame\n",
        "def summarize(df):\n",
        "    summary = pd.DataFrame({\n",
        "        'dtype': df.dtypes.astype(str),\n",
        "        'missing': df.isnull().sum(),\n",
        "        'unique': df.nunique(),\n",
        "        'sample_values': df.apply(lambda col: col.dropna().unique()[:5] if col.nunique() > 5 else col.unique())\n",
        "    })\n",
        "    return summary\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from src.nfl_kicker_analysis.data.loader import DataLoader\n",
        "\n",
        "    loader = DataLoader()\n",
        "    df_raw = loader.load_complete_dataset()\n",
        "    engineer = FeatureEngineer()\n",
        "    df_feat = engineer.create_all_features(df_raw)\n",
        "\n",
        "    summary_fg = summarize(df_feat)\n",
        "    print(\"---------------summary_fg-----------------\")\n",
        "    print(summary_fg)\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/nfl_kicker_analysis/data/feature_selection.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/nfl_kicker_analysis/data/feature_selection.py\n",
        "\"\"\"\n",
        "New in v0.2.0\n",
        "-------------\n",
        "* Added Random-Forest impurity importance (`compute_rf_importance`)\n",
        "* Added tri-modal merge and multicollinearity pruning\n",
        "* Re-worked `select_final_features` to call these helpers\n",
        "\n",
        "New in v0.3.0\n",
        "-------------\n",
        "* Added mutable `FEATURE_LISTS` dictionary for flexible schema management\n",
        "* Added `DynamicSchema` class to replace hardcoded `_ColumnSchema` \n",
        "* Added `make_feature_matrix` helper for consistent X/y construction\n",
        "* Updated functions to accept explicit schema parameter\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ── NEW: model and importance imports\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.inspection import permutation_importance\n",
        "from sklearn.preprocessing import minmax_scale\n",
        "import shap\n",
        "from pathlib import Path\n",
        "import shapiq\n",
        "from sklearn.utils import resample\n",
        "from itertools import combinations\n",
        "import json\n",
        "\n",
        "# ── NEW: dataclass and typing imports for DynamicSchema\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Dict\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "# 🧩 Lightweight runtime schema object\n",
        "# ------------------------------------------------------------------\n",
        "@dataclass\n",
        "class DynamicSchema:\n",
        "    lists: Dict[str, List[str]] = field(default_factory=dict)\n",
        "\n",
        "    @property\n",
        "    def numerical(self):   return self.lists.get(\"numerical\", [])\n",
        "    @property\n",
        "    def ordinal(self):     return self.lists.get(\"ordinal\", [])\n",
        "    @property\n",
        "    def nominal(self):     return self.lists.get(\"nominal\", [])\n",
        "    @property\n",
        "    def target(self):      return self.lists.get(\"y_variable\", [])[0]\n",
        "    def all_features(self): return (\n",
        "        self.numerical + self.ordinal + self.nominal\n",
        "    )\n",
        "\n",
        "# ── Schema-aware utilities ──────────────────────────────────────────────\n",
        "def restrict_to_numerical(df: pd.DataFrame, schema: DynamicSchema) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Return a view of `df` that contains only the columns listed under\n",
        "    schema.numerical. Trust the schema's numerical list as the source of truth.\n",
        "    \"\"\"\n",
        "    return df[schema.numerical].copy()\n",
        "\n",
        "\n",
        "def update_schema_numerical(schema: DynamicSchema, new_numericals: list[str]) -> None:\n",
        "    \"\"\"\n",
        "    In-place replacement of the numerical list inside the DynamicSchema.\n",
        "    Keeps a copy of the old list for logging/debugging.\n",
        "    \"\"\"\n",
        "    old = schema.numerical\n",
        "    schema.lists[\"numerical\"] = sorted(new_numericals)\n",
        "    added   = sorted(set(new_numericals) - set(old))\n",
        "    removed = sorted(set(old)            - set(new_numericals))\n",
        "    print(f\"🔄  Schema update → numerical features now = {len(new_numericals)} columns\")\n",
        "    if added:   print(f\"   ➕ added   : {added}\")\n",
        "    if removed: print(f\"   ➖ removed : {removed}\")\n",
        "\n",
        "\n",
        "def make_feature_matrix(df: pd.DataFrame,\n",
        "                        schema: DynamicSchema,\n",
        "                        numeric_only: bool = True\n",
        "                       ) -> tuple[pd.DataFrame, pd.Series]:\n",
        "    \"\"\"Return X (features) and y (target) based on the supplied schema.\"\"\"\n",
        "    X: pd.DataFrame = restrict_to_numerical(df, schema) if numeric_only else df[schema.all_features()].copy()\n",
        "    y: pd.Series = df[schema.target]\n",
        "    return X, y\n",
        "\n",
        "def train_baseline_model(X, y):\n",
        "    \"\"\"\n",
        "    Fit a RandomForestRegressor on X, y.\n",
        "    Returns the fitted model.\n",
        "    \"\"\"\n",
        "    # You can adjust hyperparameters as needed\n",
        "    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "    model.fit(X, y)\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "def compute_permutation_importance(\n",
        "    model,\n",
        "    X: pd.DataFrame,\n",
        "    y: pd.Series,\n",
        "    n_repeats: int = 10,\n",
        "    n_jobs: int = 1,\n",
        "    max_samples: float | int | None = None,\n",
        "    random_state: int = 42,\n",
        "    verbose: bool = True\n",
        ") -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Compute permutation importances with controlled resource usage.\n",
        "    \n",
        "    Parameters\n",
        "    ----------\n",
        "    model : estimator\n",
        "        Fitted model implementing .predict and .score.\n",
        "    X : pd.DataFrame\n",
        "        Features.\n",
        "    y : pd.Series or array\n",
        "        Target.\n",
        "    n_repeats : int\n",
        "        Number of shuffles per feature.\n",
        "    n_jobs : int\n",
        "        Number of parallel jobs (avoid -1 on Windows).\n",
        "    max_samples : float or int, optional\n",
        "        If float in (0,1], fraction of rows to sample.\n",
        "        If int, absolute number of rows to sample.\n",
        "    random_state : int\n",
        "        Seed for reproducibility.\n",
        "    verbose : bool\n",
        "        Print debug info if True.\n",
        "        \n",
        "    Returns\n",
        "    -------\n",
        "    pd.DataFrame\n",
        "        Columns: feature, importance_mean, importance_std.\n",
        "        Sorted descending by importance_mean.\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"⏳ Computing permutation importances on {X.shape[0]} rows × {X.shape[1]} features\")\n",
        "        print(f\"   n_repeats={n_repeats}, n_jobs={n_jobs}, max_samples={max_samples}\")\n",
        "\n",
        "    # Subsample if requested\n",
        "    X_sel, y_sel = X, y\n",
        "    if max_samples is not None:\n",
        "        if isinstance(max_samples, float):\n",
        "            nsamp = int(len(X) * max_samples)\n",
        "        else:\n",
        "            nsamp = int(max_samples)\n",
        "        if verbose:\n",
        "            print(f\"   Subsampling to {nsamp} rows for speed\")\n",
        "        X_sel, y_sel = resample(X, y, replace=False, n_samples=nsamp, random_state=random_state)\n",
        "\n",
        "    try:\n",
        "        result = permutation_importance(\n",
        "            model,\n",
        "            X_sel, y_sel,\n",
        "            n_repeats=n_repeats,\n",
        "            random_state=random_state,\n",
        "            n_jobs=n_jobs,\n",
        "        )\n",
        "    except OSError as e:\n",
        "        # Graceful fallback to single job\n",
        "        if verbose:\n",
        "            print(f\"⚠️  OSError ({e}). Retrying with n_jobs=1\")\n",
        "        result = permutation_importance(\n",
        "            model,\n",
        "            X_sel, y_sel,\n",
        "            n_repeats=n_repeats,\n",
        "            random_state=random_state,\n",
        "            n_jobs=1,\n",
        "        )\n",
        "\n",
        "    # Build and sort DataFrame\n",
        "    importance_df = (\n",
        "        pd.DataFrame({\n",
        "            \"feature\": X.columns,\n",
        "            \"importance_mean\": result.importances_mean,\n",
        "            \"importance_std\": result.importances_std,\n",
        "        })\n",
        "        .sort_values(\"importance_mean\", ascending=False)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "    if verbose:\n",
        "        print(\"✅ Permutation importances computed.\")\n",
        "    return importance_df\n",
        "\n",
        "\n",
        "def compute_shap_importance(model, X, nsamples=100):\n",
        "    \"\"\"\n",
        "    Compute mean absolute SHAP values per feature.\n",
        "    Returns a DataFrame sorted by importance.\n",
        "    \"\"\"\n",
        "    # DeepExplainer or TreeExplainer for tree-based models\n",
        "    explainer = shap.TreeExplainer(model)\n",
        "    # sample for speed\n",
        "    X_sample = X.sample(n=min(nsamples, len(X)), random_state=42)\n",
        "    shap_values = explainer.shap_values(X_sample)\n",
        "    # For regression, shap_values is a 2D array\n",
        "    mean_abs_shap = pd.DataFrame({\n",
        "        \"feature\": X_sample.columns,\n",
        "        \"shap_importance\": np.abs(shap_values).mean(axis=0),\n",
        "    })\n",
        "    mean_abs_shap = mean_abs_shap.sort_values(\"shap_importance\", ascending=False).reset_index(drop=True)\n",
        "    return mean_abs_shap\n",
        "\n",
        "\n",
        "def compute_rf_importance(model, feature_names: list[str]) -> pd.DataFrame:\n",
        "    \"\"\"Return impurity-based RF importances.\"\"\"\n",
        "    if not hasattr(model, \"feature_importances_\"):\n",
        "        raise AttributeError(\"Model has no feature_importances_ attribute\")\n",
        "    return (\n",
        "        pd.DataFrame(\n",
        "            {\"feature\": feature_names,\n",
        "             \"rf_importance\": model.feature_importances_}\n",
        "        )\n",
        "        .sort_values(\"rf_importance\", ascending=False)\n",
        "        .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "\n",
        "def merge_and_score_importances(perm_df, shap_df, rf_df) -> pd.DataFrame:\n",
        "    \"\"\"Merge the three importance tables and add a combined_score column.\"\"\"\n",
        "    merged = (\n",
        "        perm_df.merge(shap_df, on=\"feature\", how=\"outer\")\n",
        "               .merge(rf_df,  on=\"feature\", how=\"outer\")\n",
        "               .fillna(0.0)\n",
        "    )\n",
        "    # Min-max normalise each column so weights are comparable\n",
        "    for col in [\"importance_mean\", \"shap_importance\", \"rf_importance\"]:\n",
        "        merged[f\"norm_{col}\"] = minmax_scale(merged[col].values)\n",
        "    merged[\"combined_score\"] = merged[\n",
        "        [\"norm_importance_mean\", \"norm_shap_importance\", \"norm_rf_importance\"]\n",
        "    ].mean(axis=1)\n",
        "    return merged.sort_values(\"combined_score\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "\n",
        "def drop_multicollinear(X: pd.DataFrame,\n",
        "                        ranked_feats: pd.DataFrame,\n",
        "                        corr_threshold: float = 0.85,\n",
        "                        method: str = \"pearson\") -> list[str]:\n",
        "    \"\"\"\n",
        "    Remove the lower-scoring feature from each highly correlated pair.\n",
        "    \"\"\"\n",
        "    corr = X[ranked_feats[\"feature\"]].corr().abs()\n",
        "    to_drop = set()\n",
        "    for f1, f2 in combinations(ranked_feats[\"feature\"], 2):\n",
        "        if corr.loc[f1, f2] > corr_threshold:\n",
        "            # keep the one with higher combined_score\n",
        "            better = f1 if ranked_feats.set_index(\"feature\").loc[f1,\"combined_score\"] \\\n",
        "                     >= ranked_feats.set_index(\"feature\").loc[f2,\"combined_score\"] else f2\n",
        "            worse  = f2 if better == f1 else f1\n",
        "            to_drop.add(worse)\n",
        "    return [f for f in ranked_feats[\"feature\"] if f not in to_drop]\n",
        "\n",
        "\n",
        "def filter_permutation_features(\n",
        "    importance_df: pd.DataFrame,\n",
        "    threshold: float\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Return features whose permutation importance_mean exceeds threshold.\n",
        "    \"\"\"\n",
        "    kept = importance_df.loc[\n",
        "        importance_df[\"importance_mean\"] > threshold, \"feature\"\n",
        "    ]\n",
        "    return kept.tolist()\n",
        "\n",
        "\n",
        "def filter_shap_features(\n",
        "    importance_df: pd.DataFrame,\n",
        "    threshold: float\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Return features whose shap_importance exceeds threshold.\n",
        "    \"\"\"\n",
        "    kept = importance_df.loc[\n",
        "        importance_df[\"shap_importance\"] > threshold, \"feature\"\n",
        "    ]\n",
        "    return kept.tolist()\n",
        "\n",
        "\n",
        "def select_final_features(perm_df,\n",
        "                          shap_df,\n",
        "                          rf_df,\n",
        "                          X: pd.DataFrame,\n",
        "                          schema: DynamicSchema,\n",
        "                          perm_thresh: float = 0.01,\n",
        "                          shap_thresh: float = 0.01,\n",
        "                          rf_thresh: float = 0.01,\n",
        "                          corr_threshold: float = 0.85) -> list[str]:\n",
        "    \"\"\"\n",
        "    Return the final feature list after:\n",
        "      1. Individual thresholding on each importance type\n",
        "      2. Union/intersection logic (here: *intersection*)\n",
        "      3. Combined-score ranking\n",
        "      4. Multicollinearity pruning\n",
        "    \"\"\"\n",
        "    # --- step 1: individual screens ---\n",
        "    keep_perm = perm_df.loc[perm_df[\"importance_mean\"] > perm_thresh, \"feature\"]\n",
        "    keep_shap = shap_df.loc[shap_df[\"shap_importance\"]   > shap_thresh, \"feature\"]\n",
        "    keep_rf   = rf_df.loc[rf_df[\"rf_importance\"]         > rf_thresh,   \"feature\"]\n",
        "    intersect = set(keep_perm) & set(keep_shap) & set(keep_rf)\n",
        "\n",
        "    # --- step 2: combine & rank only those ---\n",
        "    merged = merge_and_score_importances(\n",
        "        perm_df[perm_df[\"feature\"].isin(intersect)],\n",
        "        shap_df[shap_df[\"feature\"].isin(intersect)],\n",
        "        rf_df[rf_df[\"feature\"].isin(intersect)]\n",
        "    )\n",
        "\n",
        "    # --- step 3: drop multicollinear ---\n",
        "    final_feats = drop_multicollinear(X, merged, corr_threshold)\n",
        "\n",
        "    return final_feats\n",
        "\n",
        "\n",
        "# ============== BACKWARD COMPATIBILITY HELPERS ==============\n",
        "\n",
        "def select_final_features_legacy(\n",
        "    perm_feats: list[str],\n",
        "    shap_feats: list[str],\n",
        "    mode: str = \"intersection\"\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    DEPRECATED: Legacy version for backward compatibility.\n",
        "    Use select_final_features with DataFrame inputs for enhanced functionality.\n",
        "    \"\"\"\n",
        "    import warnings\n",
        "    warnings.warn(\n",
        "        \"select_final_features_legacy is deprecated. Use select_final_features with DataFrame inputs.\", \n",
        "        DeprecationWarning,\n",
        "        stacklevel=2\n",
        "    )\n",
        "    set_perm = set(perm_feats)\n",
        "    set_shap = set(shap_feats)\n",
        "    if mode == \"union\":\n",
        "        final = set_perm | set_shap\n",
        "    else:\n",
        "        final = set_perm & set_shap\n",
        "    return sorted(final)\n",
        "\n",
        "\n",
        "# ==============================================================\n",
        "\n",
        "\n",
        "def load_final_features(\n",
        "    file_path: str = \"data/models/features/final_features.txt\"\n",
        ") -> list[str]:\n",
        "    \"\"\"\n",
        "    Read the newline-delimited feature names file and return as a list.\n",
        "    \"\"\"\n",
        "    with open(file_path, \"r\") as fp:\n",
        "        return [line.strip() for line in fp if line.strip()]\n",
        "\n",
        "\n",
        "def filter_to_final_features(df: pd.DataFrame,\n",
        "                             final_feats_file: str,\n",
        "                             schema: DynamicSchema,\n",
        "                             id_cols: list[str] = None\n",
        "                            ) -> pd.DataFrame:\n",
        "    \"\"\"Return df[id + final + y] using the dynamic schema.\"\"\"\n",
        "    final_feats = load_final_features(final_feats_file)\n",
        "    id_cols = id_cols or []\n",
        "    missing = set(id_cols + final_feats + [schema.target]) - set(df.columns)\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing columns after filtering: {missing}\")\n",
        "    return df[id_cols + final_feats + [schema.target]].copy()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from src.nfl_kicker_analysis.data.loader import DataLoader\n",
        "    from src.nfl_kicker_analysis.data.feature_engineering import FeatureEngineer\n",
        "    import os\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 🔧 Single source of truth for column roles – edit freely\n",
        "    # ------------------------------------------------------------------\n",
        "    FEATURE_LISTS = {\n",
        "        \"numerical\": [\n",
        "            \"attempt_yards\", \"age_at_attempt\", \"distance_squared\",\n",
        "            \"career_length_years\", \"season_progress\", \"rolling_success_rate\",\n",
        "            \"current_streak\", \"distance_zscore\", \"distance_percentile\",\n",
        "        ],\n",
        "        \"ordinal\":  [\"season\", \"week\", \"month\", \"day_of_year\"],\n",
        "        \"nominal\":  [\n",
        "            \"kicker_id\", \"is_long_attempt\", \"is_very_long_attempt\",\n",
        "            \"is_rookie_attempt\", \"distance_category\", \"experience_category\",\n",
        "        ],\n",
        "        \"y_variable\": [\"success\"],\n",
        "    }\n",
        "\n",
        "    # ➊  Build schema from the dict\n",
        "    schema = DynamicSchema(FEATURE_LISTS)\n",
        "\n",
        "    # ➋  Load & feature-engineer\n",
        "    loader = DataLoader()\n",
        "    df_raw = loader.load_complete_dataset()\n",
        "    engineer = FeatureEngineer()\n",
        "    df_feat = engineer.create_all_features(df_raw)\n",
        "    print(\"---------------df_feat---------------\")\n",
        "    print(df_feat.head())\n",
        "    print(\"---------------df_feat.columns---------------\")\n",
        "    print(df_feat.columns)\n",
        "    print(\"---------------schema.all_features()---------------\")\n",
        "    print(schema.all_features())\n",
        "\n",
        "    # ➌  Make X / y using the new helper\n",
        "    X, y = make_feature_matrix(df_feat, schema)\n",
        "    \n",
        "    print(f\"\\n📊 Starting enhanced feature selection with {X.shape[1]} features\")\n",
        "    print(f\"   Schema contains {len(schema.all_features())} total features defined\")\n",
        "\n",
        "    # ➍  Run the tri-modal importance pipeline exactly as before\n",
        "    print(\"\\n🌲 Training Random Forest model...\")\n",
        "    model = train_baseline_model(X, y)\n",
        "    \n",
        "    print(\"\\n⚡ Computing permutation importance...\")\n",
        "    perm_df = compute_permutation_importance(model, X, y, max_samples=0.3)\n",
        "    \n",
        "    print(\"\\n🔍 Computing SHAP importance...\")\n",
        "    shap_df = compute_shap_importance(model, X, nsamples=100)\n",
        "    \n",
        "    print(\"\\n🌳 Computing Random Forest importance...\")\n",
        "    rf_df = compute_rf_importance(model, X.columns.tolist())\n",
        "\n",
        "    # 📊 Display top 10 features for each importance metric\n",
        "    print(\"\\n📈 Top 10 Features by Importance Metric:\")\n",
        "    print(\"\\nPermutation Importance:\")\n",
        "    print(perm_df.head(10)[[\"feature\", \"importance_mean\"]].to_string(index=False))\n",
        "    \n",
        "    print(\"\\nSHAP Importance:\")\n",
        "    print(shap_df.head(10)[[\"feature\", \"shap_importance\"]].to_string(index=False))\n",
        "    \n",
        "    print(\"\\nRandom Forest Importance:\")\n",
        "    print(rf_df.head(10)[[\"feature\", \"rf_importance\"]].to_string(index=False))\n",
        "\n",
        "    # 🔬 Select & de-correlate with detailed output\n",
        "    print(\"\\n🔍 Analyzing feature correlations...\")\n",
        "    corr = X[X.columns].corr().abs()\n",
        "    \n",
        "    # Find highly correlated pairs before feature selection\n",
        "    high_corr_pairs = []\n",
        "    for f1, f2 in combinations(X.columns, 2):\n",
        "        if corr.loc[f1, f2] > 0.85:  # Using same threshold as drop_multicollinear\n",
        "            high_corr_pairs.append({\n",
        "                'feature1': f1,\n",
        "                'feature2': f2,\n",
        "                'correlation': corr.loc[f1, f2]\n",
        "            })\n",
        "    \n",
        "    if high_corr_pairs:\n",
        "        print(\"\\n📊 Highly correlated feature pairs (correlation > 0.85):\")\n",
        "        for pair in sorted(high_corr_pairs, key=lambda x: x['correlation'], reverse=True):\n",
        "            print(f\"\\n{pair['feature1']} ↔️ {pair['feature2']}\")\n",
        "            print(f\"Correlation: {pair['correlation']:.3f}\")\n",
        "            \n",
        "            # Get importance scores for both features\n",
        "            f1_scores = {\n",
        "                'perm': float(perm_df[perm_df.feature == pair['feature1']].importance_mean.iloc[0]),\n",
        "                'shap': float(shap_df[shap_df.feature == pair['feature1']].shap_importance.iloc[0]),\n",
        "                'rf': float(rf_df[rf_df.feature == pair['feature1']].rf_importance.iloc[0])\n",
        "            }\n",
        "            f2_scores = {\n",
        "                'perm': float(perm_df[perm_df.feature == pair['feature2']].importance_mean.iloc[0]),\n",
        "                'shap': float(shap_df[shap_df.feature == pair['feature2']].shap_importance.iloc[0]),\n",
        "                'rf': float(rf_df[rf_df.feature == pair['feature2']].rf_importance.iloc[0])\n",
        "            }\n",
        "            \n",
        "            print(f\"{pair['feature1']} importance scores:\")\n",
        "            print(f\"  Permutation: {f1_scores['perm']:.4f}\")\n",
        "            print(f\"  SHAP: {f1_scores['shap']:.4f}\")\n",
        "            print(f\"  RF: {f1_scores['rf']:.4f}\")\n",
        "            \n",
        "            print(f\"{pair['feature2']} importance scores:\")\n",
        "            print(f\"  Permutation: {f2_scores['perm']:.4f}\")\n",
        "            print(f\"  SHAP: {f2_scores['shap']:.4f}\")\n",
        "            print(f\"  RF: {f2_scores['rf']:.4f}\")\n",
        "            \n",
        "            # Calculate average importance\n",
        "            f1_avg = sum(f1_scores.values()) / 3\n",
        "            f2_avg = sum(f2_scores.values()) / 3\n",
        "            \n",
        "            keeper = pair['feature1'] if f1_avg >= f2_avg else pair['feature2']\n",
        "            dropped = pair['feature2'] if keeper == pair['feature1'] else pair['feature1']\n",
        "            print(f\"\\n➡️ Decision: Keep {keeper} (avg importance: {max(f1_avg, f2_avg):.4f})\")\n",
        "            print(f\"❌ Drop {dropped} (avg importance: {min(f1_avg, f2_avg):.4f})\")\n",
        "    else:\n",
        "        print(\"No highly correlated feature pairs found.\")\n",
        "\n",
        "    # Run feature selection\n",
        "    final_features = select_final_features(\n",
        "        perm_df, shap_df, rf_df, X, schema,\n",
        "        perm_thresh=0.005, shap_thresh=0.005, rf_thresh=0.005\n",
        "    )\n",
        "    print(f\"---------------final_features---------------\")\n",
        "    print(final_features)\n",
        "    # output final_features to final_features.txt\n",
        "    with open(\"data/models/features/final_features.txt\", \"w\") as f:\n",
        "        for feat in final_features:\n",
        "            f.write(feat + \"\\n\")\n",
        "            \n",
        "    \n",
        "    # read final_features.txt\n",
        "    with open(\"data/models/features/final_features.txt\", \"r\") as f:\n",
        "        final_features = [line.strip() for line in f]\n",
        "    print(f\"---------------final_features---------------\")\n",
        "    print(final_features)\n",
        "    numeric_final = [f for f in final_features if f in schema.numerical]\n",
        "\n",
        "    print(f\"\\n✨ Final feature count: {len(numeric_final)}\")\n",
        "    print(\"Selected features:\")\n",
        "    for feat in numeric_final:\n",
        "        print(f\"  • {feat}\")\n",
        "\n",
        "    # 🔄 Push into schema so every later stage sees the new list\n",
        "    update_schema_numerical(schema, numeric_final)\n",
        "\n",
        "    # output final_features from schema\n",
        "    FEATURE_LISTS = schema.lists\n",
        "    print(f\"---------------FEATURE_LISTS---------------\")\n",
        "    print(FEATURE_LISTS)\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Data Preprocessing Module\n",
        "\n",
        "Handles data cleaning, feature engineering, and preparation for modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/nfl_kicker_analysis/data/preprocessor.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/nfl_kicker_analysis/data/preprocessor.py\n",
        "\"\"\"\n",
        "Data preprocessing module for NFL kicker analysis.\n",
        "Handles filtering, feature selection, and preparation for modeling.\n",
        "\n",
        "New in v0.4.0\n",
        "--------------\n",
        "* Added **inverse‑preprocessing** utilities so that any matrix produced by the\n",
        "  fitted `ColumnTransformer` can be projected back into human‑readable feature\n",
        "  space.  This is handy for debugging, error analysis, or piping model outputs\n",
        "  into post‑processing code that expects the raw‑scale feature values.\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from typing import Optional, Tuple, Dict, List, Union, cast, Any, TypeVar, Protocol\n",
        "from datetime import datetime\n",
        "\n",
        "from sklearn.preprocessing import FunctionTransformer, StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from scipy import sparse\n",
        "\n",
        "from src.nfl_kicker_analysis.data.feature_engineering import FeatureEngineer\n",
        "from src.nfl_kicker_analysis.data.feature_schema import FeatureSchema\n",
        "from src.nfl_kicker_analysis.config import config\n",
        "\n",
        "\n",
        "# Type variables for better type hints\n",
        "T = TypeVar('T')\n",
        "MatrixType = Union[np.ndarray, sparse.spmatrix]\n",
        "\n",
        "\n",
        "class Transformer(Protocol):\n",
        "    \"\"\"Protocol for scikit-learn transformers.\"\"\"\n",
        "    def fit_transform(self, X: Any, y: Any = None) -> MatrixType: ...\n",
        "    def transform(self, X: Any) -> MatrixType: ...\n",
        "    def inverse_transform(self, X: MatrixType) -> np.ndarray: ...\n",
        "\n",
        "\n",
        "class DataPreprocessor:\n",
        "    \"\"\"Handles data preprocessing, feature engineering, and inversion.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"Create a preprocessor with defaults from central config.\"\"\"\n",
        "        # 1️⃣ Initialize all attributes (as before)\n",
        "        self.MIN_DISTANCE: int | None = None\n",
        "        self.MAX_DISTANCE: int | None = None\n",
        "        self.MIN_KICKER_ATTEMPTS: int | None = None\n",
        "        self.SEASON_TYPES: list[str] | None = None\n",
        "        \n",
        "        self.NUMERICAL_FEATURES: list[str] = []\n",
        "        self.ORDINAL_FEATURES: list[str] = []\n",
        "        self.NOMINAL_FEATURES: list[str] = []\n",
        "        self.TARGET: str = \"success\"\n",
        "        \n",
        "        self.INCLUDE_PERFORMANCE_HISTORY: bool | None = None\n",
        "        self.INCLUDE_STATISTICAL_FEATURES: bool | None = None\n",
        "        self.INCLUDE_PLAYER_STATUS: bool | None = None\n",
        "        self.PERFORMANCE_WINDOW: int | None = None\n",
        "        \n",
        "        # Runtime artifacts\n",
        "        self.feature_engineer = FeatureEngineer()\n",
        "        self.raw_data: pd.DataFrame | None = None\n",
        "        self.processed_data: pd.DataFrame | None = None\n",
        "        self.schema: FeatureSchema | None = None\n",
        "        self.column_transformer_: ColumnTransformer | None = None\n",
        "        self._feature_cols_: List[str] | None = None\n",
        "        \n",
        "        # 2️⃣ Immediately inject defaults from the global config instance\n",
        "        from src.nfl_kicker_analysis.config import config\n",
        "        defaults = config  # config is already the Config() instance\n",
        "        \n",
        "        self.update_config(\n",
        "            min_distance=defaults.MIN_DISTANCE,\n",
        "            max_distance=defaults.MAX_DISTANCE,\n",
        "            min_kicker_attempts=defaults.MIN_KICKER_ATTEMPTS,\n",
        "            season_types=list(defaults.SEASON_TYPES),\n",
        "            include_performance_history=True,\n",
        "            include_statistical_features=False,\n",
        "            include_player_status=True,\n",
        "            performance_window=12\n",
        "        )\n",
        "        \n",
        "        # 3️⃣ Inject the feature lists from your central FEATURE_LISTS\n",
        "        from src.nfl_kicker_analysis.config import FEATURE_LISTS\n",
        "        self.update_feature_lists(\n",
        "            numerical=FEATURE_LISTS[\"numerical\"],\n",
        "            ordinal=FEATURE_LISTS[\"ordinal\"],\n",
        "            nominal=FEATURE_LISTS[\"nominal\"],\n",
        "            y_variable=FEATURE_LISTS[\"y_variable\"]\n",
        "        )\n",
        "        \n",
        "        print(\"******* DataPreprocessor initialized with defaults\")\n",
        "\n",
        "\n",
        "\n",
        "    def update_feature_lists(self, \n",
        "                           numerical: Optional[List[str]] = None,\n",
        "                           ordinal: Optional[List[str]] = None,\n",
        "                           nominal: Optional[List[str]] = None,\n",
        "                           y_variable: Optional[List[str]] = None):\n",
        "        \"\"\"\n",
        "        Update feature lists for easy experimentation.\n",
        "        \n",
        "        Args:\n",
        "            numerical: List of numerical features to use\n",
        "            ordinal: List of ordinal features to use  \n",
        "            nominal: List of nominal categorical features to use\n",
        "            y_variable: List containing target variable name\n",
        "        \"\"\"\n",
        "        if numerical is not None:\n",
        "            self.NUMERICAL_FEATURES = numerical\n",
        "        if ordinal is not None:\n",
        "            self.ORDINAL_FEATURES = ordinal\n",
        "        if nominal is not None:\n",
        "            self.NOMINAL_FEATURES = nominal\n",
        "        if y_variable is not None:\n",
        "            self.TARGET = y_variable[0]  # Single target assumed\n",
        "            \n",
        "        print(\"******* Feature lists updated\")\n",
        "    \n",
        "    def update_config(self, \n",
        "                     min_distance: Optional[int] = None,\n",
        "                     max_distance: Optional[int] = None,\n",
        "                     min_kicker_attempts: Optional[int] = None,\n",
        "                     season_types: Optional[List[str]] = None,\n",
        "                     include_performance_history: Optional[bool] = None,\n",
        "                     include_statistical_features: Optional[bool] = None,\n",
        "                     include_player_status: Optional[bool] = None,\n",
        "                     performance_window: Optional[int] = None):\n",
        "        \"\"\"\n",
        "        Update preprocessing configuration.\n",
        "        \n",
        "        Args:\n",
        "            min_distance: Minimum field goal distance to include\n",
        "            max_distance: Maximum field goal distance to include\n",
        "            min_kicker_attempts: Minimum attempts required per kicker\n",
        "            season_types: List of season types to include\n",
        "            include_performance_history: Whether to include performance history features\n",
        "            include_statistical_features: Whether to include statistical features\n",
        "            performance_window: Window size for rolling performance features\n",
        "        \"\"\"\n",
        "        if min_distance is not None:\n",
        "            self.MIN_DISTANCE = min_distance\n",
        "        if max_distance is not None:\n",
        "            self.MAX_DISTANCE = max_distance\n",
        "        if min_kicker_attempts is not None:\n",
        "            self.MIN_KICKER_ATTEMPTS = min_kicker_attempts\n",
        "        if season_types is not None:\n",
        "            self.SEASON_TYPES = season_types\n",
        "        if include_performance_history is not None:\n",
        "            self.INCLUDE_PERFORMANCE_HISTORY = include_performance_history\n",
        "        if include_statistical_features is not None:\n",
        "            self.INCLUDE_STATISTICAL_FEATURES = include_statistical_features\n",
        "        if include_player_status is not None:\n",
        "            self.INCLUDE_PLAYER_STATUS = include_player_status\n",
        "        if performance_window is not None:\n",
        "            self.PERFORMANCE_WINDOW = performance_window\n",
        "            \n",
        "        print(\"******* Configuration updated\")\n",
        "    \n",
        "    def _validate_config(self):\n",
        "        \"\"\"Validate that required configuration is set.\"\"\"\n",
        "        missing = []\n",
        "        if self.MIN_DISTANCE is None:\n",
        "            missing.append(\"MIN_DISTANCE\")\n",
        "        if self.MAX_DISTANCE is None:\n",
        "            missing.append(\"MAX_DISTANCE\")\n",
        "        if self.MIN_KICKER_ATTEMPTS is None:\n",
        "            missing.append(\"MIN_KICKER_ATTEMPTS\")\n",
        "        if self.SEASON_TYPES is None:\n",
        "            missing.append(\"SEASON_TYPES\")\n",
        "        if self.INCLUDE_PERFORMANCE_HISTORY is None:\n",
        "            missing.append(\"INCLUDE_PERFORMANCE_HISTORY\")\n",
        "        if self.INCLUDE_STATISTICAL_FEATURES is None:\n",
        "            missing.append(\"INCLUDE_STATISTICAL_FEATURES\")\n",
        "        if self.INCLUDE_PLAYER_STATUS is None:\n",
        "            missing.append(\"INCLUDE_PLAYER_STATUS\")\n",
        "        if self.PERFORMANCE_WINDOW is None:\n",
        "            missing.append(\"PERFORMANCE_WINDOW\")\n",
        "            \n",
        "        if missing:\n",
        "            raise ValueError(f\"Configuration not set. Please call update_config() first. Missing: {missing}\")\n",
        "    \n",
        "    def filter_season_type(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Filter data by season type.\"\"\"\n",
        "        self._validate_config()\n",
        "        assert self.SEASON_TYPES is not None  # guaranteed after validation\n",
        "        filtered_df = cast(pd.DataFrame, df[df['season_type'].isin(self.SEASON_TYPES)].copy())\n",
        "        print(f\"******* Filtered to {self.SEASON_TYPES} season(s): {len(filtered_df):,} attempts\")\n",
        "        return filtered_df\n",
        "    \n",
        "    def filter_blocked_field_goals(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Filter out blocked field goals since they don't reflect kicker skill.\"\"\"\n",
        "        filtered_df = cast(pd.DataFrame, df[df['field_goal_result'] != 'Blocked'].copy())\n",
        "        removed = len(df) - len(filtered_df)\n",
        "        print(f\"******* Filtered out blocked field goals\")\n",
        "        print(f\"   Removed {removed} blocked attempts, kept {len(filtered_df):,}\")\n",
        "        return filtered_df\n",
        "    \n",
        "    def filter_retired_injured_players(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Optionally drop Retired/Injured attempts (status created upstream).\n",
        "        Controlled by config.FILTER_RETIRED_INJURED.\n",
        "        \"\"\"\n",
        "        if \"player_status\" not in df.columns or not config.FILTER_RETIRED_INJURED:\n",
        "            return df                # no-op if flag is False or column missing\n",
        "\n",
        "        keep_mask = df[\"player_status\"] != \"Retired/Injured\"\n",
        "        dropped   = (~keep_mask).sum()\n",
        "        players   = df.loc[~keep_mask, \"player_name\"].nunique()\n",
        "\n",
        "        print(f\"🗑️  Filtered {dropped} attempts from {players} retired/injured players\")\n",
        "        return cast(pd.DataFrame, df[keep_mask].copy())\n",
        "    \n",
        "    def filter_distance_range(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Filter data by distance range to remove outliers.\"\"\"\n",
        "        self._validate_config()\n",
        "        filtered_df = cast(pd.DataFrame, df[\n",
        "            (df['attempt_yards'] >= self.MIN_DISTANCE) &\n",
        "            (df['attempt_yards'] <= self.MAX_DISTANCE)\n",
        "        ].copy())\n",
        "        \n",
        "        removed = len(df) - len(filtered_df)\n",
        "        print(f\"******* Filtered distance range {self.MIN_DISTANCE}-{self.MAX_DISTANCE} yards\")\n",
        "        print(f\"   Removed {removed} extreme attempts, kept {len(filtered_df):,}\")\n",
        "        return filtered_df\n",
        "    \n",
        "    def filter_min_attempts(self, df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Filter out kickers with too few attempts.\"\"\"\n",
        "        self._validate_config()\n",
        "        # Count attempts per kicker\n",
        "        kicker_counts = df['player_name'].value_counts()\n",
        "        valid_kickers = kicker_counts[kicker_counts >= self.MIN_KICKER_ATTEMPTS]\n",
        "        # Convert index to list safely using pandas Series methods\n",
        "        valid_kicker_names = cast(pd.Series, valid_kickers).index.tolist()\n",
        "        \n",
        "        # Filter to valid kickers only\n",
        "        filtered_df = cast(pd.DataFrame, df[df['player_name'].isin(valid_kicker_names)].copy())\n",
        "        \n",
        "        removed_kickers = len(kicker_counts) - len(valid_kickers)\n",
        "        print(f\"******* Filtered kickers with <{self.MIN_KICKER_ATTEMPTS} attempts\")\n",
        "        print(f\"   Removed {removed_kickers} kickers, kept {len(valid_kickers)}\")\n",
        "        print(f\"   Final dataset: {len(filtered_df):,} attempts\")\n",
        "        \n",
        "        return filtered_df\n",
        "    \n",
        "    def _get_selected_features(self) -> list[str]:\n",
        "        \"\"\"\n",
        "        Return the columns that will actually be fed into the model.\n",
        "\n",
        "        Priority order\n",
        "        --------------\n",
        "        1. If self.schema has been built, trust **only** the columns that the\n",
        "           schema says are present in `processed_data`.  This guarantees that\n",
        "           every returned name exists after filtering & feature-engineering.\n",
        "        2. Otherwise fall back to the raw configuration lists (old behaviour).\n",
        "\n",
        "        The list is deduplicated while preserving order.\n",
        "        \"\"\"\n",
        "        if self.schema is not None:\n",
        "            feats = (\n",
        "                self.schema.numerical +\n",
        "                self.schema.ordinal   +\n",
        "                self.schema.nominal\n",
        "            )\n",
        "        else:  # happens only before preprocess_complete()\n",
        "            feats = (\n",
        "                self.NUMERICAL_FEATURES +\n",
        "                self.ORDINAL_FEATURES   +\n",
        "                self.NOMINAL_FEATURES\n",
        "            )\n",
        "        # Preserve order, drop dups\n",
        "        return list(dict.fromkeys(feats))\n",
        "\n",
        "    \n",
        "    def _build_schema(self, df: pd.DataFrame) -> FeatureSchema:\n",
        "        \"\"\"Build the feature schema based on selected features.\"\"\"\n",
        "        selected_features = self._get_selected_features()\n",
        "        \n",
        "        # Filter feature lists to only include features that exist in the dataframe\n",
        "        available_features = set(df.columns)\n",
        "        \n",
        "        numerical = [f for f in self.NUMERICAL_FEATURES if f in available_features]\n",
        "        ordinal = [f for f in self.ORDINAL_FEATURES if f in available_features]\n",
        "        nominal = [f for f in self.NOMINAL_FEATURES if f in available_features]\n",
        "        \n",
        "        schema = FeatureSchema(\n",
        "            numerical=numerical,\n",
        "            binary=[],  # Binary features are now in nominal\n",
        "            ordinal=ordinal,\n",
        "            nominal=nominal,\n",
        "            target=self.TARGET,\n",
        "        )\n",
        "        \n",
        "        # Validate schema\n",
        "        try:\n",
        "            schema.assert_in_dataframe(df)\n",
        "        except AssertionError as e:\n",
        "            print(f\"Warning: Schema validation failed: {e}\")\n",
        "            print(\"Available features:\", list(available_features))\n",
        "            print(\"Requested features:\", selected_features)\n",
        "        \n",
        "        return schema\n",
        "\n",
        "    def make_column_transformer(self) -> ColumnTransformer:\n",
        "        \"\"\"\n",
        "        Return a scikit-learn ColumnTransformer based on the feature schema.\n",
        "        Call after `preprocess_complete()`.\n",
        "        \"\"\"\n",
        "        if self.schema is None:\n",
        "            raise AttributeError(\"Run preprocess_complete() before building transformers.\")\n",
        "\n",
        "        # Numeric pipeline - standard scaling\n",
        "        numeric_pipe = Pipeline(\n",
        "            steps=[\n",
        "                (\"scale\", StandardScaler())\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        # Build transformers list\n",
        "        transformers = []\n",
        "        \n",
        "        if self.schema.numerical:\n",
        "            transformers.append((\"num_scaled\", numeric_pipe, self.schema.numerical))\n",
        "            \n",
        "        if self.schema.binary:\n",
        "            transformers.append((\"binary_passthrough\", \"passthrough\", self.schema.binary))\n",
        "            \n",
        "        if self.schema.ordinal:\n",
        "            transformers.append((\"ordinal_passthrough\", \"passthrough\", self.schema.ordinal))\n",
        "            \n",
        "        if self.schema.nominal:\n",
        "            transformers.append((\"nominal_onehot\", \n",
        "                               OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True),\n",
        "                               self.schema.nominal))\n",
        "\n",
        "        # Categorical features are now included in nominal features\n",
        "        # No separate categorical handling needed\n",
        "\n",
        "        ct = ColumnTransformer(\n",
        "            transformers=transformers,\n",
        "            remainder=\"drop\",\n",
        "            verbose_feature_names_out=False,\n",
        "        )\n",
        "        return ct\n",
        "    \n",
        "    def preprocess_slice(self, raw_df: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"Run the exact notebook filters on an arbitrary slice *without* mutating\n",
        "           global state (so train/test can differ).\"\"\"\n",
        "        self._validate_config()\n",
        "\n",
        "        df = raw_df.copy()\n",
        "        print(f\"\\n🔍 Preprocessing slice of shape {df.shape}\")\n",
        "        \n",
        "        # Check if features are already engineered (to avoid double processing)\n",
        "        features_already_present = 'success' in df.columns and 'player_status' in df.columns\n",
        "        print(f\"Features already engineered: {features_already_present}\")\n",
        "        \n",
        "        if not features_already_present:\n",
        "            # Only run feature engineering if not already done\n",
        "            print(\"Running feature engineering...\")\n",
        "            df = self.feature_engineer.create_all_features(\n",
        "                df,\n",
        "                include_performance_history=self.INCLUDE_PERFORMANCE_HISTORY,\n",
        "                performance_window=self.PERFORMANCE_WINDOW,\n",
        "                include_player_status=self.INCLUDE_PLAYER_STATUS,\n",
        "            )\n",
        "            if self.INCLUDE_STATISTICAL_FEATURES:\n",
        "                df = self.feature_engineer.create_statistical_features(df)\n",
        "            print(f\"After feature engineering: {df.shape}\")\n",
        "        \n",
        "        # Apply filtering steps\n",
        "        print(\"\\nApplying filters:\")\n",
        "        print(f\"Initial rows: {len(df)}\")\n",
        "        \n",
        "        df = self.filter_season_type(df)\n",
        "        print(f\"After season type filter: {len(df)}\")\n",
        "        \n",
        "        df = self.filter_blocked_field_goals(df)\n",
        "        print(f\"After blocked FG filter: {len(df)}\")\n",
        "        \n",
        "        df = self.filter_distance_range(df)\n",
        "        print(f\"After distance range filter: {len(df)}\")\n",
        "        \n",
        "        df = self.filter_min_attempts(df)\n",
        "        print(f\"After min attempts filter: {len(df)}\") \n",
        "        \n",
        "        # Filter retired/injured players (needs player_status column)\n",
        "        df = self.filter_retired_injured_players(df)\n",
        "        print(f\"After retired/injured filter: {len(df)}\")\n",
        "\n",
        "        self._build_schema(df)\n",
        "        print(f\"\\nFinal preprocessed shape: {df.shape}\")\n",
        "        print(f\"Number of kickers: {df['kicker_id'].nunique()}\")\n",
        "        print(f\"Success rate: {df['success'].mean():.4f}\")\n",
        "        return df\n",
        "\n",
        "\n",
        "    def preprocess_complete(self, raw_df: pd.DataFrame, *, inplace: bool = True) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Complete preprocessing pipeline.\n",
        "        \n",
        "        Args:\n",
        "            raw_df: Raw merged DataFrame\n",
        "            inplace: If True (default), store results in instance attributes.\n",
        "                    If False, behaves like preprocess_slice() and leaves internal state untouched.\n",
        "            \n",
        "        Returns:\n",
        "            Fully preprocessed DataFrame ready for modeling\n",
        "        \"\"\"\n",
        "        if inplace:\n",
        "            # Validate configuration first\n",
        "            self._validate_config()\n",
        "            \n",
        "            self.raw_data = raw_df.copy()\n",
        "            \n",
        "            print(\"Starting complete preprocessing pipeline...\")\n",
        "            print(f\"Configuration: {self.MIN_DISTANCE}-{self.MAX_DISTANCE} yards, \"\n",
        "                  f\"min {self.MIN_KICKER_ATTEMPTS} attempts, {self.SEASON_TYPES} seasons\")\n",
        "            \n",
        "            self.processed_data = self.preprocess_slice(raw_df)\n",
        "            self.schema = self._build_schema(self.processed_data)\n",
        "            \n",
        "            print(f\"******* Preprocessing complete: {len(self.processed_data):,} attempts ready for modeling\")\n",
        "            print(f\"******* Features selected: {len(self._get_selected_features())} total\")\n",
        "            return self.processed_data\n",
        "        else:\n",
        "            return self.preprocess_slice(raw_df)\n",
        "    \n",
        "    def get_feature_summary(self) -> Dict:\n",
        "        \"\"\"Get summary of selected features by category.\"\"\"\n",
        "        if self.processed_data is None:\n",
        "            raise ValueError(\"No processed data available\")\n",
        "            \n",
        "        selected_features = self._get_selected_features()\n",
        "        available_features = set(self.processed_data.columns)\n",
        "        \n",
        "        summary = {\n",
        "            'total_selected': len(selected_features),\n",
        "            'total_available': len(available_features),\n",
        "            'numerical': [f for f in self.NUMERICAL_FEATURES if f in available_features],\n",
        "            'ordinal': [f for f in self.ORDINAL_FEATURES if f in available_features],\n",
        "            'nominal': [f for f in self.NOMINAL_FEATURES if f in available_features],\n",
        "            'missing_features': [f for f in selected_features if f not in available_features],\n",
        "        }\n",
        "        return summary\n",
        "    \n",
        "    def get_preprocessing_summary(self) -> Dict:\n",
        "        \"\"\"Get summary of preprocessing steps and results.\"\"\"\n",
        "        if self.processed_data is None:\n",
        "            raise ValueError(\"No processed data available\")\n",
        "            \n",
        "        summary = {\n",
        "            'original_size': len(self.raw_data) if self.raw_data is not None else 0,\n",
        "            'final_size': len(self.processed_data),\n",
        "            'unique_kickers': self.processed_data['player_name'].nunique(),\n",
        "            'success_rate': self.processed_data[self.TARGET].mean(),\n",
        "            'distance_range': (\n",
        "                self.processed_data['attempt_yards'].min(),\n",
        "                self.processed_data['attempt_yards'].max()\n",
        "            ),\n",
        "            'config': {\n",
        "                'min_distance': self.MIN_DISTANCE,\n",
        "                'max_distance': self.MAX_DISTANCE,\n",
        "                'min_kicker_attempts': self.MIN_KICKER_ATTEMPTS,\n",
        "                'season_types': self.SEASON_TYPES,\n",
        "                'include_performance_history': self.INCLUDE_PERFORMANCE_HISTORY,\n",
        "                'include_statistical_features': self.INCLUDE_STATISTICAL_FEATURES,\n",
        "            },\n",
        "            'features': self.get_feature_summary()\n",
        "        }\n",
        "        return summary\n",
        "\n",
        "    def fit_transform_features(\n",
        "        self,\n",
        "        *,                     # keyword-only\n",
        "        drop_missing: bool = True\n",
        "    ) -> tuple[MatrixType, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Fit the ColumnTransformer and return X / y.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        drop_missing : If True (default) silently removes any feature that is\n",
        "            absent from `processed_data` after printing a warning.  If False,\n",
        "            the old strict KeyError behaviour is retained.\n",
        "        \"\"\"\n",
        "        if self.processed_data is None:\n",
        "            raise ValueError(\"Run preprocess_complete() first.\")\n",
        "\n",
        "        feature_cols = self._get_selected_features()\n",
        "        missing = [c for c in feature_cols if c not in self.processed_data.columns]\n",
        "\n",
        "        if missing:\n",
        "            if drop_missing:\n",
        "                print(\n",
        "                    f\"⚠️  [DataPreprocessor] Dropping {len(missing)} \"\n",
        "                    f\"unavailable feature(s): {missing}\"\n",
        "                )\n",
        "                feature_cols = [c for c in feature_cols if c not in missing]\n",
        "            else:\n",
        "                raise KeyError(\n",
        "                    f\"These features are missing from processed_data: {missing}\"\n",
        "                )\n",
        "\n",
        "        # Build & fit transformer on the *pruned* list\n",
        "        self.column_transformer_ = self.make_column_transformer()\n",
        "        self._feature_cols_      = feature_cols\n",
        "        X = self.column_transformer_.fit_transform(self.processed_data[feature_cols])\n",
        "        y = self.processed_data[self.TARGET].values\n",
        "        return X, y\n",
        "\n",
        "\n",
        "    \n",
        "    def transform_features(self, df: pd.DataFrame) -> MatrixType:\n",
        "        \"\"\"Transform a *new* DataFrame using the already‑fitted transformer.\n",
        "        \n",
        "        Args:\n",
        "            df: DataFrame to transform, must have same columns as training data\n",
        "            \n",
        "        Returns:\n",
        "            Transformed feature matrix (sparse or dense)\n",
        "            \n",
        "        Raises:\n",
        "            ValueError: If transformer hasn't been fitted yet\n",
        "        \"\"\"\n",
        "        if self.column_transformer_ is None:\n",
        "            raise ValueError(\"Transformer not fitted – call fit_transform_features() first.\")\n",
        "            \n",
        "        feature_cols = self._get_selected_features()\n",
        "        X = self.column_transformer_.transform(df[feature_cols])\n",
        "        return cast(MatrixType, X)\n",
        "    \n",
        "    def invert_preprocessing(self, X_transformed: MatrixType) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Version-safe inverse transform.\n",
        "        Works on scikit-learn ≥0.24 (native) and ≤0.23 (manual fallback).\n",
        "        \"\"\"\n",
        "        if self.column_transformer_ is None or self._feature_cols_ is None:\n",
        "            raise ValueError(\"Transformer not fitted – call fit_transform_features() first.\")\n",
        "\n",
        "        # --- 1 ▸ Modern sklearn: just call the native helper -----------------\n",
        "        if hasattr(self.column_transformer_, \"inverse_transform\"):\n",
        "            X_inv = self.column_transformer_.inverse_transform(X_transformed)\n",
        "            return pd.DataFrame(X_inv, columns=self._feature_cols_)\n",
        "\n",
        "        # --- 2 ▸ Legacy sklearn: manual reconstruction -----------------------\n",
        "        X_dense = X_transformed.toarray() if sparse.issparse(X_transformed) else np.asarray(X_transformed)\n",
        "\n",
        "        col_arrays, current = [], 0\n",
        "        for name, trans, cols in self.column_transformer_.transformers_:\n",
        "            # -------- width calculation WITHOUT touching the transformer -----\n",
        "            if name == \"num_scaled\":\n",
        "                width = len(cols)  # numeric slice = number of original columns\n",
        "            elif name == \"nominal_onehot\":\n",
        "                enc = cast(OneHotEncoder, trans)\n",
        "                width = sum(len(c) for c in enc.categories_)\n",
        "            else:                           # passthrough groups\n",
        "                width = len(cols)\n",
        "\n",
        "            slice_ = X_dense[:, current: current + width]\n",
        "            current += width\n",
        "\n",
        "            # -------- inverse for each block ---------------------------------\n",
        "            if name == \"num_scaled\":\n",
        "                scaler = cast(StandardScaler, trans.named_steps[\"scale\"])\n",
        "                slice_inv = (slice_ * scaler.scale_) + scaler.mean_     # docs :contentReference[oaicite:2]{index=2}\n",
        "                col_arrays.append(slice_inv)\n",
        "\n",
        "            elif name == \"nominal_onehot\":\n",
        "                enc = cast(OneHotEncoder, trans)\n",
        "                slice_inv = enc.inverse_transform(slice_)               # docs :contentReference[oaicite:3]{index=3}\n",
        "                col_arrays.append(slice_inv)\n",
        "\n",
        "            else:                                                       # passthrough\n",
        "                col_arrays.append(slice_)\n",
        "\n",
        "        X_inv_full = np.column_stack(col_arrays)\n",
        "        return pd.DataFrame(X_inv_full, columns=self._feature_cols_)\n",
        "\n",
        "    def signature_hash(self) -> str:\n",
        "        \"\"\"\n",
        "        Create a stable hash of the preprocessor's configuration for caching.\n",
        "        \n",
        "        Returns:\n",
        "            SHA-1 hash (8 characters) of the configuration dict\n",
        "        \"\"\"\n",
        "        import json\n",
        "        import hashlib\n",
        "        \n",
        "        # Create a config dict with all the settings that affect preprocessing\n",
        "        config_dict = {\n",
        "            'min_distance': self.MIN_DISTANCE,\n",
        "            'max_distance': self.MAX_DISTANCE,\n",
        "            'min_kicker_attempts': self.MIN_KICKER_ATTEMPTS,\n",
        "            'season_types': self.SEASON_TYPES,\n",
        "            'include_performance_history': self.INCLUDE_PERFORMANCE_HISTORY,\n",
        "            'include_statistical_features': self.INCLUDE_STATISTICAL_FEATURES,\n",
        "            'include_player_status': self.INCLUDE_PLAYER_STATUS,\n",
        "            'performance_window': self.PERFORMANCE_WINDOW,\n",
        "            'numerical_features': self.NUMERICAL_FEATURES,\n",
        "            'ordinal_features': self.ORDINAL_FEATURES,\n",
        "            'nominal_features': self.NOMINAL_FEATURES,\n",
        "            'target': self.TARGET,\n",
        "        }\n",
        "        \n",
        "        # Create stable hash\n",
        "        raw = json.dumps(config_dict, sort_keys=True).encode()\n",
        "        return hashlib.sha1(raw).hexdigest()[:8]\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from src.nfl_kicker_analysis.data.loader import DataLoader\n",
        "    from src.nfl_kicker_analysis.data.feature_selection import (\n",
        "        DynamicSchema,\n",
        "        filter_to_final_features,\n",
        "        update_schema_numerical,\n",
        "    )\n",
        "\n",
        "    # ─── 1 Load raw data ──────────────────────────────────────────\n",
        "    loader = DataLoader()\n",
        "    df_raw = loader.load_complete_dataset()\n",
        "    \n",
        "    # ─── 2 Feature engineering pass ───────────────────────────────\n",
        "    engineer = FeatureEngineer()\n",
        "    df_feat = engineer.create_all_features(df_raw)\n",
        "\n",
        "    for category, details in engineer.get_available_features(df_feat).items():\n",
        "        print(f\"-- {category} --\")\n",
        "        for feat, uniques in details.items():\n",
        "            print(f\"   {feat}: {len(uniques)} unique | sample {uniques[:5] if uniques else '...'}\")\n",
        "\n",
        "    # ─── 3 Define all tunables in one place ─────────────────────\n",
        "    CONFIG = {\n",
        "        'min_distance': 20,\n",
        "        'max_distance': 60,\n",
        "        'min_kicker_attempts': 8,\n",
        "        'season_types': ['Reg', 'Post'],  # now include playoffs\n",
        "        'include_performance_history': True,\n",
        "        'include_statistical_features': False,\n",
        "        'include_player_status': True,  # ✅ FIX: Added missing parameter\n",
        "        'performance_window': 12,\n",
        "    }\n",
        "\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # 🔧 Single source of truth for column roles – edit freely\n",
        "    # ------------------------------------------------------------------\n",
        "    FEATURE_LISTS = {\n",
        "        \"numerical\": [\n",
        "            \"attempt_yards\", \"age_at_attempt\", \"distance_squared\",\n",
        "            \"career_length_years\", \"season_progress\", \"rolling_success_rate\",\n",
        "            \"current_streak\", \"distance_zscore\", \"distance_percentile\",\n",
        "        ],\n",
        "        \"ordinal\":  [\"season\", \"week\", \"month\", \"day_of_year\"],\n",
        "        \"nominal\":  [\n",
        "            \"kicker_id\", \"kicker_idx\", \"is_long_attempt\", \"is_very_long_attempt\",\n",
        "            \"is_rookie_attempt\", \"distance_category\", \"experience_category\",\n",
        "        ],\n",
        "        \"y_variable\": [\"success\"],\n",
        "    }\n",
        "\n",
        "    # ➊  Build schema from the dict\n",
        "    schema = DynamicSchema(FEATURE_LISTS)\n",
        "    \n",
        "    # read final_features.txt\n",
        "    with open(\"data/models/features/final_features.txt\", \"r\") as f:\n",
        "        final_features = [line.strip() for line in f]\n",
        "    print(f\"---------------final_features---------------\")\n",
        "    print(final_features)\n",
        "    numeric_final = [f for f in final_features if f in schema.numerical]\n",
        "\n",
        "    print(f\"\\n✨ Final feature count: {len(numeric_final)}\")\n",
        "    print(\"Selected features:\")\n",
        "    for feat in numeric_final:\n",
        "        print(f\"  • {feat}\")\n",
        "\n",
        "    # 🔄 Push into schema so every later stage sees the new list\n",
        "    update_schema_numerical(schema, numeric_final)\n",
        "\n",
        "    # output final_features from schema\n",
        "    FEATURE_LISTS = schema.lists\n",
        "    print(f\"---------------FEATURE_LISTS---------------\")\n",
        "    print(FEATURE_LISTS)\n",
        "\n",
        "    pre = DataPreprocessor()\n",
        "    pre.update_config(**CONFIG)\n",
        "    pre.update_feature_lists(**FEATURE_LISTS)\n",
        "    _ = pre.preprocess_complete(df_feat)\n",
        "    X, y = pre.fit_transform_features()\n",
        "\n",
        "    print(\"First 5 rows after inverse‑transform round‑trip →\")\n",
        "    print(pre.invert_preprocessing(X[:5]).head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Package Init Files\n",
        "\n",
        "Creating __init__.py files for all modules.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/nfl_kicker_analysis/__init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/nfl_kicker_analysis/__init__.py\n",
        "\"\"\"\n",
        "NFL Kicker Analysis Package\n",
        "A comprehensive toolkit for analyzing NFL field goal kicker performance.\n",
        "\"\"\"\n",
        "\n",
        "__version__ = \"1.0.0\"\n",
        "__author__ = \"NFL Analytics Team\"\n",
        "\n",
        "# Import main classes for easy access\n",
        "from .config import config\n",
        "from .data.loader import DataLoader\n",
        "from .data.preprocessor import DataPreprocessor\n",
        "\n",
        "__all__ = [\n",
        "    'config',\n",
        "    'DataLoader',\n",
        "    'DataPreprocessor'\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/nfl_kicker_analysis/data/__init__.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/nfl_kicker_analysis/data/__init__.py\n",
        "\"\"\"\n",
        "Data module for NFL kicker analysis.\n",
        "\"\"\"\n",
        "\n",
        "from .loader import DataLoader\n",
        "from .preprocessor import DataPreprocessor\n",
        "\n",
        "__all__ = ['DataLoader', 'DataPreprocessor']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Metrics and Utilities Module\n",
        "\n",
        "Core utilities for EPA calculations and performance metrics.\n",
        "\n",
        "1. EPACalculator\n",
        "\n",
        "    Purpose\n",
        "    Compute an Expected Points Added (EPA)–style rating for each kicker, based on how their made-field-goal rate at various distances compares to the league average.\n",
        "\n",
        "    Key Methods\n",
        "\n",
        "        calculate_empirical_success_rate(data, kicker_name, distance, …)\n",
        "        Looks up a kicker’s historical success rate in a ±distance_range window around a given yard line; if they don’t have enough attempts there, it falls back to the league average.\n",
        "\n",
        "        calculate_league_average_epa(data)\n",
        "        Computes the league-wide EPA per kick by weighting each distance’s success rate (from config.DISTANCE_PROFILE and config.DISTANCE_WEIGHTS) times 3 points per make.\n",
        "\n",
        "        calculate_kicker_epa_plus(data, kicker_name)\n",
        "        For one kicker, sums up their distance-weighted EPA and subtracts the league average to produce an “EPA-FG+” number.\n",
        "\n",
        "        calculate_all_kicker_ratings(data)\n",
        "        Loops over every kicker in the dataset and builds a leaderboard DataFrame of EPA-FG+ and rank.\n",
        "\n",
        "    How to use it\n",
        "    After you’ve loaded and preprocessed your DataFrame (so it has at least these columns:\n",
        "\n",
        "'player_name', 'player_id', 'attempt_yards', 'success'\n",
        "\n",
        "), you would call:\n",
        "\n",
        "    from nfl_kicker_analysis.utils.metrics import EPACalculator\n",
        "    epa = EPACalculator()\n",
        "    leaderboard = epa.calculate_all_kicker_ratings(processed_df)\n",
        "\n",
        "    That gives you a standalone table of kicker ratings.\n",
        "\n",
        "2. ModelEvaluator\n",
        "\n",
        "    Purpose\n",
        "    Compute standard classification metrics and compare different models’ results in a tidy DataFrame.\n",
        "\n",
        "    Key Methods\n",
        "\n",
        "        calculate_classification_metrics(y_true, y_pred_proba)\n",
        "        Returns a dict with\n",
        "\n",
        "            auc_roc\n",
        "\n",
        "            brier_score\n",
        "\n",
        "            accuracy (thresholded at 0.5)\n",
        "\n",
        "            log_loss\n",
        "\n",
        "        compare_models(models_results)\n",
        "        Takes a dict of { model_name: metrics_dict } and turns it into a DataFrame, sorted by AUC.\n",
        "\n",
        "    How to use it\n",
        "    After fitting your model(s) and getting back prediction probabilities:\n",
        "\n",
        "    from nfl_kicker_analysis.utils.metrics import ModelEvaluator\n",
        "\n",
        "    evaluator = ModelEvaluator()\n",
        "    metrics = evaluator.calculate_classification_metrics(y_test, y_pred_probs)\n",
        "    results = {\n",
        "        'MyModel': metrics,\n",
        "        'Baseline': other_metrics,\n",
        "    }\n",
        "    comparison_df = evaluator.compare_models(results)\n",
        "    print(comparison_df)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/nfl_kicker_analysis/utils/metrics.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/nfl_kicker_analysis/utils/metrics.py\n",
        "\"\"\"\n",
        "Metrics utilities for NFL kicker analysis.\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import (\n",
        "    roc_auc_score,\n",
        "    average_precision_score,     # AUC-PR\n",
        "    log_loss,\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    brier_score_loss,\n",
        ")\n",
        "try:\n",
        "    from sklearn.calibration import calibration_curve\n",
        "except ImportError:\n",
        "    from sklearn.metrics import calibration_curve\n",
        "from typing import Dict, Optional, Union, List, Any, Callable, Tuple, cast\n",
        "from numpy.typing import NDArray\n",
        "import arviz as az\n",
        "from src.nfl_kicker_analysis.config import config\n",
        "\n",
        "class EPACalculator:\n",
        "    \"\"\"Calculates EPA-based metrics for kickers.\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the EPA calculator.\"\"\"\n",
        "        self.baseline_probs: Dict[int, float] = {}\n",
        "        self.distance_profile = config.DISTANCE_PROFILE\n",
        "        self.distance_weights = config.DISTANCE_WEIGHTS\n",
        "    \n",
        "    def calculate_baseline_probs(self, data: pd.DataFrame) -> Dict[int, float]:\n",
        "        \"\"\"\n",
        "        Calculate baseline success probabilities by distance.\n",
        "        \n",
        "        Args:\n",
        "            data: DataFrame with attempt_yards and success\n",
        "            \n",
        "        Returns:\n",
        "            Dictionary mapping distance to success probability\n",
        "        \"\"\"\n",
        "        baseline = data.groupby('attempt_yards')['success'].mean()\n",
        "        self.baseline_probs = baseline.to_dict()\n",
        "        return self.baseline_probs\n",
        "    \n",
        "    def calculate_epa_fg_plus(self, data: pd.DataFrame) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Calculate EPA-FG+ for each kicker.\n",
        "        \n",
        "        Args:\n",
        "            data: DataFrame with kicker attempts\n",
        "            \n",
        "        Returns:\n",
        "            DataFrame with kicker EPA-FG+ ratings\n",
        "        \"\"\"\n",
        "        if not self.baseline_probs:\n",
        "            self.baseline_probs = self.calculate_baseline_probs(data)\n",
        "            \n",
        "        # Calculate expected points\n",
        "        data = data.copy()\n",
        "        data.loc[:, 'expected_points'] = data['attempt_yards'].map(lambda x: self.baseline_probs.get(x, 0.5)) * 3\n",
        "        data.loc[:, 'actual_points'] = data['success'] * 3\n",
        "        data.loc[:, 'epa'] = data['actual_points'] - data['expected_points']\n",
        "        \n",
        "        # Calculate EPA-FG+ per kicker\n",
        "        kicker_stats = data.groupby('player_name').agg({\n",
        "            'epa': ['count', 'mean', 'sum'],\n",
        "            'player_id': 'first'  # Keep player ID\n",
        "        })\n",
        "        \n",
        "        kicker_stats.columns = ['attempts', 'epa_per_kick', 'total_epa', 'player_id']\n",
        "        kicker_stats.loc[:, 'epa_fg_plus'] = kicker_stats['epa_per_kick']\n",
        "        \n",
        "        # Add rank\n",
        "        kicker_stats.loc[:, 'rank'] = kicker_stats['epa_fg_plus'].rank(ascending=False, method='min')\n",
        "        \n",
        "        return kicker_stats.reset_index()\n",
        "    \n",
        "    def calculate_clutch_rating_with_shrinkage(\n",
        "        self, \n",
        "        data: pd.DataFrame, \n",
        "        prior_a: float = 8.0, \n",
        "        prior_b: float = 2.0\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Calculate clutch field goal rating with beta-binomial shrinkage.\n",
        "        \n",
        "        Args:\n",
        "            data: DataFrame with kicker attempts including is_clutch column\n",
        "            prior_a: Beta prior alpha parameter (favors success)\n",
        "            prior_b: Beta prior beta parameter (favors failure)\n",
        "            \n",
        "        Returns:\n",
        "            DataFrame with clutch ratings per kicker\n",
        "        \"\"\"\n",
        "        # Default prior centers on 80% (8/(8+2))\n",
        "        \n",
        "        results = []\n",
        "        for player_name, grp in data.groupby('player_name'):\n",
        "            if 'is_clutch' in grp.columns:\n",
        "                clutch_attempts = grp[grp['is_clutch'] == 1]\n",
        "            else:\n",
        "                # Fallback if no clutch column\n",
        "                clutch_attempts = grp[grp['success'] == 0]  # Empty fallback\n",
        "                \n",
        "            made_clutch = clutch_attempts['success'].sum() if len(clutch_attempts) > 0 else 0\n",
        "            miss_clutch = len(clutch_attempts) - made_clutch if len(clutch_attempts) > 0 else 0\n",
        "            \n",
        "            # Beta-binomial posterior\n",
        "            post_a = prior_a + made_clutch\n",
        "            post_b = prior_b + miss_clutch\n",
        "            clutch_rate_shrunk = post_a / (post_a + post_b)\n",
        "            \n",
        "            # Raw clutch rate for comparison\n",
        "            raw_clutch_rate = clutch_attempts['success'].mean() if len(clutch_attempts) > 0 else 0.0\n",
        "            \n",
        "            results.append({\n",
        "                'player_name': player_name,\n",
        "                'player_id': int(grp['player_id'].iat[0]),\n",
        "                'total_attempts': len(grp),\n",
        "                'clutch_attempts': len(clutch_attempts),\n",
        "                'clutch_made': made_clutch,\n",
        "                'raw_clutch_rate': raw_clutch_rate,\n",
        "                'clutch_rate_shrunk': clutch_rate_shrunk,\n",
        "                'shrinkage_applied': abs(clutch_rate_shrunk - raw_clutch_rate)\n",
        "            })\n",
        "            \n",
        "        df = pd.DataFrame(results)\n",
        "        df['clutch_rank'] = df['clutch_rate_shrunk'].rank(ascending=False, method='min')\n",
        "        return df.sort_values('clutch_rank')\n",
        "    \n",
        "    # ────────────────────────────────────────────────────────────────────\n",
        "    # NEW helper ─ bootstrap EPA-FG⁺ draws for ONE kicker\n",
        "    # ────────────────────────────────────────────────────────────────────\n",
        "    def _bootstrap_kicker_epa(\n",
        "        self,\n",
        "        grp: pd.DataFrame,\n",
        "        *,\n",
        "        n_draws: int = 2_000,\n",
        "        rng: np.random.Generator,\n",
        "    ) -> NDArray[np.float_]:\n",
        "        \"\"\"\n",
        "        Non-parametric bootstrap: resample the kicker's attempts (with replacement)\n",
        "        and recompute mean EPA-FG⁺.  Returns an array of shape (n_draws,).\n",
        "        \n",
        "        Args:\n",
        "            grp: DataFrame with kicker's attempts\n",
        "            n_draws: Number of bootstrap draws\n",
        "            rng: Random number generator\n",
        "            \n",
        "        Returns:\n",
        "            Array of bootstrap EPA-FG+ draws\n",
        "        \"\"\"\n",
        "        # pre-compute baseline EPA for every attempt in the group\n",
        "        base_pts = grp[\"attempt_yards\"].map(\n",
        "            lambda x: self.baseline_probs.get(x, 0.5)\n",
        "        ).to_numpy(np.float_) * 3.0\n",
        "        actual   = grp[\"success\"].to_numpy(np.int_) * 3.0\n",
        "        diff     = actual - base_pts                    # vector of EPA per attempt\n",
        "\n",
        "        if diff.size == 0:\n",
        "            return np.array([np.nan])                   # safeguard – should not happen\n",
        "\n",
        "        boot = rng.choice(diff, size=(n_draws, diff.size), replace=True)\n",
        "        return boot.mean(axis=1)                       # average per draw\n",
        "\n",
        "    # ────────────────────────────────────────────────────────────────────\n",
        "    # PUBLIC – EPA-FG⁺ with 95 % interval & certainty flag\n",
        "    # ────────────────────────────────────────────────────────────────────\n",
        "    def calculate_epa_fg_plus_ci(\n",
        "        self,\n",
        "        data: pd.DataFrame,\n",
        "        *,\n",
        "        n_draws: int = 2_000,\n",
        "        alpha: float = 0.05,\n",
        "        random_state: int | None = 42,\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Bootstrap EPA-FG⁺ per kicker, returning mean, lower, upper bounds and a\n",
        "        qualitative certainty label (high | medium | low).\n",
        "\n",
        "        Args:\n",
        "            data: DataFrame with kicker attempts\n",
        "            n_draws: Number of bootstrap draws\n",
        "            alpha: Significance level for confidence intervals\n",
        "            random_state: Random seed for reproducibility\n",
        "            \n",
        "        Returns:\n",
        "            DataFrame with bootstrap EPA-FG+ ratings and confidence intervals\n",
        "            \n",
        "        Notes\n",
        "        -----\n",
        "        * Uses the **Jeffreys-prior** interpretation of a (1–alpha) credible\n",
        "          interval via percentile bootstrap.\n",
        "        * CI width thresholds (empirical 33rd & 66th percentiles) define the\n",
        "          certainty bands as recommended in sports-analytics literature.\n",
        "        \"\"\"\n",
        "        if not self.baseline_probs:\n",
        "            self.calculate_baseline_probs(data)\n",
        "\n",
        "        rng = np.random.default_rng(random_state)\n",
        "\n",
        "        records: list[dict[str, Any]] = []\n",
        "        for player, grp in data.groupby(\"player_name\"):\n",
        "            draws = self._bootstrap_kicker_epa(grp, n_draws=n_draws, rng=rng)\n",
        "            mean  = float(np.nanmean(draws))\n",
        "            lower = float(np.nanpercentile(draws, 100 * alpha / 2))\n",
        "            upper = float(np.nanpercentile(draws, 100 * (1 - alpha / 2)))\n",
        "            width = upper - lower\n",
        "            records.append({\n",
        "                \"player_name\": player,\n",
        "                \"player_id\":   int(grp[\"player_id\"].iat[0]),\n",
        "                \"attempts\":    int(len(grp)),\n",
        "                \"epa_fg_plus_mean\":  mean,\n",
        "                \"hdi_lower\":   lower,\n",
        "                \"hdi_upper\":   upper,\n",
        "                \"ci_width\":    width,\n",
        "            })\n",
        "\n",
        "        tbl = pd.DataFrame(records).set_index(\"player_name\")\n",
        "\n",
        "        # Certainty levels by tercile of CI-width\n",
        "        q33, q66 = tbl[\"ci_width\"].quantile([.33, .66])\n",
        "        tbl[\"certainty\"] = np.where(\n",
        "            tbl[\"ci_width\"] <= q33, \"high\",\n",
        "            np.where(tbl[\"ci_width\"] <= q66, \"medium\", \"low\")\n",
        "        )\n",
        "\n",
        "        tbl[\"rank\"] = tbl[\"epa_fg_plus_mean\"].rank(ascending=False, method=\"min\")\n",
        "        return tbl.sort_values(\"rank\")\n",
        "    \n",
        "    def calculate_all_kicker_ratings(self, data: pd.DataFrame, include_ci: bool = False) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Calculate complete kicker ratings, optionally with uncertainty intervals.\n",
        "        \n",
        "        Args:\n",
        "            data: Complete dataset\n",
        "            include_ci: Whether to include bootstrap confidence intervals\n",
        "            \n",
        "        Returns:\n",
        "            DataFrame with kicker ratings\n",
        "        \"\"\"\n",
        "        print(\"Calculating EPA-FG+ ratings...\")\n",
        "        \n",
        "        if include_ci:\n",
        "            ratings = self.calculate_epa_fg_plus_ci(data)\n",
        "            metric_col = \"epa_fg_plus_mean\"\n",
        "        else:\n",
        "            ratings = self.calculate_epa_fg_plus(data)\n",
        "            metric_col = \"epa_fg_plus\"\n",
        "        \n",
        "        print(f\"\\nTop 5 kickers by EPA-FG+:\")\n",
        "        display_cols = ['attempts', metric_col, 'rank']\n",
        "        if include_ci:\n",
        "            display_cols.extend(['hdi_lower', 'hdi_upper', 'certainty'])\n",
        "        \n",
        "        top_5 = ratings.head(5) if include_ci else ratings.nlargest(5, metric_col)\n",
        "        if include_ci:\n",
        "            print(top_5[display_cols].to_string(index=True))\n",
        "        else:\n",
        "            print(top_5[['player_name'] + display_cols].to_string(index=False))\n",
        "        \n",
        "        return ratings\n",
        "\n",
        "class ModelEvaluator:\n",
        "    \"\"\"Compute a rich set of discrimination & calibration metrics.\"\"\"\n",
        "\n",
        "    # ---------- single-metric helpers ----------\n",
        "    @staticmethod\n",
        "    def calculate_auc(y, p) -> float:\n",
        "        return float(roc_auc_score(y, p))\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_auc_pr(y, p) -> float:\n",
        "        return float(average_precision_score(y, p))  # AUC-PR\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_log_loss(y, p) -> float:\n",
        "        return float(log_loss(y, p))\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_brier_score(y, p) -> float:\n",
        "        return float(brier_score_loss(y, p))\n",
        "\n",
        "    @staticmethod\n",
        "    def calculate_threshold_metrics(y, p, thresh: float = 0.5) -> Tuple[float, float, float, float]:\n",
        "        \"\"\"Return accuracy, precision, recall, F1 at a chosen threshold.\"\"\"\n",
        "        pred = (p >= thresh).astype(int)\n",
        "        acc = float(accuracy_score(y, pred))\n",
        "        prec = float(precision_score(y, pred, zero_division=\"warn\"))\n",
        "        rec = float(recall_score(y, pred, zero_division=\"warn\"))\n",
        "        f1 = float(f1_score(y, pred, zero_division=\"warn\"))\n",
        "        return acc, prec, rec, f1\n",
        "\n",
        "    # ---------- calibration helpers ----------\n",
        "    @staticmethod\n",
        "    def calculate_ece(y, p, n_bins: int = 10) -> float:\n",
        "        \"\"\"\n",
        "        Expected Calibration Error (ECE) using equally spaced probability bins.\n",
        "\n",
        "        Handles the fact that `sklearn.calibration.calibration_curve` drops\n",
        "        empty bins, which otherwise causes a length-mismatch when you try to\n",
        "        multiply by fixed-length weights.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # 1 – Compute reliability data.  The function *silently* removes\n",
        "            #     any empty bin, so prob_true/pred may be < n_bins long.\n",
        "            prob_true, prob_pred = calibration_curve(\n",
        "                y, p, n_bins=n_bins, strategy=\"uniform\"\n",
        "            )\n",
        "\n",
        "            # 2 – Re-create the original histogram and keep *only* non-empty bins\n",
        "            bin_counts, _ = np.histogram(p, bins=n_bins, range=(0, 1))\n",
        "            non_empty_mask = bin_counts > 0\n",
        "\n",
        "            # Make sure shapes now align\n",
        "            bin_counts = bin_counts[non_empty_mask]\n",
        "            prob_true  = np.asarray(prob_true)\n",
        "            prob_pred  = np.asarray(prob_pred)\n",
        "\n",
        "            # Sanity check – all arrays must have identical length\n",
        "            assert len(prob_true) == len(bin_counts) == len(prob_pred), (\n",
        "                \"Length mismatch after masking empty bins\"\n",
        "            )\n",
        "\n",
        "            # 3 – Compute weighted absolute gap\n",
        "            weights = bin_counts / bin_counts.sum()\n",
        "            ece = np.sum(np.abs(prob_true - prob_pred) * weights)\n",
        "            return float(ece)\n",
        "\n",
        "        except Exception as exc:\n",
        "            # Fallback – simple loop (same logic as original fallback)\n",
        "            bin_edges = np.linspace(0.0, 1.0, n_bins + 1)\n",
        "            ece = 0.0\n",
        "            for lo, hi in zip(bin_edges[:-1], bin_edges[1:]):\n",
        "                in_bin = (p > lo) & (p <= hi)\n",
        "                if in_bin.any():\n",
        "                    acc_bin  = y[in_bin].mean()\n",
        "                    conf_bin = p[in_bin].mean()\n",
        "                    ece      += np.abs(acc_bin - conf_bin) * in_bin.mean()\n",
        "            return float(ece)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def reliability_curve(y, p, n_bins: int = 10) -> pd.DataFrame:\n",
        "        \"\"\"Return a DataFrame for plotting a reliability diagram.\"\"\"\n",
        "        try:\n",
        "            prob_true, prob_pred = calibration_curve(y, p, n_bins=n_bins, strategy=\"uniform\")\n",
        "            return pd.DataFrame({\"prob_pred\": prob_pred, \"prob_true\": prob_true})\n",
        "        except (ImportError, TypeError):\n",
        "            # Fallback implementation\n",
        "            bin_boundaries = np.linspace(0, 1, n_bins + 1)\n",
        "            bin_centers = (bin_boundaries[:-1] + bin_boundaries[1:]) / 2\n",
        "            bin_lowers = bin_boundaries[:-1]\n",
        "            bin_uppers = bin_boundaries[1:]\n",
        "            \n",
        "            prob_true = []\n",
        "            prob_pred = []\n",
        "            \n",
        "            for bin_lower, bin_upper, bin_center in zip(bin_lowers, bin_uppers, bin_centers):\n",
        "                in_bin = (p > bin_lower) & (p <= bin_upper)\n",
        "                if np.sum(in_bin) > 0:\n",
        "                    prob_true.append(y[in_bin].astype(float).mean())\n",
        "                    prob_pred.append(bin_center)\n",
        "            \n",
        "            return pd.DataFrame({\"prob_pred\": prob_pred, \"prob_true\": prob_true})\n",
        "\n",
        "    # ---------- public aggregator ----------\n",
        "    def calculate_classification_metrics(self, y_true, y_pred_proba) -> Dict[str, float]:\n",
        "        \"\"\"Return a full metric dictionary.\"\"\"\n",
        "        metrics: Dict[str, float] = {\n",
        "            \"auc_roc\":   self.calculate_auc(y_true, y_pred_proba),\n",
        "            \"auc_pr\":    self.calculate_auc_pr(y_true, y_pred_proba),\n",
        "            \"log_loss\":  self.calculate_log_loss(y_true, y_pred_proba),\n",
        "            \"brier\":     self.calculate_brier_score(y_true, y_pred_proba),\n",
        "            \"ece\":       self.calculate_ece(y_true, y_pred_proba),\n",
        "        }\n",
        "        acc, prec, rec, f1 = self.calculate_threshold_metrics(y_true, y_pred_proba)\n",
        "        metrics.update({\n",
        "            \"accuracy\":  acc,\n",
        "            \"precision\": prec,\n",
        "            \"recall\":    rec,\n",
        "            \"f1\":        f1,\n",
        "        })\n",
        "        return metrics  # 10 metrics total\n",
        "\n",
        "    # ---------- comparison helper ----------\n",
        "    def compare_models(self, results: Dict[str, Dict[str, float]]) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Turn {model: metric_dict} into a tidy table ordered by AUC-ROC.\n",
        "        \"\"\"\n",
        "        df = pd.DataFrame(results).T  # one row per model\n",
        "        # guarantee consistent column order\n",
        "        desired_cols: List[str] = [\n",
        "            \"auc_roc\", \"auc_pr\", \"log_loss\", \"brier\", \"ece\",\n",
        "            \"accuracy\", \"precision\", \"recall\", \"f1\"\n",
        "        ]\n",
        "        for c in desired_cols:\n",
        "            if c not in df.columns:\n",
        "                df[c] = np.nan\n",
        "        return df[desired_cols].sort_values(\"auc_roc\", ascending=False)\n",
        "\n",
        "class BayesianEvaluator:\n",
        "    \"\"\"Wrap ArviZ to calculate WAIC and PSIS-LOO in one call.\"\"\"\n",
        "    def information_criteria(self, trace, pointwise: bool = False) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Return WAIC and PSIS-LOO for a fitted PyMC model.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        trace : arviz.InferenceData\n",
        "        pointwise : bool\n",
        "            If True, also return the pointwise arrays for advanced analysis.\n",
        "        \"\"\"\n",
        "        waic_res = az.waic(trace, scale=\"deviance\")\n",
        "        loo_res  = az.loo(trace,  scale=\"deviance\")\n",
        "        info = {\n",
        "            \"waic\":      float(waic_res.waic),\n",
        "            \"waic_se\":   float(waic_res.waic_se),\n",
        "            \"psis_loo\":  float(loo_res.loo),\n",
        "            \"psis_loo_se\": float(loo_res.loo_se),\n",
        "        }\n",
        "        if pointwise:\n",
        "            info[\"waic_i\"] = waic_res.waic_i\n",
        "            info[\"loo_i\"]  = loo_res.loo_i\n",
        "        return info\n",
        "\n",
        "\n",
        "\n",
        "def train_test_split_by_season(\n",
        "    df: pd.DataFrame,\n",
        "    *,\n",
        "    train_seasons: list[int] = list(range(2010, 2018)),\n",
        "    test_seasons:  list[int] = [2018]\n",
        ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Leakage-free season split used across the package.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : DataFrame with a 'season' column\n",
        "    train_seasons : seasons to assign to the training fold\n",
        "    test_seasons  : seasons to assign to the test   fold\n",
        "    \"\"\"\n",
        "    train = cast(pd.DataFrame, df[df[\"season\"].isin(train_seasons)].copy())\n",
        "    test  = cast(pd.DataFrame, df[df[\"season\"].isin(test_seasons)].copy())\n",
        "    return train, test \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from src.nfl_kicker_analysis.data.loader import DataLoader\n",
        "    # Test the data loader\n",
        "    print(\"Testing DataLoader...\")\n",
        "    \n",
        "    loader = DataLoader()\n",
        "    \n",
        "    try:\n",
        "        # Load complete dataset\n",
        "        df = loader.load_complete_dataset()\n",
        "        \n",
        "        # Print summary\n",
        "        summary = loader.get_data_summary()\n",
        "        print(\"\\nData Summary:\")\n",
        "        print(f\"Total attempts: {summary['total_attempts']:,}\")\n",
        "        print(f\"Unique kickers: {summary['unique_kickers']}\")\n",
        "        print(f\"Seasons: {summary['unique_seasons']}\")\n",
        "        print(f\"Outcomes: {summary['outcome_counts']}\")\n",
        "        \n",
        "        print(\"******* DataLoader tests passed!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"-------------- Error testing DataLoader: {e}\")\n",
        "        print(\"Note: This is expected if data files are not present.\")\n",
        "        \n",
        "    \n",
        "    \n",
        "    # Test the metrics module\n",
        "    print(\"Testing EPA Calculator...\")\n",
        "\n",
        "    \n",
        "    # Test EPA calculator\n",
        "    epa_calc = EPACalculator()\n",
        "    \n",
        "    try:\n",
        "        # Test league average calculation\n",
        "        league_avg = epa_calc.calculate_league_average_epa(df)\n",
        "        print(f\"League average EPA: {league_avg:.3f}\")\n",
        "        \n",
        "        # Test individual kicker rating\n",
        "        rating = epa_calc.calculate_kicker_epa_plus(df, 'Player A')\n",
        "        print(f\"Player A EPA-FG+: {rating['epa_fg_plus']:.3f}\")\n",
        "        \n",
        "        # Test all kicker ratings\n",
        "        all_ratings = epa_calc.calculate_all_kicker_ratings(df)\n",
        "        print(f\"Calculated ratings for {len(all_ratings)} kickers\")\n",
        "        \n",
        "        # Test model evaluator\n",
        "        evaluator = ModelEvaluator()\n",
        "        y_true = np.random.choice([0, 1], 100)\n",
        "        y_pred = np.random.random(100)\n",
        "        \n",
        "        metrics = evaluator.calculate_classification_metrics(y_true, y_pred)\n",
        "        print(f\"Sample model AUC: {metrics['auc']:.3f}\")\n",
        "        \n",
        "        print(\"******* Metrics module tests passed!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"-------------- Error testing metrics: {e}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/nfl_kicker_analysis/utils/model_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/nfl_kicker_analysis/utils/model_utils.py\n",
        "\"\"\"\n",
        "Model persistence utilities for NFL Kicker Analysis.\n",
        "\n",
        "This module provides functions to save and load fitted models (Bayesian or scikit-learn)\n",
        "to/from disk using joblib, avoiding Streamlit caching issues with un-picklable objects.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "import json\n",
        "import hashlib\n",
        "import datetime as _dt\n",
        "from pathlib import Path\n",
        "from typing import Any, Optional, Union, cast, Dict\n",
        "import joblib  # fast, compressed persistence\n",
        "import mlflow\n",
        "import shutil\n",
        "from mlflow.tracking import MlflowClient\n",
        "from mlflow.pyfunc.model import PythonModel, PythonModelContext\n",
        "from mlflow.models.signature import ModelSignature\n",
        "from mlflow.types.schema import Schema\n",
        "from mlflow.exceptions import MlflowException\n",
        "from mlflow import sklearn as mlflow_sklearn\n",
        "from src.nfl_kicker_analysis.config import config\n",
        "# Ensure experiments directory or tracking server is initialized,\n",
        "# and bind to a named experiment (creates it if missing).\n",
        "mlflow.set_experiment(config.MLFLOW_EXPERIMENT_NAME)\n",
        "from mlflow.models import infer_signature\n",
        "from mlflow.pyfunc import load_model as mlflow_load_model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cloudpickle\n",
        "\n",
        "# Use config paths instead of hardcoded ones\n",
        "DEFAULT_DIR = config.MODELS_DIR / \"mlruns\" / \"models\"  # Point estimate models go here\n",
        "MLRUNS_DIR = config.PROJECT_ROOT / \"mlruns\"\n",
        "\n",
        "def cleanup_all_models():\n",
        "    \"\"\"\n",
        "    Clean up all existing models and MLflow runs.\n",
        "    \"\"\"\n",
        "    # ─── Helper for rmtree errors ───────────────────────────────────\n",
        "    def _on_rm_error(func, path, exc_info):\n",
        "        err = exc_info[1]\n",
        "        if isinstance(err, OSError) and err.errno == 16:\n",
        "            print(f\"⚠️  Could not remove {path!r} (resource busy), skipping.\")\n",
        "        else:\n",
        "            raise\n",
        "\n",
        "    # ─── Remove local models directory ─────────────────────────────\n",
        "    if DEFAULT_DIR.exists():\n",
        "        shutil.rmtree(DEFAULT_DIR, onerror=_on_rm_error)\n",
        "    DEFAULT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # ─── Remove MLflow runs directory ──────────────────────────────\n",
        "    if MLRUNS_DIR.exists():\n",
        "        try:\n",
        "            shutil.rmtree(MLRUNS_DIR, onerror=_on_rm_error)\n",
        "        except Exception as e:\n",
        "            print(f\"⚠️  Unexpected error removing mlruns: {e}\")\n",
        "    MLRUNS_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    (MLRUNS_DIR / \".trash\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # ─── Delete registered models from MLflow registry ────────────\n",
        "    registry_dir = MLRUNS_DIR / \"models\"\n",
        "    if registry_dir.exists():\n",
        "        client = MlflowClient()\n",
        "        for rm in client.search_registered_models():\n",
        "            client.delete_registered_model(rm.name)\n",
        "    else:\n",
        "        print(f\"ℹ️  MLflow registry folder {registry_dir!r} not found; skipping registry cleanup.\")\n",
        "\n",
        "\n",
        "def get_best_metrics(name: str) -> dict[str, float] | None:\n",
        "    \"\"\"\n",
        "    Get the best metrics for `name`:\n",
        "      1. Try MLflow registry (preferred).\n",
        "      2. If not found or MLflow error, scan local version folders for metrics.json.\n",
        "      3. Otherwise return None.\n",
        "    \"\"\"\n",
        "    client = MlflowClient()\n",
        "\n",
        "    # 1) MLflow lookup\n",
        "    try:\n",
        "        versions = client.get_latest_versions(name)\n",
        "        if versions:\n",
        "            run_id = versions[0].run_id\n",
        "            if run_id:\n",
        "                run = client.get_run(run_id)\n",
        "                return dict(run.data.metrics)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    # 2) Local fallback: look inside version subdirectories\n",
        "    model_dir = DEFAULT_DIR / name\n",
        "    if model_dir.exists():\n",
        "        # list timestamped subfolders\n",
        "        version_dirs = sorted(\n",
        "            [d for d in model_dir.iterdir() if d.is_dir()]\n",
        "        )\n",
        "        if version_dirs:\n",
        "            latest = version_dirs[-1]\n",
        "            meta_path = latest / \"metrics.json\"\n",
        "            if meta_path.exists():\n",
        "                try:\n",
        "                    return json.load(meta_path.open(\"r\"))\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "    # 3) Nothing found\n",
        "    return None\n",
        "\n",
        "\n",
        "\n",
        "def _timestamp() -> str:\n",
        "    \"\"\"Helper for version folders - returns current timestamp as string.\"\"\"\n",
        "    return _dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "def _hash_dict(d: dict) -> str:\n",
        "    \"\"\"Create stable 8-char SHA-1 hash of dict for cache keys.\"\"\"\n",
        "    raw = json.dumps(d, sort_keys=True).encode()\n",
        "    return hashlib.sha1(raw).hexdigest()[:8]\n",
        "\n",
        "def get_model_metadata(\n",
        "    name: str,\n",
        "    version: str | None = \"latest\",\n",
        "    base_dir: Path = DEFAULT_DIR\n",
        ") -> dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Load metadata for a saved model without loading the model itself.\n",
        "    \n",
        "    Args:\n",
        "        name: Model name\n",
        "        version: Specific version or \"latest\"\n",
        "        base_dir: Base directory for model storage\n",
        "        \n",
        "    Returns:\n",
        "        Metadata dictionary\n",
        "    \"\"\"\n",
        "    model_dir = base_dir / name\n",
        "    \n",
        "    if version == \"latest\":\n",
        "        version_dirs = sorted(model_dir.iterdir(), reverse=True)\n",
        "        if not version_dirs:\n",
        "            raise FileNotFoundError(f\"No saved model found for '{name}'\")\n",
        "        model_dir = version_dirs[0]\n",
        "    else:\n",
        "        if version is None:\n",
        "            raise ValueError(\"Version cannot be None when not 'latest'\")\n",
        "        model_dir = model_dir / version\n",
        "\n",
        "    meta_path = model_dir / \"meta.json\"\n",
        "    if not meta_path.exists():\n",
        "        return {}\n",
        "        \n",
        "    with meta_path.open(\"r\") as fp:\n",
        "        return json.load(fp)\n",
        "\n",
        "def list_registered_models() -> dict[str, list[str]]:\n",
        "    \"\"\"\n",
        "    Return a mapping of all MLflow-registered model names → list of version strings.\n",
        "    \"\"\"\n",
        "    client = MlflowClient()\n",
        "    out: dict[str, list[str]] = {}\n",
        "    # search_registered_models is the correct method in recent MLflow releases\n",
        "    for rm in client.search_registered_models():\n",
        "        name = rm.name\n",
        "        # get_latest_versions still works\n",
        "        versions = client.get_latest_versions(name)\n",
        "        out[name] = [v.version for v in versions]\n",
        "    return out\n",
        "\n",
        "def list_saved_models(base_dir: Path = DEFAULT_DIR) -> dict[str, list[str]]:\n",
        "    \"\"\"\n",
        "    List all saved filesystem models and their version subdirectories.\n",
        "    \"\"\"\n",
        "    if not base_dir.exists():\n",
        "        return {}\n",
        "    models = {}\n",
        "    for d in base_dir.iterdir():\n",
        "        if d.is_dir():\n",
        "            versions = [v.name for v in d.iterdir() if v.is_dir()]\n",
        "            if versions:\n",
        "                models[d.name] = sorted(versions, reverse=True)\n",
        "    return models\n",
        "\n",
        "def _save_leaderboard(df: pd.DataFrame):\n",
        "    \"\"\"\n",
        "    Persist the dynamic Bayesian leaderboard to disk.\n",
        "    \"\"\"\n",
        "    out = df.reset_index().rename(columns={\"index\": \"player_id\"})\n",
        "    out.to_csv(config.LEADERBOARD_FILE, index=False)\n",
        "\n",
        "class _WrappedModel(PythonModel):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self._model = model\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        context: PythonModelContext,\n",
        "        model_input: pd.DataFrame\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        context: MLflow-provided context (unused here)\n",
        "        model_input: pandas DataFrame of features\n",
        "        returns: pandas DataFrame or Series of predictions\n",
        "        \"\"\"\n",
        "        # Delegate to the underlying model\n",
        "        return pd.DataFrame(self._model.predict(model_input))\n",
        "\n",
        "from mlflow.exceptions import MlflowException\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "from yaml.representer import RepresenterError\n",
        "\n",
        "def save_model(\n",
        "    model: Any,\n",
        "    name: str,\n",
        "    metrics: dict[str, float],\n",
        "    meta: dict | None = None,\n",
        "    register: bool = True,\n",
        "    stage: str = \"Staging\"\n",
        ") -> str | None:\n",
        "    \"\"\"\n",
        "    Persist model to MLflow and register it if its metrics improve.\n",
        "    Catches YAML serialization errors during stage transition.\n",
        "    \"\"\"\n",
        "    if \"accuracy\" not in metrics:\n",
        "        raise ValueError(\"Metrics must include 'accuracy'\")\n",
        "\n",
        "    best = get_best_metrics(name)\n",
        "    if best and metrics[\"accuracy\"] <= best.get(\"accuracy\", 0):\n",
        "        print(f\"⚠️ New accuracy ({metrics['accuracy']:.4f}) ≤ best ({best['accuracy']:.4f}); skipping save.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        with mlflow.start_run(nested=True) as run:\n",
        "            mlflow.log_metrics(metrics)\n",
        "            kwargs = {\n",
        "                \"artifact_path\": name,\n",
        "                \"registered_model_name\": None,\n",
        "                \"metadata\": meta or {},\n",
        "            }\n",
        "            if hasattr(model, \"predict\") and \"sklearn\" in type(model).__module__:\n",
        "                mlflow_sklearn.log_model(sk_model=model, **kwargs)\n",
        "            else:\n",
        "                mlflow.pyfunc.log_model(\n",
        "                    python_model=_WrappedModel(model),\n",
        "                    **kwargs\n",
        "                )\n",
        "            artifact_uri = f\"runs:/{run.info.run_id}/{name}\"\n",
        "            print(f\"✅ Logged model to {artifact_uri}\")\n",
        "\n",
        "            if register:\n",
        "                result = mlflow.register_model(\n",
        "                    model_uri=artifact_uri,\n",
        "                    name=name\n",
        "                )\n",
        "                version = result.version\n",
        "                print(f\"✅ Registered model '{name}' as version {version}\")\n",
        "\n",
        "                client = MlflowClient()\n",
        "                try:\n",
        "                    client.transition_model_version_stage(\n",
        "                        name, version, stage,\n",
        "                        archive_existing_versions=False\n",
        "                    )\n",
        "                    uri = f\"models:/{name}/{stage}\"\n",
        "                    print(f\"✅ Transitioned version {version} to stage '{stage}'\")\n",
        "                    return uri\n",
        "                except RepresenterError as rep_err:\n",
        "                    print(f\"⚠️ Could not serialize model-version metadata: {rep_err}; skipping stage transition.\")\n",
        "                    return f\"models:/{name}/{version}\"\n",
        "\n",
        "            return artifact_uri\n",
        "\n",
        "    except MlflowException as e:\n",
        "        # Fallback to local filesystem\n",
        "        ts = _timestamp()\n",
        "        version_dir = DEFAULT_DIR / name / ts\n",
        "        version_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Save metrics.json\n",
        "        metrics_path = version_dir / \"metrics.json\"\n",
        "        with open(metrics_path, \"w\") as f:\n",
        "            json.dump(metrics, f)\n",
        "\n",
        "        path = version_dir / \"model.joblib\"\n",
        "        joblib.dump(model, path)\n",
        "        print(f\"⚠️ MLflow error ({e}); saved locally to {path}\")\n",
        "        return str(path)\n",
        "\n",
        "# ── New imports ─────────────────────────────────────────────\n",
        "from mlflow.sklearn import load_model as sklearn_load_model\n",
        "from mlflow.pyfunc import load_model as pyfunc_load_model\n",
        "import yaml\n",
        "# ──────────────────────────────────────────────────────────\n",
        "\n",
        "def load_model(\n",
        "    name: str,\n",
        "    version: str | None = \"latest\",\n",
        "    base_dir: Path = DEFAULT_DIR\n",
        ") -> Any:\n",
        "    \"\"\"\n",
        "    Load a saved model with enhanced MLflow registry support:\n",
        "      1) Try the sklearn flavor (full API w/ predict_proba).\n",
        "      2) Fallback to the pyfunc flavor (generic predict-only).\n",
        "      3) Fallback to local filesystem using MLflow registry metadata.\n",
        "      4) Final fallback to legacy local filesystem (joblib/cloudpickle).\n",
        "    \"\"\"\n",
        "    model_uri = f\"models:/{name}/{version or 'latest'}\"\n",
        "    \n",
        "    print(f\"[DEBUG] Loading model '{name}' (version: {version or 'latest'})\")\n",
        "\n",
        "    # 1. sklearn flavor\n",
        "    try:\n",
        "        model = sklearn_load_model(model_uri)\n",
        "        print(f\"[SUCCESS] Loaded sklearn model '{name}' from '{model_uri}'\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"[WARNING] sklearn flavor load failed ({e}); trying pyfunc...\")\n",
        "\n",
        "    # 2. pyfunc flavor\n",
        "    try:\n",
        "        model = pyfunc_load_model(model_uri)\n",
        "        print(f\"[SUCCESS] Loaded pyfunc model '{name}' from '{model_uri}'\")\n",
        "        return model\n",
        "    except Exception as e:\n",
        "        print(f\"[WARNING] pyfunc flavor load failed ({e}); trying MLflow registry metadata...\")\n",
        "\n",
        "    # 3. MLflow registry metadata fallback\n",
        "    try:\n",
        "        model = _load_model_from_registry_metadata(name, version, base_dir)\n",
        "        if model is not None:\n",
        "            print(f\"[SUCCESS] Loaded model '{name}' from MLflow registry metadata\")\n",
        "            return model\n",
        "    except Exception as e:\n",
        "        print(f\"[WARNING] MLflow registry metadata load failed ({e}); trying legacy filesystem...\")\n",
        "\n",
        "    # 4. Legacy filesystem fallback\n",
        "    model_root = base_dir / name\n",
        "    if not model_root.exists():\n",
        "        raise FileNotFoundError(f\"No model directory for '{name}' at {model_root}\")\n",
        "\n",
        "    # […existing local-loading logic…]\n",
        "    versions = sorted(d for d in model_root.iterdir() if d.is_dir())\n",
        "    if not versions:\n",
        "        raise FileNotFoundError(f\"No versions for '{name}' in {model_root}\")\n",
        "    model_dir = versions[-1]\n",
        "\n",
        "    joblib_path = model_dir / \"model.joblib\"\n",
        "    if joblib_path.exists():\n",
        "        print(f\"[SUCCESS] Loaded model '{name}' from legacy joblib: {joblib_path}\")\n",
        "        return joblib.load(joblib_path)\n",
        "\n",
        "    pkl_path = model_dir / \"model.pkl\"\n",
        "    if pkl_path.exists():\n",
        "        import cloudpickle\n",
        "        with open(pkl_path, \"rb\") as f:\n",
        "            print(f\"[SUCCESS] Loaded model '{name}' from legacy pickle: {pkl_path}\")\n",
        "            return cloudpickle.load(f)\n",
        "\n",
        "    raise FileNotFoundError(f\"No model file in {model_dir}. Expected 'model.joblib' or 'model.pkl'.\")\n",
        "\n",
        "\n",
        "def _load_model_from_registry_metadata(\n",
        "    name: str,\n",
        "    version: str | None = \"latest\",\n",
        "    base_dir: Path = DEFAULT_DIR\n",
        ") -> Any:\n",
        "    \"\"\"\n",
        "    Load a model by reading MLflow registry metadata to find the actual storage location.\n",
        "    \n",
        "    Args:\n",
        "        name: Model name\n",
        "        version: Version (or \"latest\")\n",
        "        base_dir: Base directory for model registry\n",
        "        \n",
        "    Returns:\n",
        "        Loaded model or None if not found\n",
        "    \"\"\"\n",
        "    # Find the model registry directory\n",
        "    models_registry_dir = config.PROJECT_ROOT / \"mlruns\" / \"models\" / name\n",
        "    if not models_registry_dir.exists():\n",
        "        raise FileNotFoundError(f\"No model registry directory for '{name}' at {models_registry_dir}\")\n",
        "    \n",
        "    # Find version directories\n",
        "    version_dirs = [d for d in models_registry_dir.iterdir() if d.is_dir()]\n",
        "    if not version_dirs:\n",
        "        raise FileNotFoundError(f\"No version directories for '{name}' in {models_registry_dir}\")\n",
        "    \n",
        "    # Select version directory\n",
        "    if version == \"latest\" or version is None:\n",
        "        # Sort by version number (extract number from version-X)\n",
        "        version_dirs.sort(key=lambda d: int(d.name.split('-')[1]) if '-' in d.name else 0, reverse=True)\n",
        "        version_dir = version_dirs[0]\n",
        "    else:\n",
        "        version_dir = models_registry_dir / f\"version-{version}\"\n",
        "        if not version_dir.exists():\n",
        "            raise FileNotFoundError(f\"Version directory '{version_dir}' not found\")\n",
        "    \n",
        "    print(f\"[DEBUG] Reading metadata from: {version_dir}\")\n",
        "    \n",
        "    # Read metadata\n",
        "    meta_file = version_dir / \"meta.yaml\"\n",
        "    if not meta_file.exists():\n",
        "        raise FileNotFoundError(f\"Metadata file not found: {meta_file}\")\n",
        "    \n",
        "    with open(meta_file, 'r') as f:\n",
        "        metadata = yaml.safe_load(f)\n",
        "    \n",
        "    # Extract storage location\n",
        "    storage_location = metadata.get('storage_location', '')\n",
        "    if not storage_location:\n",
        "        raise ValueError(f\"No storage_location found in metadata for '{name}'\")\n",
        "    \n",
        "    print(f\"[DEBUG] Storage location: {storage_location}\")\n",
        "    \n",
        "    # Parse storage location to find artifacts\n",
        "    if storage_location.startswith('file://'):\n",
        "        # Remove file:// prefix and handle path conversion\n",
        "        artifacts_path = storage_location[7:]  # Remove 'file://'\n",
        "        \n",
        "        # Convert workspace path to actual path\n",
        "        if artifacts_path.startswith('/workspace/'):\n",
        "            # Replace /workspace/ with the actual project root\n",
        "            relative_path = artifacts_path[11:]  # Remove '/workspace/'\n",
        "            artifacts_path = config.PROJECT_ROOT / relative_path\n",
        "        else:\n",
        "            # Handle other file:// paths\n",
        "            artifacts_path = Path(artifacts_path)\n",
        "        \n",
        "        print(f\"[DEBUG] Artifacts path: {artifacts_path}\")\n",
        "        \n",
        "        if not artifacts_path.exists():\n",
        "            raise FileNotFoundError(f\"Artifacts directory not found: {artifacts_path}\")\n",
        "        \n",
        "        # Try to load the model from artifacts\n",
        "        # First try model.pkl (most common for sklearn models)\n",
        "        pkl_path = artifacts_path / \"model.pkl\"\n",
        "        if pkl_path.exists():\n",
        "            print(f\"[DEBUG] Loading from: {pkl_path}\")\n",
        "            import cloudpickle\n",
        "            with open(pkl_path, \"rb\") as f:\n",
        "                return cloudpickle.load(f)\n",
        "        \n",
        "        # Then try model.joblib\n",
        "        joblib_path = artifacts_path / \"model.joblib\"\n",
        "        if joblib_path.exists():\n",
        "            print(f\"[DEBUG] Loading from: {joblib_path}\")\n",
        "            return joblib.load(joblib_path)\n",
        "        \n",
        "        # Check what files are actually available\n",
        "        available_files = list(artifacts_path.iterdir())\n",
        "        print(f\"[WARNING] Available files in artifacts: {[f.name for f in available_files]}\")\n",
        "        \n",
        "        raise FileNotFoundError(f\"No model file found in {artifacts_path}. Expected 'model.pkl' or 'model.joblib'\")\n",
        "    \n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported storage location format: {storage_location}\")\n",
        "\n",
        "from mlflow.tracking import MlflowClient\n",
        "\n",
        "def get_best_model_info(name: str) -> tuple[str, float | None]:\n",
        "    \"\"\"\n",
        "    Return the latest registered version and its accuracy for `name`.\n",
        "    \"\"\"\n",
        "    client = MlflowClient()\n",
        "    versions = client.get_latest_versions(name)\n",
        "    if not versions:\n",
        "        raise ValueError(f\"No registered versions found for '{name}'\")\n",
        "    latest_ver = versions[0].version\n",
        "    metrics = get_best_metrics(name)\n",
        "    acc = metrics.get(\"accuracy\") if metrics else None\n",
        "    return latest_ver, acc\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Clean up existing models first\n",
        "    cleanup_all_models()\n",
        "    print(\"🧹 Cleaned up existing models and MLflow runs\")\n",
        "    # # Re-initialize MLflow now that mlruns/ is fresh\n",
        "    # mlflow.set_experiment(config.MLFLOW_EXPERIMENT_NAME)\n",
        "\n",
        "    import numpy as np\n",
        "    import pymc as pm\n",
        "\n",
        "    # 1️⃣ Dummy test with metrics\n",
        "    class Dummy:\n",
        "        def __init__(self, x):\n",
        "            self.x = x\n",
        "            \n",
        "        def predict(self, X):\n",
        "            return np.array([self.x] * len(X))\n",
        "\n",
        "    dummy = Dummy(42)\n",
        "    uri = save_model(dummy, name=\"dummy_model\", metrics=config.metrics)\n",
        "    \n",
        "    # Try saving a worse model - should be rejected\n",
        "    dummy2 = Dummy(43)\n",
        "    worse_metrics = {\"accuracy\": 0.80, \"f1\": 0.78}\n",
        "    uri2 = save_model(dummy2, name=\"dummy_model\", metrics=worse_metrics)\n",
        "    assert uri2 is None, \"Worse model should have been rejected\"\n",
        "    \n",
        "    # Try saving a better model - should be accepted\n",
        "    dummy3 = Dummy(44)\n",
        "    better_metrics = {\"accuracy\": 0.90, \"f1\": 0.88}\n",
        "    uri3 = save_model(dummy3, name=\"dummy_model\", metrics=better_metrics)\n",
        "    assert uri3 is not None, \"Better model should have been accepted\"\n",
        "    \n",
        "    loaded_dummy = load_model(\"dummy_model\")\n",
        "    #assert isinstance(loaded_dummy, Dummy)\n",
        "    print(\"✅ Dummy save/load with metrics passed!\")\n",
        "\n",
        "    # 2️⃣ LogisticRegression test with metrics\n",
        "    from sklearn.linear_model import LogisticRegression\n",
        "    X, y = [[0,0],[1,1],[1,0],[0,1]], [0,1,1,0]\n",
        "    model = LogisticRegression().fit(X, y)\n",
        "    metrics = {\"accuracy\": 0.75, \"f1\": 0.73}\n",
        "    save_model(model, name=\"logreg_test\", metrics=metrics)\n",
        "    reloaded = load_model(\"logreg_test\")\n",
        "    assert (reloaded.predict(X) == model.predict(X)).all()\n",
        "    print(\"✅ LogisticRegression save/load with metrics passed!\")\n",
        "\n",
        "    # 3️⃣ RandomForestClassifier test with metrics\n",
        "    from sklearn.ensemble import RandomForestClassifier\n",
        "    rf = RandomForestClassifier(n_estimators=10, random_state=0).fit(X, y)\n",
        "    metrics = {\"accuracy\": 0.85, \"f1\": 0.83}\n",
        "    save_model(rf, name=\"rf_test\", metrics=metrics)\n",
        "    rf2 = load_model(\"rf_test\")\n",
        "    assert (rf2.predict(X) == rf.predict(X)).all()\n",
        "    print(\"✅ RandomForestClassifier save/load with metrics passed!\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Traditional Models Module\n",
        "\n",
        "Implementation of traditional machine learning models for comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/nfl_kicker_analysis/models/tree_based_bayes_optimized_models.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/nfl_kicker_analysis/models/tree_based_bayes_optimized_models.py\n",
        "\"\"\"\n",
        "Traditional ML models for NFL kicker analysis.\n",
        "Includes simple logistic regression, ridge logistic regression, and random forest.\n",
        "Each model can be optionally tuned using Bayesian optimization with Optuna.\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from typing import Dict, Tuple, Optional, Union, Any\n",
        "import optuna\n",
        "import xgboost as xgb\n",
        "from catboost import CatBoostClassifier\n",
        "from numpy.typing import NDArray\n",
        "\n",
        "from src.nfl_kicker_analysis.utils.metrics import ModelEvaluator\n",
        "from src.nfl_kicker_analysis.utils.metrics import EPACalculator\n",
        "from src.nfl_kicker_analysis.config import config, FEATURE_LISTS\n",
        "from src.nfl_kicker_analysis.utils.model_utils import save_model, load_model\n",
        "from src.nfl_kicker_analysis.data.preprocessor import DataPreprocessor\n",
        "import scipy.special  # new import\n",
        "\n",
        "def predict_proba_for_ridge(model, X):\n",
        "    \"\"\"\n",
        "    Fallback for RidgeClassifier: use decision_function + sigmoid to approximate probabilities.\n",
        "    \"\"\"\n",
        "    scores = model.decision_function(X)\n",
        "    probs  = scipy.special.expit(scores)\n",
        "    return np.vstack([1 - probs, probs]).T\n",
        "\n",
        "\n",
        "class TreeBasedModelSuite:\n",
        "    def __init__(self, *, feature_lists: dict[str, list[str]] | None = None):\n",
        "        \"\"\"Initialise evaluator, preprocessor & CV splitter.\"\"\"\n",
        "        self.fitted_models: dict[str, Any] = {}\n",
        "        self.evaluator      = ModelEvaluator()\n",
        "        self._tss           = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "        # One shared preprocessor per suite\n",
        "        self.preprocessor = DataPreprocessor()\n",
        "        if feature_lists is not None:\n",
        "            # Keep config MIN/MAX distance etc. but overwrite column roles\n",
        "            self.preprocessor.update_feature_lists(**feature_lists)\n",
        "        \n",
        "    def prepare_features(self, data: pd.DataFrame) -> Tuple[NDArray[np.float_], NDArray[np.float_], NDArray[np.int_], OneHotEncoder]:\n",
        "        \"\"\"\n",
        "        Prepare feature matrices for modeling.\n",
        "        \n",
        "        Args:\n",
        "            data: DataFrame with attempt_yards and kicker_id\n",
        "            \n",
        "        Returns:\n",
        "            Tuple of:\n",
        "            - Distance-only features\n",
        "            - Combined features (distance + one-hot kicker)\n",
        "            - Kicker IDs\n",
        "            - OneHotEncoder for kickers\n",
        "        \"\"\"\n",
        "        # Distance features\n",
        "        X_distance = data['attempt_yards'].values.astype(np.float_).reshape(-1, 1)\n",
        "        \n",
        "        # Kicker IDs for tree models (fall back to player_id)\n",
        "        ids_col = 'kicker_id' if 'kicker_id' in data.columns else 'player_id'\n",
        "        kicker_ids = data[ids_col].values.astype(np.int_).reshape(-1, 1)\n",
        "        \n",
        "        # One-hot encode kickers for linear models\n",
        "        encoder = OneHotEncoder(sparse_output=True)\n",
        "        kicker_onehot = encoder.fit_transform(kicker_ids)\n",
        "        X_combined = np.hstack([X_distance, kicker_onehot.toarray()])\n",
        "        \n",
        "        return X_distance, X_combined, kicker_ids, encoder\n",
        "    \n",
        "    def create_time_split(self, data: pd.DataFrame) -> Tuple[NDArray[np.int_], NDArray[np.int_]]:\n",
        "        \"\"\"\n",
        "        Create train/test split by time.\n",
        "        \n",
        "        Args:\n",
        "            data: DataFrame with game_date\n",
        "            \n",
        "        Returns:\n",
        "            Train and test indices\n",
        "        \"\"\"\n",
        "        train_mask = data['season'] <= 2017\n",
        "        test_mask = data['season'] == 2018\n",
        "        \n",
        "        train_idx = np.where(train_mask)[0]\n",
        "        test_idx = np.where(test_mask)[0]\n",
        "        \n",
        "        print(f\"Train: {len(train_idx):,} attempts ({train_mask.mean():.1%})\")\n",
        "        print(f\"Test: {len(test_idx):,} attempts ({test_mask.mean():.1%})\")\n",
        "        \n",
        "        return train_idx, test_idx\n",
        "\n",
        "    def _tune_simple_logistic_optuna(\n",
        "        self,\n",
        "        X: NDArray[np.float_],\n",
        "        y: NDArray[np.int_],\n",
        "        n_trials: int | None = None,\n",
        "    ) -> LogisticRegression:\n",
        "        \"\"\"Bayesian-optimize a simple LogisticRegression.\"\"\"\n",
        "        n_trials = n_trials or config.OPTUNA_TRIALS[\"simple_logistic\"]\n",
        "        \n",
        "        def objective(trial: optuna.Trial) -> float:\n",
        "            C = trial.suggest_float(\"C\", 1e-5, 100, log=True)\n",
        "            model = LogisticRegression(C=C, random_state=42)\n",
        "            model.fit(X, y)\n",
        "            return model.score(X, y)\n",
        "            \n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
        "        \n",
        "        # Get best params and refit\n",
        "        best_C = study.best_params[\"C\"]\n",
        "        best_model = LogisticRegression(C=best_C, random_state=42)\n",
        "        best_model.fit(X, y)\n",
        "        return best_model\n",
        "        \n",
        "    def _tune_ridge_logistic_optuna(\n",
        "        self,\n",
        "        X: NDArray[np.float_],\n",
        "        y: NDArray[np.int_],\n",
        "        n_trials: int | None = None,\n",
        "    ) -> RidgeClassifier:\n",
        "        \"\"\"Bayesian-optimize a RidgeClassifier.\"\"\"\n",
        "        n_trials = n_trials or config.OPTUNA_TRIALS[\"ridge_logistic\"]\n",
        "        \n",
        "        def objective(trial: optuna.Trial) -> float:\n",
        "            alpha = trial.suggest_float(\"alpha\", 1e-5, 100, log=True)\n",
        "            model = RidgeClassifier(alpha=alpha, random_state=42)\n",
        "            model.fit(X, y)\n",
        "            return model.score(X, y)\n",
        "            \n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
        "        \n",
        "        # Get best params and refit\n",
        "        best_alpha = study.best_params[\"alpha\"]\n",
        "        best_model = RidgeClassifier(alpha=best_alpha, random_state=42)\n",
        "        best_model.fit(X, y)\n",
        "        return best_model\n",
        "        \n",
        "    def _tune_random_forest_optuna(\n",
        "        self,\n",
        "        X: NDArray[np.float_],\n",
        "        y: NDArray[np.int_],\n",
        "        n_trials: int | None = None,\n",
        "    ) -> RandomForestClassifier:\n",
        "        \"\"\"Bayesian-optimize a RandomForestClassifier.\"\"\"\n",
        "        n_trials = n_trials or config.OPTUNA_TRIALS[\"random_forest\"]\n",
        "        \n",
        "        def objective(trial: optuna.Trial) -> float:\n",
        "            params = {\n",
        "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
        "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "                \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 20),\n",
        "                \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 10)\n",
        "            }\n",
        "            model = RandomForestClassifier(**params, random_state=42)\n",
        "            model.fit(X, y)\n",
        "            return model.score(X, y)\n",
        "            \n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
        "        \n",
        "        # Get best params and refit\n",
        "        best_params = study.best_params\n",
        "        best_model = RandomForestClassifier(**best_params, random_state=42)\n",
        "        best_model.fit(X, y)\n",
        "        return best_model\n",
        "        \n",
        "    def _tune_xgboost_optuna(\n",
        "        self,\n",
        "        X: NDArray[np.float_],\n",
        "        y: NDArray[np.int_],\n",
        "        n_trials: int | None = None,\n",
        "    ) -> xgb.XGBClassifier:\n",
        "        \"\"\"Bayesian-optimize an XGBoost classifier.\"\"\"\n",
        "        n_trials = n_trials or config.OPTUNA_TRIALS[\"xgboost\"]\n",
        "        \n",
        "        def objective(trial: optuna.Trial) -> float:\n",
        "            params = {\n",
        "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
        "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 50, 300),\n",
        "                \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 7),\n",
        "                \"subsample\": trial.suggest_float(\"subsample\", 0.6, 1.0),\n",
        "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.6, 1.0)\n",
        "            }\n",
        "            model = xgb.XGBClassifier(**params, random_state=42)\n",
        "            model.fit(X, y)\n",
        "            return model.score(X, y)\n",
        "            \n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
        "        \n",
        "        # Get best params and refit\n",
        "        best_params = study.best_params\n",
        "        best_model = xgb.XGBClassifier(**best_params, random_state=42)\n",
        "        best_model.fit(X, y)\n",
        "        return best_model\n",
        "        \n",
        "    def _tune_catboost_optuna(\n",
        "        self,\n",
        "        X: np.ndarray,\n",
        "        y: np.ndarray,\n",
        "        n_trials: int | None = None,\n",
        "    ) -> CatBoostClassifier:\n",
        "        \"\"\"Bayesian-optimize a CatBoost classifier on numeric array.\"\"\"\n",
        "        n_trials = n_trials or config.OPTUNA_TRIALS[\"catboost\"]\n",
        "\n",
        "        def objective(trial: optuna.Trial) -> float:\n",
        "            params = {\n",
        "                \"iterations\": trial.suggest_int(\"iterations\", 50, 300),\n",
        "                \"depth\":      trial.suggest_int(\"depth\", 3, 10),\n",
        "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n",
        "                \"l2_leaf_reg\":    trial.suggest_float(\"l2_leaf_reg\", 1e-8, 10.0, log=True)\n",
        "            }\n",
        "            model = CatBoostClassifier(**params, random_state=42, verbose=False)\n",
        "            model.fit(X, y)\n",
        "            return model.score(X, y)\n",
        "\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(objective, n_trials=n_trials, show_progress_bar=False)\n",
        "\n",
        "        best_params = study.best_params\n",
        "        best_model = CatBoostClassifier(**best_params, random_state=42, verbose=False)\n",
        "        best_model.fit(X, y)\n",
        "        return best_model\n",
        "\n",
        "\n",
        "    def fit_all_models(self, data: pd.DataFrame) -> Dict[str, Dict[str, float]]:\n",
        "        \"\"\"\n",
        "        Fit all traditional & boosted models and return their metrics.\n",
        "        Also saves pipelines, leaderboards, and metrics to disk.\n",
        "        \"\"\"\n",
        "        print(\"🔍 [fit_all_models] columns:\", list(data.columns))\n",
        "\n",
        "        # Ensure 'success' exists\n",
        "        if \"success\" not in data.columns:\n",
        "            if \"field_goal_result\" in data.columns:\n",
        "                print(\"🔄 Deriving 'success' from 'field_goal_result'\")\n",
        "                data = data.copy()\n",
        "                data[\"success\"] = (data[\"field_goal_result\"] == \"Made\").astype(int)\n",
        "            else:\n",
        "                raise KeyError(\"fit_all_models expects a 'success' column or 'field_goal_result'\")\n",
        "\n",
        "        # Preprocess and transform\n",
        "        processed = self.preprocessor.preprocess_complete(data)\n",
        "        X_full, y = self.preprocessor.fit_transform_features()\n",
        "        train_idx, test_idx = self.create_time_split(processed)\n",
        "\n",
        "        # Train each model\n",
        "        print(\"\\nTraining models on transformed features...\")\n",
        "        self.fitted_models['simple_logistic'] = self._tune_simple_logistic_optuna(\n",
        "            X_full[train_idx], y[train_idx]\n",
        "        )\n",
        "        self.fitted_models['ridge_logistic'] = self._tune_ridge_logistic_optuna(\n",
        "            X_full[train_idx], y[train_idx]\n",
        "        )\n",
        "        self.fitted_models['random_forest'] = self._tune_random_forest_optuna(\n",
        "            X_full[train_idx], y[train_idx]\n",
        "        )\n",
        "        self.fitted_models['xgboost'] = self._tune_xgboost_optuna(\n",
        "            X_full[train_idx], y[train_idx]\n",
        "        )\n",
        "        self.fitted_models['catboost'] = self._tune_catboost_optuna(\n",
        "            X_full[train_idx], y[train_idx]\n",
        "        )\n",
        "\n",
        "        # Evaluate on hold-out\n",
        "        print(\"\\nEvaluating models on hold-out set…\")\n",
        "        metrics: Dict[str, Dict[str, float]] = {}\n",
        "        for name, model in self.fitted_models.items():\n",
        "            df_slice = processed.iloc[test_idx].reset_index(drop=True)\n",
        "            X_test = self.preprocessor.transform_features(df_slice)\n",
        "            if hasattr(model, \"predict_proba\"):\n",
        "                y_pred = model.predict_proba(X_test)[:, 1]\n",
        "            else:\n",
        "                y_pred = predict_proba_for_ridge(model, X_test)[:, 1]\n",
        "            y_true = data[\"success\"].values[test_idx]\n",
        "            metrics[name] = self.evaluator.calculate_classification_metrics(y_true, y_pred)\n",
        "\n",
        "        # Persist pipelines\n",
        "        from sklearn.pipeline import Pipeline\n",
        "        for name, model in self.fitted_models.items():\n",
        "            pipeline = Pipeline([\n",
        "                (\"transformer\", self.preprocessor.column_transformer_),\n",
        "                (\"model\", model),\n",
        "            ])\n",
        "            save_model(pipeline, name, metrics=metrics[name])\n",
        "\n",
        "        # Save leaderboards for each model\n",
        "        for name in self.fitted_models.keys():\n",
        "            lb = self.get_epa_leaderboard(data, model_name=name, top_n=10)\n",
        "            lb_path = config.OUTPUT_DIR / f\"{name}_leaderboard.csv\"\n",
        "            lb.to_csv(lb_path, index=False)\n",
        "            print(f\"✅ Saved {name} leaderboard to {lb_path}\")\n",
        "\n",
        "        # Save metrics summary\n",
        "        metrics_df = pd.DataFrame(metrics).T\n",
        "        metrics_path = config.OUTPUT_DIR / \"model_metrics.csv\"\n",
        "        metrics_df.to_csv(metrics_path)\n",
        "        print(f\"✅ Saved model metrics to {metrics_path}\")\n",
        "\n",
        "        return metrics\n",
        "\n",
        "\n",
        "    def predict(self, model_name: str, data: pd.DataFrame) -> NDArray[np.float_]:\n",
        "        \"\"\"\n",
        "        Predict probabilities with any fitted model in the suite,\n",
        "        applying the same preprocessing pipeline as used at training.\n",
        "        \"\"\"\n",
        "        if model_name not in self.fitted_models:\n",
        "            raise ValueError(f\"Model {model_name} not fitted\")\n",
        "\n",
        "        model = self.fitted_models[model_name]\n",
        "\n",
        "        # -- Use the preprocessor's transform_features for all models --\n",
        "        X = self.preprocessor.transform_features(data)\n",
        "\n",
        "        # -- Obtain probabilities whether predict_proba exists or not --\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            probs = model.predict_proba(X)[:, 1]\n",
        "        else:\n",
        "            # Fallback for RidgeClassifier using logistic sigmoid\n",
        "            from scipy.special import expit\n",
        "            scores = model.decision_function(X)\n",
        "            p = expit(scores)  # logistic(sigmoid) transform\n",
        "            probs = p  # already the positive-class probability\n",
        "\n",
        "        return probs.astype(np.float_)\n",
        "\n",
        "\n",
        "    def get_feature_importance(self, model_name: str) -> Optional[NDArray[np.float_]]:\n",
        "        \"\"\"\n",
        "        Get feature importance for tree-based models.\n",
        "        \"\"\"\n",
        "        if model_name not in self.fitted_models:\n",
        "            return None\n",
        "\n",
        "        model = self.fitted_models[model_name]\n",
        "\n",
        "        if model_name in {\"catboost\"}:\n",
        "            return model.get_feature_importance().astype(np.float_)\n",
        "        elif hasattr(model, \"feature_importances_\"):\n",
        "            return model.feature_importances_.astype(np.float_)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def get_epa_leaderboard(\n",
        "        self,\n",
        "        data: pd.DataFrame,\n",
        "        model_name: str = \"random_forest\",\n",
        "        top_n: int = 10\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Generate EPA leaderboard using predictions from specified model.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        data : DataFrame with required columns\n",
        "        model_name : which model to use for predictions\n",
        "        top_n : how many kickers to include (default 10)\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        DataFrame with columns:\n",
        "            rank, player_name, attempts, success_rate, epa_per_attempt\n",
        "        \"\"\"\n",
        "        # Get predictions\n",
        "        y_pred = self.predict(model_name, data)\n",
        "        \n",
        "        # Compute EPA\n",
        "        data = data.copy()\n",
        "        data[\"pred_prob\"] = y_pred\n",
        "        data[\"epa\"] = 3 * (data[\"success\"] - data[\"pred_prob\"])\n",
        "        \n",
        "        # Build leaderboard\n",
        "        lb = (data.groupby([\"player_id\", \"player_name\"])\n",
        "                  .agg(\n",
        "                      attempts=(\"success\", \"size\"),\n",
        "                      success_rate=(\"success\", \"mean\"),\n",
        "                      epa_per_attempt=(\"epa\", \"mean\")\n",
        "                  )\n",
        "                  .reset_index()\n",
        "                  .sort_values(\"epa_per_attempt\", ascending=False)\n",
        "                  .head(top_n)\n",
        "                  .reset_index(drop=True))\n",
        "        lb[\"rank\"] = lb.index + 1\n",
        "        \n",
        "        return lb[[\"rank\", \"player_name\", \"attempts\", \"success_rate\", \"epa_per_attempt\"]]\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    from src.nfl_kicker_analysis.data.loader import DataLoader\n",
        "    from src.nfl_kicker_analysis.data.preprocessor import DataPreprocessor\n",
        "\n",
        "    # 1️⃣ Ensure directories\n",
        "    config.ensure_directories()\n",
        "\n",
        "    # 2️⃣ Load & preprocess\n",
        "    loader = DataLoader()\n",
        "    raw = loader.merge_datasets()\n",
        "    pre = DataPreprocessor()\n",
        "\n",
        "    processed = pre.preprocess_slice(raw)\n",
        "\n",
        "    # 3️⃣ Train & save\n",
        "    suite = TreeBasedModelSuite()\n",
        "    metrics = suite.fit_all_models(processed)\n",
        "    print(\"\\n📊 Final metrics:\")\n",
        "    for name, m in metrics.items():\n",
        "        print(f\"  • {name}: {m}\")\n",
        "\n",
        "    # 4️⃣ Generate, save & print RF leaderboard\n",
        "    try:\n",
        "        rf_lb = suite.get_epa_leaderboard(processed, model_name=\"random_forest\", top_n=10)\n",
        "        out_path = config.OUTPUT_DIR / \"rf_leaderboard.csv\"\n",
        "        rf_lb.to_csv(out_path, index=False)\n",
        "        print(f\"\\n✅ Saved RF leaderboard to {out_path}\\n\")\n",
        "        print(\"🏆 Top-10 RF EPA Leaderboard:\")\n",
        "        print(rf_lb.to_string(index=False))\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Could not build RF leaderboard: {e}\")\n",
        "\n",
        "    print(\"\\n🔄 Loading saved 'random_forest' pipeline for inference…\")\n",
        "    rf_pipe = load_model(\"random_forest\")\n",
        "    sample = processed.iloc[:5]   \n",
        "    preds  = rf_pipe.predict_proba(sample)[:, 1]\n",
        "    print(\"Sample RF preds:\", preds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/nfl_kicker_analysis/models/bayes_model_utils.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/nfl_kicker_analysis/models/bayes_model_utils.py\n",
        "import numpy as np\n",
        "import pymc as pm            # PyMC v5+ core API\n",
        "import arviz as az           # InferenceData container and I/O\n",
        "\n",
        "def fit_bayesian_logreg(\n",
        "    X: np.ndarray,\n",
        "    y: np.ndarray,\n",
        "    draws: int = 1000,\n",
        "    tune: int = 1000,\n",
        "    chains: int = 2,\n",
        "    random_seed: int | None = None\n",
        ") -> tuple[pm.Model, az.InferenceData]:\n",
        "    \"\"\"\n",
        "    Fit a Bayesian logistic regression model using PyMC v5 with mutable Data containers.\n",
        "    Returns the compiled model and its InferenceData.\n",
        "    \"\"\"\n",
        "    with pm.Model() as model:\n",
        "        # ▶️ Register data as mutable containers for easy swapping later\n",
        "        X_shared = pm.Data(\"X_shared\", X)                      # :contentReference[oaicite:9]{index=9}\n",
        "        y_shared = pm.Data(\"y_shared\", y)\n",
        "\n",
        "        # ▶️ Prior definitions\n",
        "        alpha = pm.Normal(\"alpha\", 0, 5)\n",
        "        betas = pm.Normal(\"betas\", 0, 5, shape=X.shape[1])\n",
        "\n",
        "        # ▶️ Likelihood\n",
        "        logit_p = alpha + pm.math.dot(X_shared, betas)\n",
        "        pm.Bernoulli(\"obs\", logit_p=logit_p, observed=y_shared)\n",
        "\n",
        "        # ▶️ Sample; returns an ArviZ InferenceData container by default\n",
        "        idata = pm.sample(\n",
        "            draws=draws,\n",
        "            tune=tune,\n",
        "            chains=chains,\n",
        "            random_seed=random_seed,\n",
        "            return_inferencedata=True\n",
        "        )                                                     # :contentReference[oaicite:10]{index=10}\n",
        "    return model, idata\n",
        "\n",
        "def save_pymc_inference(\n",
        "    idata: az.InferenceData,\n",
        "    path: str\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Persist an ArviZ InferenceData object to a NetCDF file.\n",
        "    Returns the filepath written.\n",
        "    \"\"\"\n",
        "    # ▶️ Write to NetCDF; self-describing, versioned archive\n",
        "    filepath = idata.to_netcdf(path)                        # :contentReference[oaicite:11]{index=11}\n",
        "    return filepath\n",
        "\n",
        "def load_pymc_inference(\n",
        "    path: str\n",
        ") -> az.InferenceData:\n",
        "    \"\"\"\n",
        "    Reload a NetCDF file into an ArviZ InferenceData object.\n",
        "    \"\"\"\n",
        "    idata = az.InferenceData.from_netcdf(path)               # :contentReference[oaicite:12]{index=12}\n",
        "    return idata\n",
        "\n",
        "def predict_bayesian_logreg(\n",
        "    model: pm.Model,\n",
        "    idata: az.InferenceData,\n",
        "    X_new: np.ndarray,\n",
        "    var_name: str = \"obs\",\n",
        "    random_seed: int | None = None\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Perform out-of-sample posterior predictive classification.\n",
        "    Swaps in X_new via pm.set_data, samples predictive draws, and thresholds at 0.5.\n",
        "    \"\"\"\n",
        "    # ▶️ Swap in new data\n",
        "    pm.set_data({\"X_shared\": X_new}, model=model)            # :contentReference[oaicite:13]{index=13}\n",
        "\n",
        "    # ▶️ Generate posterior predictive draws as an InferenceData\n",
        "    ppc = pm.sample_posterior_predictive(\n",
        "        idata,\n",
        "        model=model,\n",
        "        var_names=[var_name],\n",
        "        random_seed=random_seed,\n",
        "        progressbar=False,\n",
        "        return_inferencedata=True\n",
        "    )                                                        # :contentReference[oaicite:14]{index=14}\n",
        "\n",
        "    # ▶️ Extract samples and compute mean probabilities\n",
        "    samples = ppc.posterior_predictive[var_name].values      # shape (chain, draw, obs)\n",
        "    mean_preds = samples.mean(axis=(0, 1))\n",
        "    return (mean_preds > 0.5).astype(int)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # from src.nfl_kicker_analysis.utils.model_utils import (\n",
        "    #     fit_bayesian_logreg,\n",
        "    #     save_pymc_inference,\n",
        "    #     load_pymc_inference,\n",
        "    #     predict_bayesian_logreg\n",
        "    # )\n",
        "    import numpy as np\n",
        "\n",
        "    # ▶️ Simulate toy data\n",
        "    rng = np.random.default_rng(0)\n",
        "    X = rng.normal(size=(100, 2))\n",
        "    y = rng.integers(0, 2, size=100)\n",
        "\n",
        "    # ▶️ Fit model\n",
        "    model, idata = fit_bayesian_logreg(X, y, draws=200, tune=100, chains=1)\n",
        "    # ▶️ Save / reload inference\n",
        "    nc_path = save_pymc_inference(idata, \"test_bayes_logreg.nc\")\n",
        "    idata2 = load_pymc_inference(nc_path)\n",
        "    # ▶️ Predict on training data\n",
        "    preds = predict_bayesian_logreg(model, idata2, X, random_seed=1)\n",
        "\n",
        "    # ▶️ Simple consistency check\n",
        "    assert preds.shape == y.shape\n",
        "    print(\"✅ PyMC Bayesian logistic regression end-to-end smoke test passed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting src/nfl_kicker_analysis/models/bayesian.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile src/nfl_kicker_analysis/models/bayesian.py\n",
        "\"\"\"\n",
        "Bayesian models for NFL kicker analysis.\n",
        "Provides hierarchical Bayesian logistic regression and evaluation utilities using PyMC.\n",
        "\"\"\"\n",
        "from __future__ import annotations\n",
        "\n",
        "import numpy as np                             # Core numerical arrays\n",
        "import pandas as pd                            # DataFrame handling\n",
        "import pymc as pm                              # Bayesian modeling\n",
        "import arviz as az                             # Posterior analysis\n",
        "from arviz.data.inference_data import InferenceData\n",
        "import xarray as xr                            # 📥 Required for posterior casting :contentReference[oaicite:6]{index=6}\n",
        "import matplotlib.pyplot as plt                # Plot utilities\n",
        "from matplotlib.axes import Axes\n",
        "from scipy import stats                        # ECDF and stats functions\n",
        "import time                                    # Timestamps for file naming\n",
        "import json                                    # JSON metadata\n",
        "from pathlib import Path\n",
        "from typing import (\n",
        "    Dict, Any, TYPE_CHECKING, Optional, Union,\n",
        "    List, Tuple, cast, Protocol\n",
        ")\n",
        "\n",
        "from src.nfl_kicker_analysis.utils.metrics import ModelEvaluator\n",
        "from src.nfl_kicker_analysis.config import config, FEATURE_LISTS  # Move import to module level\n",
        "from src.nfl_kicker_analysis.data.preprocessor import DataPreprocessor\n",
        "from src.nfl_kicker_analysis.eda import _FIELD_GOAL_RESULT_SUCCESS  # NEW: Import success constant\n",
        "\n",
        "\n",
        "from xarray import Dataset\n",
        "import xarray as xr\n",
        "from numpy.typing import ArrayLike\n",
        "\n",
        "__all__ = [\n",
        "    \"BayesianModelSuite\",\n",
        "]\n",
        "\n",
        "# Add debug flag\n",
        "DEBUG = True\n",
        "\n",
        "def debug_print(*args, **kwargs):\n",
        "    if DEBUG:\n",
        "        print(\"🔍 DEBUG:\", *args, **kwargs)\n",
        "\n",
        "class HasPosterior(Protocol):\n",
        "    \"\"\"Protocol for objects with posterior attribute.\"\"\"\n",
        "    posterior: Any\n",
        "\n",
        "class HasPosteriorPredictive(Protocol):\n",
        "    \"\"\"Protocol for objects with posterior_predictive attribute.\"\"\"\n",
        "    posterior_predictive: Any\n",
        "\n",
        "def _get_posterior_mean(trace: InferenceData, var_name: str) -> Union[float, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Safely extract posterior means from an ArviZ InferenceData object.\n",
        "    \"\"\"\n",
        "    if not hasattr(trace, \"posterior\"):\n",
        "        raise RuntimeError(\"InferenceData object has no posterior group\")\n",
        "    # Access posterior using getattr to handle dynamic attributes\n",
        "    posterior = getattr(trace, \"posterior\")\n",
        "    if var_name not in posterior:\n",
        "        raise KeyError(f\"Variable {var_name} not found in posterior\")\n",
        "    # Collapse over chain & draw dims to get the mean\n",
        "    result = posterior[var_name].mean((\"chain\", \"draw\")).values\n",
        "    return float(result) if isinstance(result, (float, np.floating)) else result\n",
        "\n",
        "def _get_posterior_predictive(trace: InferenceData, var_name: str) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Safely extract posterior predictive samples from an ArviZ InferenceData object.\n",
        "    \"\"\"\n",
        "    if not hasattr(trace, \"posterior_predictive\"):\n",
        "        raise RuntimeError(\"InferenceData object has no posterior_predictive group\")\n",
        "    pp = getattr(trace, \"posterior_predictive\")\n",
        "    if var_name not in pp:\n",
        "        raise KeyError(f\"Variable {var_name} not found in posterior_predictive\")\n",
        "    return np.asarray(pp[var_name].values)\n",
        "\n",
        "\n",
        "def _plot_comparison(\n",
        "    ax: Axes,\n",
        "    x_vals: ArrayLike,\n",
        "    actual: ArrayLike,\n",
        "    predicted: ArrayLike,\n",
        "    xlabel: str,\n",
        "    title: str\n",
        ") -> None:\n",
        "    \"\"\"Helper to plot actual vs predicted values.\"\"\"\n",
        "    ax.plot(np.asarray(x_vals), np.asarray(actual), marker=\"o\", label=\"Actual\", linewidth=2)\n",
        "    ax.plot(np.asarray(x_vals), np.asarray(predicted), marker=\"s\", label=\"Posterior mean\", linestyle=\"--\")\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel(\"FG make probability\")\n",
        "    ax.set_title(title)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "\n",
        "class BayesianModelSuite:\n",
        "    \"\"\"Hierarchical Bayesian logistic‑regression models for kicker analysis.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        *,\n",
        "        draws: int = 1_000,\n",
        "        tune: int = 1_000,\n",
        "        target_accept: float = 0.95,  # Increased from 0.9 to reduce divergences\n",
        "        include_random_slope: bool = False,\n",
        "        random_seed: Optional[int] = 42,\n",
        "    ) -> None:\n",
        "        debug_print(\"Initializing BayesianModelSuite\")\n",
        "        \n",
        "        # Validate configuration\n",
        "        required_config = [\n",
        "            'BAYESIAN_MCMC_SAMPLES',\n",
        "            'BAYESIAN_TUNE',\n",
        "            'BAYESIAN_CHAINS',\n",
        "            'MIN_DISTANCE',\n",
        "            'MAX_DISTANCE',\n",
        "            'MIN_KICKER_ATTEMPTS',\n",
        "            'SEASON_TYPES'\n",
        "        ]\n",
        "        \n",
        "        missing = [attr for attr in required_config if not hasattr(config, attr)]\n",
        "        if missing:\n",
        "            raise ValueError(f\"Missing required configuration attributes: {missing}\")\n",
        "            \n",
        "        # Override defaults with config values if provided\n",
        "        self.draws = config.BAYESIAN_MCMC_SAMPLES if draws == 1_000 else draws\n",
        "        self.tune = config.BAYESIAN_TUNE if tune == 1_000 else tune\n",
        "        self.target_accept = target_accept\n",
        "        self.include_random_slope = include_random_slope\n",
        "        self.random_seed = random_seed\n",
        "\n",
        "        # Model components - set during fit()\n",
        "        self._model: Optional[pm.Model] = None\n",
        "        self._trace: Optional[InferenceData] = None\n",
        "        self._kicker_map: Dict[int, int] = {}\n",
        "        self._distance_mu: float = 0.0\n",
        "        self._distance_sigma: float = 1.0\n",
        "        self.baseline_probs: Dict[int, float] = {}  # For consistent EPA baselines with EPACalculator\n",
        "        self.evaluator = ModelEvaluator()\n",
        "        debug_print(\"Initialization complete\")\n",
        "\n",
        "    def _bootstrap_distances(\n",
        "        self,\n",
        "        distances: ArrayLike,\n",
        "        n_samples: int,\n",
        "        rng: np.random.Generator,\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Bootstrap sample distances from an empirical distribution.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        distances : array of actual distances to sample from\n",
        "        n_samples : number of bootstrap samples to draw\n",
        "        rng : numpy random number generator\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        ndarray of shape (n_samples,)\n",
        "            Resampled distances with replacement\n",
        "        \"\"\"\n",
        "        return rng.choice(np.asarray(distances), size=n_samples, replace=True)\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # 🛠️  Helper utilities\n",
        "    # ---------------------------------------------------------------------\n",
        "    def _standardize(self, x: np.ndarray, *, fit: bool = False) -> np.ndarray:\n",
        "        if fit:\n",
        "            self._distance_mu = float(x.mean())\n",
        "            self._distance_sigma = float(x.std())\n",
        "        return (x - self._distance_mu) / self._distance_sigma\n",
        "\n",
        "    def _encode_kicker(self, raw_ids: ArrayLike, *, fit: bool = False,\n",
        "                       unknown_action: str = \"average\") -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Map raw kicker IDs → compact indices (kicker_idx).\n",
        "        \"\"\"\n",
        "        raw_ids_arr = np.asarray(raw_ids)\n",
        "        if fit:\n",
        "            unique_ids = np.unique(raw_ids_arr)\n",
        "            debug_print(f\"Fitting kicker map with {len(unique_ids)} unique IDs\")\n",
        "            debug_print(f\"Raw IDs: {unique_ids.tolist()}\")\n",
        "            self._kicker_map = {int(pid): i for i, pid in enumerate(unique_ids)}\n",
        "            debug_print(f\"First few mappings: {dict(list(self._kicker_map.items())[:3])}\")\n",
        "        \n",
        "        # Convert to list of ints for safer lookup\n",
        "        raw_ids_list = [int(pid) for pid in raw_ids_arr]\n",
        "        debug_print(f\"Looking up {len(raw_ids_list)} IDs\")\n",
        "        debug_print(f\"Sample raw IDs: {raw_ids_list[:5]}\")\n",
        "        debug_print(f\"Kicker map size: {len(self._kicker_map)}\")\n",
        "        \n",
        "        # Map IDs to indices, using -1 for unknown\n",
        "        idx = np.array([self._kicker_map.get(pid, -1) for pid in raw_ids_list], dtype=int)\n",
        "        n_unseen = (idx == -1).sum()\n",
        "        \n",
        "        if n_unseen > 0:\n",
        "            msg = f\"{n_unseen} unseen kicker IDs – mapped to league mean.\"\n",
        "            if unknown_action == \"raise\":\n",
        "                raise ValueError(msg)\n",
        "            elif unknown_action == \"warn\":\n",
        "                debug_print(\"⚠️ \" + msg)\n",
        "                debug_print(f\"Unmapped IDs: {[pid for pid in raw_ids_list if pid not in self._kicker_map]}\")\n",
        "\n",
        "        return idx\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # 🔨  Model construction\n",
        "    # ---------------------------------------------------------------------\n",
        "    def _build_model(\n",
        "        self,\n",
        "        distance_std: np.ndarray,\n",
        "        age_c: np.ndarray,           # <-- NEW: centered age\n",
        "        age_c2: np.ndarray,          # <-- NEW: quadratic age\n",
        "        exp_std: np.ndarray,\n",
        "        success: np.ndarray,\n",
        "        kicker_idx: np.ndarray,\n",
        "        n_kickers: int,\n",
        "    ) -> pm.Model:\n",
        "        with pm.Model() as model:\n",
        "            # Population-level effects\n",
        "            alpha = pm.Normal(\"alpha\", 1.5, 1.0)\n",
        "            beta_dist = pm.Normal(\"beta_dist\", -1.5, 0.8)\n",
        "            \n",
        "            # Age effects (linear + quadratic)\n",
        "            beta_age  = pm.Normal(\"beta_age\",  0.0, 0.5)\n",
        "            beta_age2 = pm.Normal(\"beta_age2\", 0.0, 0.5)\n",
        "            beta_exp  = pm.Normal(\"beta_exp\",  0.0, 0.5)\n",
        "\n",
        "            # Per-kicker random intercepts (non-centered)\n",
        "            σ_u   = pm.HalfNormal(\"sigma_u\", 0.8)\n",
        "            u_raw = pm.Normal(\"u_raw\", 0.0, 1.0, shape=n_kickers)\n",
        "            u     = pm.Deterministic(\"u\", σ_u * u_raw)\n",
        "\n",
        "            # Per-kicker random aging slopes (optional enhancement)\n",
        "            if self.include_random_slope:\n",
        "                σ_age = pm.HalfNormal(\"sigma_age\", 0.5)\n",
        "                a_raw = pm.Normal(\"a_raw\", 0.0, 1.0, shape=n_kickers)\n",
        "                a_k   = pm.Deterministic(\"a_k\", σ_age * a_raw)\n",
        "                age_slope_effect = a_k[kicker_idx] * age_c\n",
        "            else:\n",
        "                age_slope_effect = 0.0\n",
        "\n",
        "            # Linear predictor\n",
        "            lin_pred = (\n",
        "                alpha\n",
        "                + (beta_dist * distance_std)\n",
        "                + (beta_age * age_c) + age_slope_effect\n",
        "                + (beta_age2 * age_c2)\n",
        "                + (beta_exp * exp_std)\n",
        "                + u[kicker_idx]\n",
        "            )\n",
        "\n",
        "            θ = pm.Deterministic(\"theta\", pm.invlogit(lin_pred))\n",
        "            pm.Bernoulli(\"obs\", p=θ, observed=success)\n",
        "        return model\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # 📈  Public API\n",
        "    # ---------------------------------------------------------------------\n",
        "    def fit(self, df, *, preprocessor=None):\n",
        "        debug_print(f\"Starting fit with preprocessor: {preprocessor is not None}\")\n",
        "        debug_print(f\"Input DataFrame rows: {len(df)}\")\n",
        "        debug_print(f\"DataFrame engineered flag: {df.attrs.get('engineered', False)}\")\n",
        "        debug_print(f\"DataFrame columns: {df.columns.tolist()}\")\n",
        "        \n",
        "        # ------------------------------------------------------------------\n",
        "        # 0️⃣  Exactly one preprocessing pass\n",
        "        if df.attrs.get(\"engineered\", False):\n",
        "            processed = df.copy()\n",
        "            debug_print(\"Using pre-engineered data\")\n",
        "        elif preprocessor is not None:\n",
        "            debug_print(\"Using provided preprocessor\")\n",
        "            debug_print(\"Preprocessor config:\", preprocessor.__dict__)\n",
        "            processed = preprocessor.preprocess_slice(df)\n",
        "        else:\n",
        "            # 🎯 AUTO-CREATE BAYESIAN-MINIMAL PREPROCESSOR \n",
        "            # Using module-level imports instead of local imports\n",
        "            debug_print(\"Creating minimal preprocessor\")\n",
        "            bayes_preprocessor = DataPreprocessor()\n",
        "            bayes_preprocessor.update_config(\n",
        "                min_distance=config.MIN_DISTANCE,\n",
        "                max_distance=config.MAX_DISTANCE, \n",
        "                min_kicker_attempts=config.MIN_KICKER_ATTEMPTS,\n",
        "                season_types=config.SEASON_TYPES,\n",
        "                include_performance_history=False,  # Not needed for Bayesian\n",
        "                include_statistical_features=False,  # Avoid complex features\n",
        "                include_player_status=True,  # Enable player status features\n",
        "                performance_window=12\n",
        "            )\n",
        "            bayes_preprocessor.update_feature_lists(**FEATURE_LISTS)\n",
        "            processed = bayes_preprocessor.preprocess_slice(df)\n",
        "            debug_print(\"Minimal preprocessing complete\")\n",
        "\n",
        "        debug_print(f\"Processed DataFrame rows: {len(processed)}\")\n",
        "        debug_print(f\"Processed columns: {processed.columns.tolist()}\")\n",
        "\n",
        "        # ------------------------------------------------------------------\n",
        "        # 1️⃣  Predictors\n",
        "        debug_print(\"Preparing predictors\")\n",
        "        dist_std = self._standardize(processed[\"attempt_yards\"].to_numpy(float), fit=True)\n",
        "        debug_print(f\"Distance stats - mean: {self._distance_mu:.2f}, std: {self._distance_sigma:.2f}\")\n",
        "        \n",
        "        # Age variables (centered & scaled)\n",
        "        age_c  = processed[\"age_c\"].to_numpy(float) if \"age_c\" in processed.columns else np.zeros(len(processed), dtype=float)\n",
        "        age_c2 = processed[\"age_c2\"].to_numpy(float) if \"age_c2\" in processed.columns else np.zeros(len(processed), dtype=float)\n",
        "        debug_print(f\"Age features present: age_c={age_c is not None}, age_c2={age_c2 is not None}\")\n",
        "\n",
        "        # Handle experience standardization\n",
        "        if \"exp_100\" in processed.columns:\n",
        "            exp_std = (\n",
        "                (processed[\"exp_100\"] - processed[\"exp_100\"].mean()) /\n",
        "                processed[\"exp_100\"].std()\n",
        "            ).to_numpy(float)\n",
        "            debug_print(\"Experience features included\")\n",
        "        else:\n",
        "            exp_std = np.zeros(len(processed), dtype=float)\n",
        "            debug_print(\"No experience features found\")\n",
        "            \n",
        "        success    = processed[\"success\"].to_numpy(int)\n",
        "        kicker_idx = self._encode_kicker(processed[\"kicker_id\"].to_numpy(int), fit=True)\n",
        "        n_kickers  = len(self._kicker_map)\n",
        "        debug_print(f\"Number of kickers: {n_kickers}\")\n",
        "        debug_print(f\"Success rate: {success.mean():.3f}\")\n",
        "\n",
        "        # ---- model & sampling -------------------------------------------\n",
        "        debug_print(\"Building model\")\n",
        "        self._model = self._build_model(\n",
        "            dist_std, age_c, age_c2, exp_std, success, kicker_idx, n_kickers\n",
        "        )\n",
        "        \n",
        "        # ── FIX: ensure pm.sample knows which model to use ────────────\n",
        "        debug_print(\"Starting MCMC sampling\")\n",
        "        with self._model:\n",
        "            self._trace = pm.sample(\n",
        "                draws=self.draws,\n",
        "                tune=self.tune,\n",
        "                chains=config.BAYESIAN_CHAINS,  # Use config value\n",
        "                target_accept=self.target_accept,\n",
        "                random_seed=self.random_seed,\n",
        "                return_inferencedata=True\n",
        "            )\n",
        "        debug_print(\"Sampling complete\")\n",
        "\n",
        "        from src.nfl_kicker_analysis.models.bayes_model_utils import save_pymc_inference\n",
        "        # ── ENSURE model directory exists & write full suite ────────────────\n",
        "        try:\n",
        "            config.MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
        "            ts = int(time.time())\n",
        "            suite_dir = config.MODEL_DIR / f\"bayesian_suite_{ts}\"\n",
        "            self.save_suite(suite_dir)\n",
        "            debug_print(f\"✅ Full Bayesian suite saved to {suite_dir!r}\")\n",
        "        except Exception as e:\n",
        "            debug_print(f\"⚠️ Failed to persist full suite: {e}\")\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        *,\n",
        "        return_ci: bool = False,\n",
        "        return_proba: bool = True,\n",
        "        preprocessor=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Predict FG make probabilities or credible intervals on new data.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        df : DataFrame to predict on\n",
        "        return_ci : bool, default False\n",
        "            If True, returns (mean, lower, upper) credible intervals\n",
        "        return_proba : bool, default True\n",
        "            If True, returns probabilities; if False, returns 0/1 predictions\n",
        "        preprocessor : optional preprocessor to use\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        If return_ci=False:\n",
        "            pd.Series of probabilities (if return_proba=True) or 0/1 (if False)\n",
        "        If return_ci=True:\n",
        "            Tuple of (mean, lower, upper) Series\n",
        "        \"\"\"\n",
        "        if self._trace is None:\n",
        "            raise RuntimeError(\"Model not yet fitted or loaded.\")\n",
        "\n",
        "        original_idx = df.index\n",
        "        # 1️⃣ Preprocess\n",
        "        proc = preprocessor.preprocess_slice(df) if preprocessor else df.copy()\n",
        "\n",
        "        # 2️⃣ Standardize distances using helper\n",
        "        dist_std = self._standardize(\n",
        "            proc[\"attempt_yards\"].to_numpy(float),\n",
        "            fit=False\n",
        "        )\n",
        "\n",
        "        # 3️⃣ Map kickers → indices\n",
        "        kicker_idx = self._encode_kicker(\n",
        "            proc[\"kicker_id\"].to_numpy(int),\n",
        "            fit=False,\n",
        "            unknown_action=\"average\"\n",
        "        )\n",
        "\n",
        "        # 4️⃣ Get age and experience features\n",
        "        age_c = proc[\"age_c\"].to_numpy(float) if \"age_c\" in proc.columns else np.zeros(len(proc))\n",
        "        age_c2 = proc[\"age_c2\"].to_numpy(float) if \"age_c2\" in proc.columns else np.zeros(len(proc))\n",
        "        \n",
        "        if \"exp_100\" in proc.columns:\n",
        "            exp_std = (\n",
        "                (proc[\"exp_100\"] - proc[\"exp_100\"].mean()) /\n",
        "                proc[\"exp_100\"].std()\n",
        "            ).to_numpy(float)\n",
        "        else:\n",
        "            exp_std = np.zeros(len(proc))\n",
        "\n",
        "        # 5️⃣ Collapse posterior to means using helper\n",
        "        a_mean = float(_get_posterior_mean(self._trace, \"alpha\"))\n",
        "        b_mean = float(_get_posterior_mean(self._trace, \"beta_dist\"))\n",
        "        u_mean_result = _get_posterior_mean(self._trace, \"u\")\n",
        "        \n",
        "        # Get age and experience coefficients\n",
        "        beta_age_mean = float(_get_posterior_mean(self._trace, \"beta_age\"))\n",
        "        beta_age2_mean = float(_get_posterior_mean(self._trace, \"beta_age2\"))\n",
        "        beta_exp_mean = float(_get_posterior_mean(self._trace, \"beta_exp\"))\n",
        "\n",
        "        # 6️⃣ Point predictions\n",
        "        if not return_ci:\n",
        "            # Handle u_mean indexing - ensure it's an array\n",
        "            if isinstance(u_mean_result, (int, float)):\n",
        "                u_effects = np.full(len(kicker_idx), u_mean_result)\n",
        "            else:\n",
        "                u_mean_arr = np.asarray(u_mean_result)\n",
        "                u_effects = u_mean_arr[kicker_idx]\n",
        "            \n",
        "            # Full linear predictor with all effects\n",
        "            logit = (\n",
        "                a_mean\n",
        "                + b_mean * dist_std\n",
        "                + beta_age_mean * age_c\n",
        "                + beta_age2_mean * age_c2\n",
        "                + beta_exp_mean * exp_std\n",
        "                + u_effects\n",
        "            )\n",
        "            \n",
        "            # Convert to probabilities\n",
        "            probs = 1 / (1 + np.exp(-logit))\n",
        "            \n",
        "            # Handle NaN values consistently\n",
        "            probs = np.where(np.isnan(logit), np.nan, probs)\n",
        "            \n",
        "            # Return probabilities or classifications\n",
        "            result = pd.Series(probs, index=proc.index).reindex(original_idx)\n",
        "            if not return_proba:\n",
        "                result = (result > 0.5).astype(float)\n",
        "            return result\n",
        "\n",
        "        # 7️⃣ Credible intervals via posterior predictive\n",
        "        ppc = pm.sample_posterior_predictive(\n",
        "            self._trace,\n",
        "            model=self._model,\n",
        "            var_names=[\"obs\"],\n",
        "            random_seed=self.random_seed,\n",
        "            progressbar=False,\n",
        "            return_inferencedata=True\n",
        "        )\n",
        "        draws = _get_posterior_predictive(ppc, \"obs\")\n",
        "        mean_preds = draws.mean(axis=(0, 1))\n",
        "        lower, upper = np.percentile(draws, [2.5, 97.5], axis=(0, 1))\n",
        "\n",
        "        s_mean = pd.Series(mean_preds, index=proc.index).reindex(original_idx)\n",
        "        s_lower = pd.Series(lower, index=proc.index).reindex(original_idx)\n",
        "        s_upper = pd.Series(upper, index=proc.index).reindex(original_idx)\n",
        "        return s_mean, s_lower, s_upper\n",
        "\n",
        "    def evaluate(\n",
        "        self, \n",
        "        df: pd.DataFrame, \n",
        "        *, \n",
        "        preprocessor=None\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"Compute AUC, Brier score & log‑loss on provided data.\n",
        "        \n",
        "        Args:\n",
        "            df: Data to evaluate on\n",
        "            preprocessor: Optional DataPreprocessor instance. If provided, will\n",
        "                         use it to preprocess the data before evaluation.\n",
        "        \"\"\"\n",
        "        # Apply preprocessing if provided  \n",
        "        if preprocessor is not None:\n",
        "            df = preprocessor.preprocess_slice(df)\n",
        "                \n",
        "        y_true = df[\"success\"].to_numpy(dtype=int)\n",
        "        y_pred_result = self.predict(df)  # predict() will handle its own preprocessing if needed\n",
        "        \n",
        "        # Handle both single prediction and CI tuple returns\n",
        "        if isinstance(y_pred_result, tuple):\n",
        "            y_pred = y_pred_result[0]  # Just use mean predictions for evaluation\n",
        "        else:\n",
        "            y_pred = y_pred_result\n",
        "            \n",
        "        return self.evaluator.calculate_classification_metrics(y_true, y_pred)\n",
        "\n",
        "    def diagnostics(self, *, return_scalars: bool = False) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Compute and return MCMC diagnostics.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        return_scalars : bool, default False\n",
        "            If True, also include convenience keys\n",
        "            ``rhat_max`` and ``ess_min`` for quick threshold checks.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        dict\n",
        "            Keys: rhat, ess (xarray.Dataset), rhat_vals, ess_vals (np.ndarray),\n",
        "            summary_ok (bool), and optionally rhat_max, ess_min (float).\n",
        "        \"\"\"\n",
        "        if self._trace is None:\n",
        "            raise RuntimeError(\"Model not yet fitted.\")\n",
        "\n",
        "        # ArviZ calls (collapse chain/draw)\n",
        "        rhats = cast(Dataset, az.rhat(self._trace))\n",
        "        ess   = cast(Dataset, az.ess(self._trace))\n",
        "\n",
        "        # Flatten → numpy for easy thresholding\n",
        "        rhat_vals = rhats.to_array().values.ravel()\n",
        "        ess_vals  = ess.to_array().values.ravel()\n",
        "\n",
        "        summary_ok = (rhat_vals <= 1.01).all() and (ess_vals >= 100).all()\n",
        "        if not summary_ok:\n",
        "            print(\"⚠️  Sampling diagnostics outside recommended thresholds.\")\n",
        "\n",
        "        out = {\n",
        "            \"rhat\": rhats,\n",
        "            \"ess\": ess,\n",
        "            \"rhat_vals\": rhat_vals,\n",
        "            \"ess_vals\": ess_vals,\n",
        "            \"summary_ok\": summary_ok,\n",
        "        }\n",
        "        if return_scalars:\n",
        "            out[\"rhat_max\"] = float(rhat_vals.max())\n",
        "            out[\"ess_min\"] = float(ess_vals.min())\n",
        "        return out\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # 🌟  NEW 1: kicker-level credible interval\n",
        "    # -----------------------------------------------------------------\n",
        "    def kicker_interval(\n",
        "        self,\n",
        "        kicker_id: int,\n",
        "        distance: float | None = None,\n",
        "        ci: float = 0.95,\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Return mean, lower, upper success probability for a *single* kicker.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "        kicker_id : raw ID as in dataframe\n",
        "        distance  : yards; if None, uses the empirical mean distance of\n",
        "                    training data, transformed with stored μ/σ.\n",
        "        ci        : central credible-interval mass (default 0.95)\n",
        "        \"\"\"\n",
        "        if self._trace is None:\n",
        "            raise RuntimeError(\"Model must be fitted first\")\n",
        "\n",
        "        # 1 → index or league-mean column\n",
        "        k_idx = self._kicker_map.get(kicker_id, -1)\n",
        "        pad_col = len(self._kicker_map)   # after pad in predict()\n",
        "\n",
        "        # 2 → choose distance\n",
        "        if distance is None:\n",
        "            distance_std = 0.0            # z-score of mean is 0\n",
        "        else:\n",
        "            distance_std = (distance - self._distance_mu) / self._distance_sigma\n",
        "\n",
        "        posterior = getattr(self._trace, \"posterior\")\n",
        "        a = posterior[\"alpha\"].values.flatten()\n",
        "        \n",
        "        # Robust lookup for the distance slope parameter (handles naming changes)\n",
        "        slope_name = \"beta_dist\" if \"beta_dist\" in posterior else \"beta\"\n",
        "        b = posterior[slope_name].values.flatten()\n",
        "        \n",
        "        u = posterior[\"u\"].values.reshape(a.size, -1)\n",
        "\n",
        "        # pad league-mean\n",
        "        u = np.pad(u, ((0, 0), (0, 1)), constant_values=0.0)\n",
        "        idx = pad_col if k_idx == -1 else k_idx\n",
        "\n",
        "        logit_p = a + b * distance_std + u[:, idx]\n",
        "        p = 1 / (1 + np.exp(-logit_p))\n",
        "\n",
        "        lower, upper = np.quantile(p, [(1-ci)/2, 1-(1-ci)/2])\n",
        "        return {\"mean\": p.mean(), \"lower\": lower, \"upper\": upper,\n",
        "                \"n_draws\": p.size, \"distance_std\": distance_std}\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # 🌟  NEW 2: posterior-predictive plot across 5-yd bins\n",
        "    # -----------------------------------------------------------------\n",
        "    def plot_distance_ppc(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        *,\n",
        "        bin_width: int = 5,\n",
        "        preprocessor = None,\n",
        "        ax: Optional[Axes] = None\n",
        "    ) -> Axes:\n",
        "        \"\"\"\n",
        "        Bin attempts by distance and overlay actual vs posterior mean make-rate.\n",
        "        \"\"\"\n",
        "        if preprocessor is not None:\n",
        "            df = preprocessor.preprocess_slice(df)\n",
        "\n",
        "        # 1 Actual success by bin\n",
        "        df = df.copy()\n",
        "        df[\"bin\"] = (df[\"attempt_yards\"] // bin_width) * bin_width\n",
        "        actual = df.groupby(\"bin\")[\"success\"].mean()\n",
        "\n",
        "        # 2 Posterior mean per attempt → group\n",
        "        preds = self.predict(df)\n",
        "        df[\"pred\"] = preds\n",
        "        posterior = df.groupby(\"bin\")[\"pred\"].mean()\n",
        "\n",
        "        # 3 Plot\n",
        "        if ax is None:\n",
        "            _, ax = plt.subplots(figsize=(8, 4))\n",
        "        \n",
        "        _plot_comparison(\n",
        "            ax,\n",
        "            x_vals=np.asarray(actual.index),\n",
        "            actual=np.asarray(actual),\n",
        "            predicted=np.asarray(posterior),\n",
        "            xlabel=\"Distance bin (yards)\",\n",
        "            title=f\"Posterior-Predictive Check ({bin_width}-yd bins)\"\n",
        "        )\n",
        "        return ax\n",
        "\n",
        "    # -----------------------------------------------------------------\n",
        "    # 🌟  NEW 3: age-binned posterior-predictive check\n",
        "    # -----------------------------------------------------------------\n",
        "    def plot_age_ppc(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        *,\n",
        "        bin_width: float = 2.0,\n",
        "        preprocessor = None,\n",
        "        ax: Optional[Axes] = None\n",
        "    ) -> Axes:\n",
        "        \"\"\"\n",
        "        Bin attempts by age and overlay actual vs posterior mean make-rate.\n",
        "        \"\"\"\n",
        "        if preprocessor is not None:\n",
        "            df = preprocessor.preprocess_slice(df)\n",
        "\n",
        "        # Use raw age for binning (more interpretable)\n",
        "        age_col = \"age_at_attempt\" if \"age_at_attempt\" in df.columns else \"age_c\"\n",
        "        df = df.copy()\n",
        "        \n",
        "        if age_col == \"age_c\":\n",
        "            # Convert back to raw age for binning\n",
        "            df[\"age_bin\"] = ((df[\"age_c\"] * 10 + 30) // bin_width) * bin_width\n",
        "        else:\n",
        "            df[\"age_bin\"] = (df[age_col] // bin_width) * bin_width\n",
        "            \n",
        "        # Actual success by age bin\n",
        "        actual = df.groupby(\"age_bin\")[\"success\"].mean()\n",
        "\n",
        "        # Posterior mean per attempt → group by age\n",
        "        preds = self.predict(df)\n",
        "        df[\"pred\"] = preds\n",
        "        posterior = df.groupby(\"age_bin\")[\"pred\"].mean()\n",
        "\n",
        "        # Plot\n",
        "        if ax is None:\n",
        "            _, ax = plt.subplots(figsize=(8, 4))\n",
        "            \n",
        "        _plot_comparison(\n",
        "            ax,\n",
        "            x_vals=np.asarray(actual.index),\n",
        "            actual=np.asarray(actual),\n",
        "            predicted=np.asarray(posterior),\n",
        "            xlabel=\"Age bin (years)\",\n",
        "            title=f\"Age-Based Posterior-Predictive Check ({bin_width:.1f}-yr bins)\"\n",
        "        )\n",
        "        return ax\n",
        "\n",
        "    # ───────────────────────────────────────────────────────────\n",
        "    # Helper: draw-level EPA simulation  (fully replaced)\n",
        "    # ───────────────────────────────────────────────────────────\n",
        "    def _epa_fg_plus_draws(\n",
        "        self,\n",
        "        league_df: pd.DataFrame,\n",
        "        *,\n",
        "        kicker_ids: ArrayLike,\n",
        "        n_samples: int,                 # renamed for clarity\n",
        "        rng: np.random.Generator,\n",
        "        distance_strategy: str = \"kicker\",\n",
        "        τ: float = 20.0,\n",
        "        **kwargs,\n",
        "    ) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Bootstrap EPA-FG⁺ draws for a set of kickers.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        league_df : DataFrame with at least ['player_id', 'attempt_yards', 'success']\n",
        "        kicker_ids : array of kicker IDs to compute for\n",
        "        n_samples : number of bootstrap samples per kicker\n",
        "        rng : numpy random number generator\n",
        "        distance_strategy : how to sample distances ('kicker' or 'league')\n",
        "        τ : shrinkage parameter for empirical Bayes\n",
        "        **kwargs : future-proofing for additional options\n",
        "        \n",
        "        Returns\n",
        "        -------\n",
        "        ndarray of shape (n_samples, len(kicker_ids))\n",
        "            Each column is the EPA-FG⁺ draws for one kicker\n",
        "        \"\"\"\n",
        "        n_draws = n_samples                # ← backward compatibility shim\n",
        "        kicker_ids_arr = np.asarray(kicker_ids)\n",
        "        \n",
        "        # Validate inputs\n",
        "        if not isinstance(league_df, pd.DataFrame):\n",
        "            raise TypeError(\"league_df must be a pandas DataFrame\")\n",
        "        if not isinstance(n_draws, int) or n_draws <= 0:\n",
        "            raise ValueError(\"n_draws must be a positive integer\")\n",
        "            \n",
        "        # Get baseline probabilities if not already computed\n",
        "        if not self.baseline_probs:\n",
        "            from src.nfl_kicker_analysis.utils.metrics import EPACalculator\n",
        "            self.baseline_probs = EPACalculator().calculate_baseline_probs(league_df)\n",
        "            \n",
        "        # Initialize output array\n",
        "        epa_draws = np.zeros((n_draws, len(kicker_ids_arr)))\n",
        "        \n",
        "        # For each kicker...\n",
        "        for k, kid in enumerate(kicker_ids_arr):\n",
        "            # Get this kicker's attempts\n",
        "            mask = league_df['player_id'] == kid\n",
        "            k_data = league_df[mask]\n",
        "            \n",
        "            if len(k_data) == 0:\n",
        "                continue  # Skip if no data (shouldn't happen given filtering)\n",
        "                \n",
        "            # Bootstrap distances based on strategy\n",
        "            if distance_strategy == \"kicker\":\n",
        "                # Sample from this kicker's empirical distribution\n",
        "                distances = self._bootstrap_distances(\n",
        "                    k_data['attempt_yards'],\n",
        "                    n_draws,\n",
        "                    rng=rng\n",
        "                )\n",
        "            else:\n",
        "                # Sample from league-wide distribution\n",
        "                distances = self._bootstrap_distances(\n",
        "                    league_df['attempt_yards'],\n",
        "                    n_draws,\n",
        "                    rng=rng\n",
        "                )\n",
        "                \n",
        "            # Get baseline probabilities for these distances\n",
        "            baseline_probs = np.array([\n",
        "                self.baseline_probs.get(int(d), 0.5) for d in distances\n",
        "            ])\n",
        "            \n",
        "            # Compute actual success rate (with Bayesian shrinkage)\n",
        "            n_attempts = len(k_data)\n",
        "            raw_rate = k_data['success'].mean()\n",
        "            \n",
        "            # Empirical Bayes shrinkage toward league average\n",
        "            shrinkage = n_attempts / (n_attempts + τ)\n",
        "            success_rate = (shrinkage * raw_rate + \n",
        "                          (1 - shrinkage) * np.mean(list(self.baseline_probs.values())))\n",
        "            \n",
        "            # EPA = (actual - expected) × points\n",
        "            epa_draws[:, k] = 3 * (success_rate - baseline_probs)\n",
        "            \n",
        "        return epa_draws\n",
        "\n",
        "    # ───────────────────────────────────────────────────────────────\n",
        "    # PUBLIC – EPA-FG⁺ leaderboard with robust column checks\n",
        "    # ───────────────────────────────────────────────────────────────\n",
        "    def epa_fg_plus(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        *,\n",
        "        min_attempts: int = 20,\n",
        "        n_samples: int | None = None,\n",
        "        return_ci: bool = False,\n",
        "    ) -> pd.DataFrame:\n",
        "        \"\"\"\n",
        "        Build an EPA-FG⁺ leaderboard, optionally with bootstrap CIs and certainty levels.\n",
        "\n",
        "        Returns a DataFrame indexed by player_id, with columns:\n",
        "        - rank, player_name, total_attempts, epa_fg_plus_mean\n",
        "        - (if return_ci=True) hdi_lower, hdi_upper, certainty_level\n",
        "        - plus raw_success_rate, avg_distance, player_status, last_age, seasons_exp\n",
        "        \"\"\"\n",
        "        debug_print(f\"epa_fg_plus called with min_attempts={min_attempts}, \"\n",
        "                    f\"n_samples={n_samples}, return_ci={return_ci}\")\n",
        "\n",
        "        # 1️⃣ Defaults & RNG\n",
        "        if n_samples is None:\n",
        "            n_samples = config.BAYESIAN_MCMC_SAMPLES\n",
        "        rng = np.random.default_rng(self.random_seed or 42)\n",
        "\n",
        "        # 2️⃣ Ensure EPA columns present\n",
        "        from src.nfl_kicker_analysis.data.feature_engineering import ensure_epa_columns\n",
        "        work = ensure_epa_columns(df.copy())\n",
        "\n",
        "        # 3️⃣ Baseline probabilities cache\n",
        "        if not self.baseline_probs:\n",
        "            from src.nfl_kicker_analysis.utils.metrics import EPACalculator\n",
        "            self.baseline_probs = EPACalculator().calculate_baseline_probs(work)\n",
        "\n",
        "        # 4️⃣ Bootstrap draws\n",
        "        kicker_ids = work[\"player_id\"].unique()\n",
        "        draws = self._epa_fg_plus_draws(\n",
        "            work, kicker_ids=kicker_ids, n_samples=n_samples, rng=rng\n",
        "        )\n",
        "\n",
        "        # 5️⃣ Summarize draws\n",
        "        means  = draws.mean(axis=0)\n",
        "        lowers, uppers = np.percentile(draws, [2.5, 97.5], axis=0)\n",
        "\n",
        "        # 6️⃣ Build base summary\n",
        "        summary = pd.DataFrame({\n",
        "            \"player_id\":   kicker_ids,\n",
        "            \"epa_fg_plus\": means,\n",
        "            \"hdi_lower\":   lowers,\n",
        "            \"hdi_upper\":   uppers,\n",
        "        })\n",
        "\n",
        "        # 7️⃣ Enrich with metadata\n",
        "        meta = (\n",
        "            work.groupby(\"player_id\")\n",
        "                .agg(\n",
        "                    player_name       = (\"player_name\", \"first\"),\n",
        "                    attempts          = (\"success\", \"size\"),\n",
        "                    raw_success_rate  = (\"success\", \"mean\"),\n",
        "                    avg_distance      = (\"attempt_yards\", \"mean\"),\n",
        "                    player_status     = (\"player_status\", \"first\"),\n",
        "                    last_age          = (\"age_at_attempt\", \"max\"),\n",
        "                    seasons_exp       = (\"season\", \"nunique\"),\n",
        "                )\n",
        "                .reset_index()\n",
        "        )\n",
        "        summary = (\n",
        "            summary\n",
        "            .merge(meta, on=\"player_id\", how=\"left\")\n",
        "            .query(\"attempts >= @min_attempts\")\n",
        "            .sort_values(\"epa_fg_plus\", ascending=False)\n",
        "            .reset_index(drop=True)\n",
        "            .assign(rank=lambda d: d.index + 1)\n",
        "        )\n",
        "\n",
        "        # 8️⃣ Rename for downstream code\n",
        "        summary = summary.rename(columns={\n",
        "            \"epa_fg_plus\": \"epa_fg_plus_mean\",\n",
        "            \"attempts\":    \"total_attempts\",\n",
        "        })\n",
        "\n",
        "        # 9️⃣ ✨ NEW: set raw player_id as the DataFrame index\n",
        "        summary = summary.set_index(\"player_id\", drop=True)\n",
        "        debug_print(\"After set_index, summary.index:\", summary.index[:5])\n",
        "\n",
        "        # 🔟 Compute certainty if requested\n",
        "        if return_ci:\n",
        "            ci_widths = summary[\"hdi_upper\"] - summary[\"hdi_lower\"]\n",
        "            q33, q66 = np.percentile(ci_widths, [33.3, 66.6])\n",
        "            def label(w):\n",
        "                if w <= q33:    return \"high\"\n",
        "                if w <= q66:    return \"medium\"\n",
        "                return \"low\"\n",
        "            summary[\"certainty_level\"] = ci_widths.map(label)\n",
        "\n",
        "        # 1️⃣1️⃣ Final column order\n",
        "        cols = [\n",
        "            \"rank\", \"player_name\", \"total_attempts\", \"epa_fg_plus_mean\",\n",
        "            \"raw_success_rate\", \"avg_distance\", \"player_status\",\n",
        "            \"last_age\", \"seasons_exp\",\n",
        "        ]\n",
        "        if return_ci:\n",
        "            cols += [\"hdi_lower\", \"hdi_upper\", \"certainty_level\"]\n",
        "\n",
        "        debug_print(\"Final EPA table columns:\", cols)\n",
        "        return summary[cols]\n",
        "\n",
        "    # ---------------------------------------------------------------------\n",
        "    # 🔍  Helper methods for kicker ID/name conversion\n",
        "    # ---------------------------------------------------------------------\n",
        "    def get_kicker_id_by_name(self, df: pd.DataFrame, player_name: str) -> int | None:\n",
        "        \"\"\"\n",
        "        Get kicker_id for a given player_name from the dataset.\n",
        "        \n",
        "        Args:\n",
        "            df: DataFrame containing kicker_id and player_name columns\n",
        "            player_name: Name of the kicker to look up\n",
        "            \n",
        "        Returns:\n",
        "            kicker_id if found, None otherwise\n",
        "        \"\"\"\n",
        "        debug_print(f\"Looking up kicker: {player_name}\")\n",
        "        debug_print(f\"DataFrame columns: {df.columns.tolist()}\")\n",
        "        debug_print(f\"Number of unique kickers: {df['player_name'].nunique()}\")\n",
        "        \n",
        "        matches = df[df[\"player_name\"] == player_name][\"kicker_id\"]\n",
        "        debug_print(f\"Found matches: {matches.tolist()}\")\n",
        "        \n",
        "        if len(matches) == 0:\n",
        "            debug_print(\"No matches found\")\n",
        "            return None\n",
        "            \n",
        "        unique_matches = matches.unique()\n",
        "        debug_print(f\"Unique kicker IDs: {unique_matches.tolist()}\")\n",
        "        \n",
        "        # Check if this ID exists in our mapping\n",
        "        raw_id = int(unique_matches[0])\n",
        "        mapped_idx = self._kicker_map.get(raw_id)\n",
        "        debug_print(f\"Raw ID: {raw_id}, Mapped index: {mapped_idx}\")\n",
        "        debug_print(f\"Kicker map size: {len(self._kicker_map)}\")\n",
        "        debug_print(f\"Available mappings: {self._kicker_map}\")\n",
        "        \n",
        "        return mapped_idx\n",
        "    \n",
        "    def get_kicker_name_by_id(self, df: pd.DataFrame, kicker_id: int) -> str | None:\n",
        "        \"\"\"\n",
        "        Get player_name for a given kicker_id from the dataset.\n",
        "        \n",
        "        Args:\n",
        "            df: DataFrame containing kicker_id and player_name columns\n",
        "            kicker_id: ID of the kicker to look up\n",
        "            \n",
        "        Returns:\n",
        "            player_name if found, None otherwise\n",
        "        \"\"\"\n",
        "        matches = df[df[\"kicker_id\"] == kicker_id][\"player_name\"].unique()\n",
        "        return str(matches[0]) if len(matches) > 0 else None\n",
        "    \n",
        "    def kicker_interval_by_name(\n",
        "        self,\n",
        "        df: pd.DataFrame,\n",
        "        player_name: str,\n",
        "        distance: float | None = None,\n",
        "        ci: float = 0.95,\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Return mean, lower, upper success probability for a kicker by name.\n",
        "        \n",
        "        Args:\n",
        "            df: DataFrame containing kicker mappings\n",
        "            player_name: Name of the kicker\n",
        "            distance: yards; if None, uses empirical mean\n",
        "            ci: central credible-interval mass (default 0.95)\n",
        "        \"\"\"\n",
        "        kicker_id = self.get_kicker_id_by_name(df, player_name)\n",
        "        if kicker_id is None:\n",
        "            raise ValueError(f\"Kicker '{player_name}' not found in dataset\")\n",
        "        return self.kicker_interval(kicker_id, distance, ci)\n",
        "\n",
        "    def save_suite(self, dirpath: Path | str) -> None:\n",
        "        \"\"\"\n",
        "        Persist the trained suite to *dirpath* in a **fully deterministic**\n",
        "        fashion so that a reload followed by ``predict`` returns *bit-for-bit*\n",
        "        identical probabilities.\n",
        "\n",
        "        What we store\n",
        "        -------------\n",
        "        1. The full ArviZ ``InferenceData`` object → ``trace.nc`` (NetCDF).\n",
        "        2. A JSON side-car ``meta.json`` containing:\n",
        "           • distance mean / std  (rounded to 12 dp to avoid JSON drift)\n",
        "           • kicker_map – serialised as **string keys**, **stable order** (sorted\n",
        "             by the *value* which is the contiguous index used by the sampler).\n",
        "        \"\"\"\n",
        "        dirpath = Path(dirpath)\n",
        "        dirpath.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # 1️⃣ Save the InferenceData exactly as returned by PyMC/ArviZ.\n",
        "        trace_file = dirpath / \"trace.nc\"\n",
        "        if self._trace is not None:\n",
        "            self._trace.to_netcdf(str(trace_file))  # Convert Path to str for to_netcdf\n",
        "\n",
        "        # 2️⃣ Build a JSON-serialisable metadata dict.\n",
        "        # Sort by value (index) for stable ordering\n",
        "        ordered_map = {\n",
        "            str(k): int(v)           # keys → str for JSON, values stay int\n",
        "            for k, v in sorted(self._kicker_map.items(), key=lambda kv: kv[1])\n",
        "        }\n",
        "\n",
        "        # Round floats to avoid JSON drift\n",
        "        meta = {\n",
        "            \"version\": \"1.0.0\",  # Add version for future compatibility\n",
        "            \"distance_mu\": round(float(self._distance_mu), 12),\n",
        "            \"distance_sigma\": round(float(self._distance_sigma), 12),\n",
        "            \"kicker_map\": ordered_map,\n",
        "        }\n",
        "\n",
        "        # Debug prints\n",
        "        debug_print(\"Saving suite metadata:\")\n",
        "        debug_print(f\"  distance_mu: {meta['distance_mu']}\")\n",
        "        debug_print(f\"  distance_sigma: {meta['distance_sigma']}\")\n",
        "        debug_print(f\"  kicker_map (first 3): {dict(list(ordered_map.items())[:3])}\")\n",
        "\n",
        "        with open(dirpath / \"meta.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(meta, f, separators=(\",\", \":\"))   # compact, stable\n",
        "\n",
        "        debug_print(f\"✅ Suite saved deterministically to {dirpath!r}\")\n",
        "\n",
        "    def save_metrics(self, dirpath: Path | str, metrics: dict) -> None:\n",
        "        \"\"\"\n",
        "        Persist evaluation metrics alongside trace.nc and meta.json.\n",
        "        \n",
        "        Parameters\n",
        "        ----------\n",
        "        dirpath : Path or str\n",
        "            Directory where the suite is saved\n",
        "        metrics : dict\n",
        "            Dictionary of evaluation metrics to persist\n",
        "            \n",
        "        Notes\n",
        "        -----\n",
        "        This ensures metrics computed during training are preserved exactly\n",
        "        and can be loaded by the Streamlit app without recomputation.\n",
        "        \"\"\"\n",
        "        dirpath = Path(dirpath)\n",
        "        metrics_file = dirpath / \"metrics.json\"\n",
        "        # Write with indentation for readability\n",
        "        with open(metrics_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(metrics, f, indent=2)\n",
        "        debug_print(f\"✅ Metrics saved to {metrics_file!r}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_suite(cls, dirpath: Path | str) -> \"BayesianModelSuite\":\n",
        "        \"\"\"\n",
        "        Reload a suite that was persisted by :py:meth:`save_suite`.\n",
        "        The returned instance is guaranteed to yield **identical** predictions\n",
        "        to the original suite (within IEEE-754 floating point rules).\n",
        "        \"\"\"\n",
        "        dirpath = Path(dirpath)\n",
        "        if not dirpath.exists():\n",
        "            raise FileNotFoundError(f\"No saved suite at {dirpath}\")\n",
        "\n",
        "        # 1️⃣ Create a fresh instance so that __init__ validations run.\n",
        "        suite = cls()\n",
        "\n",
        "        # 2️⃣ Load metadata\n",
        "        with open(dirpath / \"meta.json\", \"r\", encoding=\"utf-8\") as f:\n",
        "            meta = json.load(f)\n",
        "            \n",
        "        # Check version compatibility\n",
        "        version = meta.get(\"version\", \"0.0.0\")  # Default for older saves\n",
        "        if version != \"1.0.0\":\n",
        "            debug_print(f\"⚠️ Warning: Loading model version {version}, current version is 1.0.0\")\n",
        "\n",
        "        suite._distance_mu = float(meta[\"distance_mu\"])\n",
        "        suite._distance_sigma = float(meta[\"distance_sigma\"])\n",
        "        \n",
        "        # Reconstruct the kicker_map **exactly** as before (value-sorted)\n",
        "        suite._kicker_map = {\n",
        "            int(k): int(v) for k, v in sorted(meta[\"kicker_map\"].items(), key=lambda kv: int(kv[1]))\n",
        "        }\n",
        "\n",
        "        # Debug prints\n",
        "        debug_print(\"Loading suite metadata:\")\n",
        "        debug_print(f\"  distance_mu: {suite._distance_mu}\")\n",
        "        debug_print(f\"  distance_sigma: {suite._distance_sigma}\")\n",
        "        debug_print(f\"  kicker_map (first 3): {dict(list(suite._kicker_map.items())[:3])}\")\n",
        "\n",
        "        # 3️⃣ Load the trace\n",
        "        suite._trace = az.from_netcdf(dirpath / \"trace.nc\")\n",
        "\n",
        "        debug_print(f\"✅ Suite loaded from {dirpath!r} — distance μ/σ = {suite._distance_mu:.4f}/{suite._distance_sigma:.4f}; {len(suite._kicker_map)} kickers.\")\n",
        "        return suite\n",
        "\n",
        "    def verify_persistence(self, test_data: pd.DataFrame, *, preprocessor=None) -> bool:\n",
        "        \"\"\"\n",
        "        Verify that model persistence is working correctly by saving and loading\n",
        "        the model, then comparing predictions within floating-point tolerances.\n",
        "        \"\"\"\n",
        "        import tempfile\n",
        "        from pathlib import Path\n",
        "\n",
        "        debug_print(\"Verifying model persistence...\")\n",
        "\n",
        "        # Original predictions\n",
        "        preds_orig = self.predict(test_data, preprocessor=preprocessor)\n",
        "        orig_vals = preds_orig.values\n",
        "\n",
        "        # Save & reload\n",
        "        with tempfile.TemporaryDirectory() as tmpdir:\n",
        "            tmp_path = Path(tmpdir)\n",
        "            self.save_suite(tmp_path)\n",
        "            loaded = self.load_suite(tmp_path)\n",
        "            preds_loaded = loaded.predict(test_data, preprocessor=preprocessor)\n",
        "            loaded_vals = preds_loaded.values\n",
        "\n",
        "        # Tolerance-based comparison\n",
        "        cmp = np.allclose(\n",
        "            orig_vals,\n",
        "            loaded_vals,\n",
        "            rtol=1e-12,\n",
        "            atol=1e-12,\n",
        "            equal_nan=True\n",
        "        )\n",
        "\n",
        "        if not cmp:\n",
        "            # Identify and debug first few mismatches\n",
        "            diff_mask = ~np.isclose(\n",
        "                orig_vals,\n",
        "                loaded_vals,\n",
        "                rtol=1e-12,\n",
        "                atol=1e-12,\n",
        "                equal_nan=True\n",
        "            )\n",
        "            idx = np.where(diff_mask)[0]\n",
        "            debug_print(f\"⚠️  Persistence tolerance check failed at {len(idx)} positions\")\n",
        "            if idx.size > 0:\n",
        "                i = idx[0]\n",
        "                debug_print(f\"First mismatch at idx {i}: orig={orig_vals[i]}, loaded={loaded_vals[i]}\")\n",
        "            return False\n",
        "\n",
        "        debug_print(\"✅ Predictions match within tolerance\")\n",
        "        return True\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------------\n",
        "# CLI smoke test entrypoint with leaderboard integration\n",
        "# -------------------------------------------------------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    from src.nfl_kicker_analysis.data.loader import DataLoader\n",
        "    from src.nfl_kicker_analysis.data.feature_engineering import FeatureEngineer\n",
        "    from src.nfl_kicker_analysis.data.preprocessor import DataPreprocessor\n",
        "    from src.nfl_kicker_analysis.utils.metrics import train_test_split_by_season\n",
        "    from src.nfl_kicker_analysis import config\n",
        "    from src.nfl_kicker_analysis.utils.model_utils import _save_leaderboard\n",
        "    import numpy as np\n",
        "    import matplotlib.pyplot as plt\n",
        "    import time\n",
        "\n",
        "    # 1️⃣ Load & engineer features\n",
        "    print(\"\\n1️⃣ Loading and engineering data...\")\n",
        "    df_raw  = DataLoader().load_complete_dataset()\n",
        "    df_feat = FeatureEngineer().create_all_features(df_raw)\n",
        "\n",
        "    # 1a️⃣ Persist engineered features for Streamlit\n",
        "    feat_path = config.MODEL_DATA_FILE\n",
        "    config.OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"1a️⃣ Persisting engineered features to {feat_path}\")\n",
        "    df_feat.to_csv(feat_path, index=False)\n",
        "    print(\"🔍 Contents of OUTPUT_DIR:\", list(config.OUTPUT_DIR.iterdir()))\n",
        "\n",
        "    # 2️⃣ Season split\n",
        "    print(\"\\n2️⃣ Splitting data by season...\")\n",
        "    train_raw, test_raw = train_test_split_by_season(df_feat)\n",
        "    print(f\"  Train size: {len(train_raw):,}\")\n",
        "    print(f\"  Test size: {len(test_raw):,}\")\n",
        "\n",
        "    # 3️⃣ Configure preprocessor\n",
        "    print(\"\\n3️⃣ Configuring preprocessor...\")\n",
        "    pre = DataPreprocessor()\n",
        "    pre.update_config(\n",
        "        min_distance=config.MIN_DISTANCE,\n",
        "        max_distance=config.MAX_DISTANCE,\n",
        "        min_kicker_attempts=config.MIN_KICKER_ATTEMPTS,\n",
        "        season_types=config.SEASON_TYPES,\n",
        "        include_performance_history=False,\n",
        "        include_statistical_features=False,\n",
        "        include_player_status=True,\n",
        "        performance_window=12,\n",
        "    )\n",
        "    pre.update_feature_lists(**config.FEATURE_LISTS)\n",
        "\n",
        "    # 4️⃣ Fit Bayesian model\n",
        "    print(\"\\n4️⃣ Fitting Bayesian model...\")\n",
        "    suite = BayesianModelSuite(\n",
        "        draws=config.BAYESIAN_MCMC_SAMPLES,\n",
        "        tune=config.BAYESIAN_TUNE,\n",
        "        include_random_slope=False,\n",
        "        random_seed=42,\n",
        "    )\n",
        "    suite.fit(train_raw, preprocessor=pre)\n",
        "\n",
        "    # ── Persist the full suite ──\n",
        "    suite_dir = config.MODEL_DIR / f\"bayesian_suite_{int(time.time())}\"\n",
        "    suite.save_suite(suite_dir)\n",
        "    print(f\"✅ Full suite saved at {suite_dir}\")\n",
        "\n",
        "    # 5. Evaluate metrics\n",
        "    metrics = suite.evaluate(test_raw, preprocessor=pre)\n",
        "    print(\"\\nModel Evaluation Metrics:\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "    # Save metrics alongside the suite\n",
        "    suite.save_metrics(suite_dir, metrics)\n",
        "    print(f\"✅ Metrics saved to {suite_dir / 'metrics.json'}\")\n",
        "\n",
        "    # Validation checks\n",
        "    print(\"\\n✅ Running validation checks...\")\n",
        "    try:\n",
        "        cid = suite.kicker_interval_by_name(df_feat, \"JUSTIN TUCKER\", distance=40)\n",
        "        assert cid[\"lower\"] <= cid[\"mean\"] <= cid[\"upper\"], f\"Credible interval ordering failed: {cid}\"\n",
        "        print(\"• Credible interval check passed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"• WARNING: Credible interval check failed: {e}\")\n",
        "\n",
        "    try:\n",
        "        ax = suite.plot_distance_ppc(test_raw, bin_width=5, preprocessor=pre)\n",
        "        df_ppc = pre.preprocess_slice(test_raw).copy()\n",
        "        df_ppc[\"bin\"] = (df_ppc[\"attempt_yards\"] // 5) * 5\n",
        "        actual = df_ppc.groupby(\"bin\")[\"success\"].mean().values\n",
        "        posterior = df_ppc.assign(pred=suite.predict(df_ppc)) \\\n",
        "                       .groupby(\"bin\")[\"pred\"].mean().values\n",
        "        corr = np.corrcoef(np.asarray(actual), np.asarray(posterior))[0, 1]\n",
        "        assert corr > 0.9, f\"PPC correlation too low: {corr:.3f}\"\n",
        "        print(f\"• PPC correlation check passed (r={corr:.3f}).\")\n",
        "    except Exception as e:\n",
        "        print(f\"• WARNING: PPC correlation check failed: {e}\")\n",
        "\n",
        "    # EPA-FG+ leaderboard\n",
        "    try:\n",
        "        epa_tbl = suite.epa_fg_plus(df_feat, n_samples=500, return_ci=True)\n",
        "        print(\"\\nTop 5 Active Kickers by EPA-FG+:\")\n",
        "        display_cols = [\n",
        "            \"player_name\", \"epa_fg_plus_mean\", \"raw_success_rate\",\n",
        "            \"total_attempts\", \"avg_distance\", \"player_status\",\n",
        "            \"last_age\", \"seasons_exp\", \"certainty_level\"\n",
        "        ]\n",
        "        active_kickers = epa_tbl[epa_tbl[\"player_status\"] != \"Retired/Injured\"]\n",
        "        print(active_kickers[display_cols].head().to_string())\n",
        "\n",
        "        print(\"\\nKicker Status Breakdown:\")\n",
        "        for status, count in epa_tbl[\"player_status\"].value_counts().items():\n",
        "            print(f\"  {status}: {count}\")\n",
        "\n",
        "        # Persist leaderboard\n",
        "        epa_no_ci = epa_tbl.reset_index()[[\"player_id\",\"player_name\",\"epa_fg_plus_mean\",\"rank\"]]\n",
        "        _save_leaderboard(epa_no_ci.set_index(\"player_id\"))\n",
        "        print(f\"✓ Wrote updated leaderboard to {config.LEADERBOARD_FILE}\")\n",
        "    except Exception as e:\n",
        "        print(f\"• WARNING: EPA leaderboard generation failed: {e}\")\n",
        "\n",
        "    # MCMC diagnostics\n",
        "    try:\n",
        "        diag = suite.diagnostics(return_scalars=True)\n",
        "        assert diag[\"summary_ok\"], f\"Diagnostics failed: R-hat={diag['rhat_max']:.3f}, ESS={diag['ess_min']:.0f}\"\n",
        "        print(\"• Diagnostics check passed.\")\n",
        "    except Exception as e:\n",
        "        print(f\"• WARNING: Diagnostics check failed: {e}\")\n",
        "\n",
        "    # Test visualization with Justin Tucker\n",
        "    try:\n",
        "        print(\"\\nTesting visualization with Justin Tucker...\")\n",
        "        \n",
        "        # Get Tucker's interval\n",
        "        tucker_interval = suite.kicker_interval_by_name(df_feat, \"JUSTIN TUCKER\")\n",
        "        print(f\"Tucker's P(make) at mean distance: {tucker_interval['mean']:.3f}\")\n",
        "        print(f\"95% CI: ({tucker_interval['lower']:.3f}, {tucker_interval['upper']:.3f})\")\n",
        "        \n",
        "        # Get posterior draws for visualization\n",
        "        k_idx = suite.get_kicker_id_by_name(df_feat, \"JUSTIN TUCKER\")\n",
        "        if k_idx is not None:\n",
        "            posterior = suite._trace.posterior\n",
        "            if posterior is None:\n",
        "                raise ValueError(\"No posterior samples available\")\n",
        "                \n",
        "            a_draws = np.asarray(posterior[\"alpha\"].values.flatten())\n",
        "            b_draws = np.asarray(posterior[\"beta_dist\"].values.flatten())\n",
        "            u_draws = np.asarray(posterior[\"u\"].values.reshape(-1, posterior[\"u\"].shape[-1]))\n",
        "            \n",
        "            # Get kicker's random effects\n",
        "            u_k = u_draws[:, k_idx]\n",
        "            \n",
        "            # Compute P(make) at mean distance\n",
        "            logit = a_draws + u_k  # distance=0 (mean-centered)\n",
        "            p_make = 1 / (1 + np.exp(-logit))\n",
        "            \n",
        "            print(f\"\\nVisualization test successful!\")\n",
        "            print(f\"P(make) range: [{float(p_make.min()):.3f}, {float(p_make.max()):.3f}]\")\n",
        "            print(f\"Mean: {float(p_make.mean()):.3f}\")\n",
        "            print(f\"95% CI: [{float(np.percentile(p_make, 2.5)):.3f}, {float(np.percentile(p_make, 97.5)):.3f}]\")\n",
        "        else:\n",
        "            print(\"❌ Failed to get kicker index for Tucker\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"• WARNING: Visualization tests failed: {e}\")\n",
        "\n",
        "    print(\"\\n🎉 Core functionality complete!\")\n",
        "\n",
        "def test_tucker_visualization(suite: BayesianModelSuite, df: pd.DataFrame) -> None:\n",
        "    \"\"\"\n",
        "    Debug test for Justin Tucker visualization.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Testing Tucker Visualization ===\")\n",
        "    print(\"1. DataFrame Info:\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    print(f\"Columns: {df.columns.tolist()}\")\n",
        "    print(f\"Number of unique kickers: {df['player_name'].nunique()}\")\n",
        "    print(f\"Sample of kicker names: {df['player_name'].unique()[:5].tolist()}\")\n",
        "    \n",
        "    print(\"\\n2. Kicker Map Info:\")\n",
        "    print(f\"Size of kicker map: {len(suite._kicker_map)}\")\n",
        "    print(f\"Sample mappings: {dict(list(suite._kicker_map.items())[:3])}\")\n",
        "    \n",
        "    print(\"\\n3. Looking up Tucker:\")\n",
        "    tucker_rows = df[df[\"player_name\"] == \"JUSTIN TUCKER\"]\n",
        "    print(f\"Found {len(tucker_rows)} rows for Tucker\")\n",
        "    if len(tucker_rows) > 0:\n",
        "        print(f\"Tucker's kicker_id values: {tucker_rows['kicker_id'].unique().tolist()}\")\n",
        "        print(f\"Tucker's player_status: {tucker_rows['player_status'].iloc[0]}\")\n",
        "        \n",
        "    k_idx = suite.get_kicker_id_by_name(df, \"JUSTIN TUCKER\")\n",
        "    print(f\"Mapped index for Tucker: {k_idx}\")\n",
        "    \n",
        "    print(\"\\n4. Posterior Info:\")\n",
        "    posterior = suite._trace.posterior\n",
        "    print(f\"Posterior groups: {list(posterior.groups)}\")\n",
        "    print(f\"U parameter shape: {posterior['u'].shape}\")\n",
        "    \n",
        "    if k_idx is not None:\n",
        "        print(f\"\\n5. Checking index bounds:\")\n",
        "        u = posterior[\"u\"].values.reshape(-1, posterior[\"u\"].shape[-1])\n",
        "        print(f\"Reshaped U matrix: {u.shape}\")\n",
        "        print(f\"Attempting to access column {k_idx}\")\n",
        "        if 0 <= k_idx < u.shape[1]:\n",
        "            print(\"✅ Index is within bounds\")\n",
        "            u_k = u[:, k_idx]\n",
        "            print(f\"Successfully extracted kicker effects: shape={u_k.shape}\")\n",
        "        else:\n",
        "            print(f\"❌ Index {k_idx} is out of bounds for shape {u.shape}\")\n",
        "    \n",
        "    print(\"\\n=== Test Complete ===\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# streamlit app"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting .streamlit/config.toml\n"
          ]
        }
      ],
      "source": [
        "%%writefile .streamlit/config.toml\n",
        "[server]\n",
        "port = 8501\n",
        "enableCORS = false\n",
        "enableXsrfProtection = false\n",
        "maxUploadSize = 200\n",
        "\n",
        "[browser]\n",
        "gatherUsageStats = false\n",
        "\n",
        "[theme]\n",
        "base=\"dark\"\n",
        "primaryColor=\"#fb4f14\"  # Broncos orange\n",
        "backgroundColor=\"#121212\"  # Dark background\n",
        "secondaryBackgroundColor=\"#1E1E1E\"  # Slightly lighter for cards\n",
        "textColor=\"#E0E0E0\"  # Light text\n",
        "\n",
        "[theme.sidebar]\n",
        "backgroundColor=\"#1E1E1E\"\n",
        "secondaryBackgroundColor=\"#2A2A2A\"\n",
        "textColor=\"#E0E0E0\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "# app.py\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import streamlit as st\n",
        "import json\n",
        "import base64\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.figure import Figure\n",
        "from typing import Optional, cast, List\n",
        "import os\n",
        "\n",
        "# Try to import optional dependencies\n",
        "try:\n",
        "    import arviz as az\n",
        "    ARVIZ_AVAILABLE = True\n",
        "except ImportError:\n",
        "    ARVIZ_AVAILABLE = False\n",
        "    az = None\n",
        "\n",
        "# Import configuration\n",
        "try:\n",
        "    from src.nfl_kicker_analysis.config import config\n",
        "    CONFIG_AVAILABLE = True\n",
        "except ImportError as e:\n",
        "    st.error(\"Configuration module not found. Please ensure the project is set up correctly.\")\n",
        "    st.stop()\n",
        "except Exception as e:\n",
        "    st.error(\"Error loading configuration. Please check your project setup.\")\n",
        "    st.stop()\n",
        "\n",
        "# Try to import model utilities\n",
        "try:\n",
        "    from src.nfl_kicker_analysis.utils.model_utils import (\n",
        "        list_registered_models,\n",
        "        list_saved_models,\n",
        "        load_model,\n",
        "        get_best_model_info,\n",
        "        get_best_metrics,\n",
        "    )\n",
        "    MODEL_UTILS_AVAILABLE = True\n",
        "except ImportError:\n",
        "    MODEL_UTILS_AVAILABLE = False\n",
        "\n",
        "# Try to import Bayesian models\n",
        "try:\n",
        "    from src.nfl_kicker_analysis.models.bayesian import BayesianModelSuite\n",
        "    BAYESIAN_AVAILABLE = True\n",
        "except ImportError:\n",
        "    BAYESIAN_AVAILABLE = False\n",
        "    BayesianModelSuite = None\n",
        "\n",
        "# Try to import data modules\n",
        "try:\n",
        "    from src.nfl_kicker_analysis.data.loader import DataLoader\n",
        "    DATA_LOADER_AVAILABLE = True\n",
        "except ImportError:\n",
        "    DATA_LOADER_AVAILABLE = False\n",
        "    DataLoader = None\n",
        "\n",
        "try:\n",
        "    from src.nfl_kicker_analysis.data.preprocessor import DataPreprocessor\n",
        "    DATA_PREPROCESSOR_AVAILABLE = True\n",
        "except ImportError:\n",
        "    DATA_PREPROCESSOR_AVAILABLE = False\n",
        "    DataPreprocessor = None\n",
        "\n",
        "# Try to import EDA modules\n",
        "try:\n",
        "    from src.nfl_kicker_analysis.eda import (\n",
        "        outcome_summary, \n",
        "        distance_analysis, \n",
        "        temporal_analysis, \n",
        "        kicker_performance_analysis, \n",
        "        feature_engineering\n",
        "    )\n",
        "    EDA_AVAILABLE = True\n",
        "except ImportError:\n",
        "    EDA_AVAILABLE = False\n",
        "\n",
        "@st.cache_data\n",
        "def compute_kicker_metrics(df: pd.DataFrame, player_name: str) -> dict:\n",
        "    \"\"\"Compute and cache kicker metrics.\"\"\"\n",
        "    kicker_data = df[df['player_name'] == player_name]\n",
        "    return {\n",
        "        'total_attempts': len(kicker_data),\n",
        "        'success_rate': kicker_data['success'].mean() * 100,\n",
        "        'avg_distance': kicker_data['attempt_yards'].mean(),\n",
        "    }\n",
        "\n",
        "@st.cache_data\n",
        "def compute_distance_binned_stats(df: pd.DataFrame, player_name: str, bin_size: int = 5) -> tuple:\n",
        "    \"\"\"Compute and cache binned statistics for distance analysis.\"\"\"\n",
        "    kicker_data = df[df['player_name'] == player_name].copy()\n",
        "    kicker_data['bin'] = (kicker_data['attempt_yards'] // bin_size) * bin_size\n",
        "    actual = kicker_data.groupby('bin')['success'].mean()\n",
        "    return actual.index.values, actual.values\n",
        "\n",
        "def plot_kicker_skill_posterior(\n",
        "    suite,\n",
        "    df: pd.DataFrame,\n",
        "    player_name: str,\n",
        "    bin_width: int = 50,\n",
        ") -> Figure:\n",
        "    \"\"\"Compute and plot the posterior distribution of P(make) for a single kicker.\"\"\"\n",
        "    if not ARVIZ_AVAILABLE or not BAYESIAN_AVAILABLE:\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.text(0.5, 0.5, \"Bayesian analysis not available\", ha='center', va='center')\n",
        "        return fig\n",
        "        \n",
        "    k_idx = suite.get_kicker_id_by_name(df, player_name)\n",
        "    if k_idx is None:\n",
        "        raise ValueError(f\"Kicker '{player_name}' not found\")\n",
        "    \n",
        "    if suite._trace is None:\n",
        "        raise ValueError(\"Model has not been fit yet\")\n",
        "    \n",
        "    # Lazy import matplotlib\n",
        "    import matplotlib.pyplot as plt\n",
        "        \n",
        "    posterior = cast(az.InferenceData, suite._trace).posterior\n",
        "    a = posterior[\"alpha\"].values.flatten()\n",
        "    b = posterior[\"beta_dist\"].values.flatten()\n",
        "    u = posterior[\"u\"].values.reshape(-1, posterior[\"u\"].shape[-1])\n",
        "    u_k = u[:, k_idx]\n",
        "    logit = a + b * 0.0 + u_k\n",
        "    p = 1 / (1 + np.exp(-logit))\n",
        "    \n",
        "    fig, ax = plt.subplots()\n",
        "    ax.hist(p, bins=bin_width, density=True, alpha=0.8)\n",
        "    ax.set_title(f\"Posterior P(make) for {player_name}\")\n",
        "    ax.set_xlabel(\"Probability\")\n",
        "    ax.set_ylabel(\"Density\")\n",
        "    return fig\n",
        "\n",
        "def plot_distance_comparison(\n",
        "    suite,\n",
        "    df: pd.DataFrame,\n",
        "    player_name: str,\n",
        "    bin_size: int = 5\n",
        ") -> Figure:\n",
        "    \"\"\"Plot actual vs predicted success rates by distance.\"\"\"\n",
        "    # Lazy import matplotlib\n",
        "    import matplotlib.pyplot as plt\n",
        "    \n",
        "    # Use cached computation for actual values\n",
        "    bins, actuals = compute_distance_binned_stats(df, player_name, bin_size)\n",
        "    \n",
        "    # Get predictions for this kicker's attempts\n",
        "    kicker_data = df[df['player_name'] == player_name]\n",
        "    preds = suite.predict(kicker_data)\n",
        "    kicker_data['predicted'] = preds\n",
        "    kicker_data['bin'] = (kicker_data['attempt_yards'] // bin_size) * bin_size\n",
        "    predicted = kicker_data.groupby('bin')['predicted'].mean()\n",
        "    \n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(bins, actuals, marker='o', label='Actual', linewidth=2)\n",
        "    ax.plot(predicted.index, predicted.values, marker='s', label='Predicted', linestyle='--')\n",
        "    ax.set_xlabel('Distance (yards)')\n",
        "    ax.set_ylabel('Make Probability')\n",
        "    ax.set_title(f'Performance by Distance: {player_name}')\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "def plot_kicker_epa_distribution(\n",
        "    suite,\n",
        "    df: pd.DataFrame,\n",
        "    player_name: str,\n",
        "    n_samples: int = 500,\n",
        "    bin_width: int = 20,\n",
        ") -> Figure:\n",
        "    \"\"\"\n",
        "    Bootstrap EPA-FG⁺ draws for one kicker and plot histogram.\n",
        "    \"\"\"\n",
        "    if not BAYESIAN_AVAILABLE:\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.text(0.5, 0.5, \"Bayesian analysis not available\", ha='center', va='center')\n",
        "        return fig\n",
        "        \n",
        "    kid = suite.get_kicker_id_by_name(df, player_name)\n",
        "    if kid is None:\n",
        "        raise ValueError(f\"Kicker '{player_name}' not found\")\n",
        "        \n",
        "    work = df.copy()\n",
        "    draws = suite._epa_fg_plus_draws(\n",
        "        work,\n",
        "        kicker_ids=np.array([kid]),  # Convert to numpy array\n",
        "        n_samples=n_samples,\n",
        "        rng=np.random.default_rng(suite.random_seed),\n",
        "        distance_strategy=\"kicker\",\n",
        "    )\n",
        "    epa = draws[:, 0]\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.hist(epa, bins=bin_width, density=True, alpha=0.8)\n",
        "    ax.set_title(f\"EPA-FG⁺ Distribution for {player_name}\")\n",
        "    ax.set_xlabel(\"EPA-FG⁺ (points)\")\n",
        "    ax.set_ylabel(\"Density\")\n",
        "    return fig\n",
        "\n",
        "POINT_ESTIMATE_MODELS = [\n",
        "    \"simple_logistic\",\n",
        "    \"ridge_logistic\",\n",
        "    \"random_forest\",\n",
        "    \"xgboost\",\n",
        "    \"catboost\",\n",
        "]\n",
        "\n",
        "_FIELD_GOAL_RESULT_SUCCESS = \"Made\"\n",
        "_PRESEASON_FLAG = \"Pre\"\n",
        "\n",
        "# ───────────────────────── Streamlit page config ──────────────────────────────\n",
        "st.set_page_config(\n",
        "    page_title=\"NFL Kicker Analysis – Broncos Tech Assessment\",\n",
        "    page_icon=\"🎯\",\n",
        "    layout=\"wide\",\n",
        ")\n",
        "\n",
        "# Custom CSS for dark theme with Broncos‑flavoured palette\n",
        "BRONCOS_COLOURS = {\n",
        "    \"orange\": \"#fb4f14\",\n",
        "    \"navy\": \"#0a2343\",\n",
        "    \"steel\": \"#a5acaf\",\n",
        "}\n",
        "\n",
        "def inject_css() -> None:\n",
        "    \"\"\"Inject a full dark theme with Broncos-styled custom styling.\"\"\"\n",
        "    st.markdown(\n",
        "        f\"\"\"\n",
        "        <style>\n",
        "        /* Page background & text */\n",
        "        .appview-container, .main, .block-container {{\n",
        "            background-color: #121212 !important;\n",
        "            color: #E0E0E0 !important;\n",
        "            font-family: \"Inter\", sans-serif;\n",
        "        }}\n",
        "        \n",
        "        /* Sidebar container and elements */\n",
        "        [data-testid=\"stSidebar\"] {{\n",
        "            background-color: #1E1E1E !important;\n",
        "            color: #E0E0E0 !important;\n",
        "        }}\n",
        "        [data-testid=\"stSidebarNav\"] {{\n",
        "            background-color: #1E1E1E !important;\n",
        "            color: #E0E0E0 !important;\n",
        "        }}\n",
        "        [data-testid=\"stSidebar\"] .css-1d391kg,\n",
        "        [data-testid=\"stSidebar\"] .css-1aw8i8e,\n",
        "        [data-testid=\"stSidebar\"] .css-1vq4p4l {{\n",
        "            background-color: transparent !important;\n",
        "        }}\n",
        "        [data-testid=\"stSidebar\"] h1,\n",
        "        [data-testid=\"stSidebar\"] h2,\n",
        "        [data-testid=\"stSidebar\"] h3,\n",
        "        [data-testid=\"stSidebar\"] h4 {{\n",
        "            color: {BRONCOS_COLOURS['orange']} !important;\n",
        "        }}\n",
        "        \n",
        "        /* Headers & titles */\n",
        "        h1, h2, h3, h4, .stHeader {{\n",
        "            color: #FFFFFF !important;\n",
        "        }}\n",
        "        \n",
        "        /* Buttons */\n",
        "        button[kind=\"primary\"] {{\n",
        "            background-color: {BRONCOS_COLOURS['orange']} !important;\n",
        "            color: #121212 !important;\n",
        "            border-radius: 8px !important;\n",
        "        }}\n",
        "        \n",
        "        /* Metrics cards */\n",
        "        .stMetric {{\n",
        "            background-color: #1E1E1E !important;\n",
        "            border-radius: 8px !important;\n",
        "            box-shadow: 0 4px 6px rgba(0,0,0,0.3) !important;\n",
        "        }}\n",
        "        \n",
        "        /* Tables & dataframes */\n",
        "        .css-1lcbmhc .css-1d391kg, .stDataFrame {{\n",
        "            background-color: #1E1E1E !important;\n",
        "            color: #E0E0E0 !important;\n",
        "        }}\n",
        "        .stDataFrame td, .stDataFrame th {{\n",
        "            background-color: #2A2A2A !important;\n",
        "        }}\n",
        "        \n",
        "        /* Tabs */\n",
        "        div[data-baseweb=\"tab-list\"] button {{\n",
        "            background-color: transparent !important;\n",
        "            color: #E0E0E0 !important;\n",
        "            border-bottom: 3px solid transparent;\n",
        "        }}\n",
        "        \n",
        "        div[data-baseweb=\"tab-list\"] button[aria-selected=\"true\"] {{\n",
        "            background-color: transparent !important;\n",
        "            color: {BRONCOS_COLOURS['orange']} !important;\n",
        "            border-color: {BRONCOS_COLOURS['orange']} !important;\n",
        "            border-radius: 0 !important;\n",
        "        }}\n",
        "        \n",
        "        /* Inputs & selects */\n",
        "        input, select, .stSelectbox, .stTextInput {{\n",
        "            background-color: #2A2A2A !important;\n",
        "            color: #E0E0E0 !important;\n",
        "            border: none !important;\n",
        "            border-radius: 4px !important;\n",
        "        }}\n",
        "        \n",
        "        /* Links and accents */\n",
        "        a, .st-de, .st-ag {{\n",
        "            color: {BRONCOS_COLOURS['orange']} !important;\n",
        "        }}\n",
        "        \n",
        "        /* Card styling */\n",
        "        .st-bb {{\n",
        "            border: none;\n",
        "        }}\n",
        "        .st-bx {{\n",
        "            border-radius: 0.75rem;\n",
        "        }}\n",
        "        \n",
        "        /* Header spacing */\n",
        "        .block-container {{\n",
        "            padding-top: 2rem;\n",
        "        }}\n",
        "        \n",
        "        /* Plot backgrounds */\n",
        "        .stPlot {{\n",
        "            background-color: #1E1E1E !important;\n",
        "            border-radius: 8px !important;\n",
        "            padding: 1rem !important;\n",
        "        }}\n",
        "        \n",
        "        /* Code blocks */\n",
        "        .stCodeBlock {{\n",
        "            background-color: #2A2A2A !important;\n",
        "            color: #E0E0E0 !important;\n",
        "            border-radius: 8px !important;\n",
        "        }}\n",
        "        \n",
        "        /* Dropdown menus */\n",
        "        .stSelectbox > div > div {{\n",
        "            background-color: #2A2A2A !important;\n",
        "            color: #E0E0E0 !important;\n",
        "        }}\n",
        "        </style>\n",
        "        \"\"\",\n",
        "        unsafe_allow_html=True,\n",
        "    )\n",
        "\n",
        "inject_css()\n",
        "\n",
        "st.sidebar.header(\"⚙️ Select Model\")\n",
        "model_types = [\"Uncertainty Interval Models\", \"Point Estimate Models\"]\n",
        "model_type = st.sidebar.selectbox(\"Model Type\", model_types)\n",
        "\n",
        "# ======== Model Directories Setup =========\n",
        "if MODEL_UTILS_AVAILABLE:\n",
        "    try:\n",
        "        correct_mlflow_dir = config.PROJECT_ROOT / \"mlruns\" / \"models\"\n",
        "        fs_models = {}\n",
        "        if correct_mlflow_dir.exists():\n",
        "            fs_models.update(list_saved_models(correct_mlflow_dir))\n",
        "        old_point_estimate_dir = config.MODELS_DIR / \"mlruns\" / \"models\"\n",
        "        if old_point_estimate_dir.exists():\n",
        "            fs_models.update(list_saved_models(old_point_estimate_dir))\n",
        "        mlflow_models = list_registered_models()\n",
        "        all_models = {**mlflow_models, **fs_models}\n",
        "        point_estimate_dir = correct_mlflow_dir\n",
        "        if not point_estimate_dir.exists():\n",
        "            point_estimate_dir.mkdir(parents=True, exist_ok=True)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error accessing model directories: {e}\")\n",
        "        fs_models, mlflow_models, all_models = {}, {}, {}\n",
        "        point_estimate_dir = config.MODELS_DIR / \"mlruns\" / \"models\"\n",
        "else:\n",
        "    fs_models, mlflow_models, all_models = {}, {}, {}\n",
        "    point_estimate_dir = config.MODELS_DIR / \"mlruns\" / \"models\"\n",
        "\n",
        "# ── Cache the metrics lookup so dropdown changes are fast ──\n",
        "@st.cache_data\n",
        "def get_metrics_df(model_name: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Return a DataFrame of all logged metrics for `model_name`.\n",
        "    \"\"\"\n",
        "    metrics = get_best_metrics(model_name) or {}\n",
        "    # Convert {'accuracy':0.88, 'f1':0.82} → DataFrame\n",
        "    df = pd.DataFrame.from_dict(\n",
        "        metrics, orient=\"index\", columns=[\"Value\"]\n",
        "    ).reset_index().rename(columns={\"index\":\"Metric\"})\n",
        "    return df\n",
        "\n",
        "# Cache expensive resource instantiation\n",
        "@st.cache_resource\n",
        "def get_data_loader() -> Optional[DataLoader]:\n",
        "    \"\"\"Cache DataLoader instance for reuse.\"\"\"\n",
        "    if not DATA_LOADER_AVAILABLE:\n",
        "        return None\n",
        "    return DataLoader()\n",
        "\n",
        "@st.cache_resource\n",
        "def get_data_preprocessor() -> Optional[DataPreprocessor]:\n",
        "    \"\"\"Cache DataPreprocessor instance for reuse.\"\"\"\n",
        "    if not DATA_PREPROCESSOR_AVAILABLE:\n",
        "        return None\n",
        "    return DataPreprocessor()\n",
        "\n",
        "@st.cache_data(show_spinner=False, ttl=3600)  # Cache for 1 hour\n",
        "def load_raw_data() -> pd.DataFrame:\n",
        "    \"\"\"Load raw data with fallback between Parquet and CSV formats.\"\"\"\n",
        "    loader = get_data_loader()\n",
        "    if loader is None:\n",
        "        return pd.DataFrame()\n",
        "        \n",
        "    try:\n",
        "        # Try Parquet first (faster)\n",
        "        parquet_path = Path(\"data/raw/field_goal_attempts.parquet\")\n",
        "        if parquet_path.exists():\n",
        "            return pd.read_parquet(parquet_path)\n",
        "            \n",
        "        # Fallback to CSV\n",
        "        return loader.load_complete_dataset()\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading data: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "@st.cache_data(show_spinner=False, ttl=3600)\n",
        "def load_preprocessed_data() -> pd.DataFrame:\n",
        "    \"\"\"Load preprocessed data with optimized format handling.\"\"\"\n",
        "    loader = get_data_loader()\n",
        "    preprocessor = get_data_preprocessor()\n",
        "    \n",
        "    if loader is None or preprocessor is None:\n",
        "        # Try to load from preprocessed files directly\n",
        "        try:\n",
        "            # Check Parquet first\n",
        "            if hasattr(config, 'MODEL_DATA_FILE'):\n",
        "                parquet_path = config.MODEL_DATA_FILE.with_suffix('.parquet')\n",
        "                if parquet_path.exists():\n",
        "                    return pd.read_parquet(parquet_path)\n",
        "            \n",
        "            # Fallback to CSV locations\n",
        "            if hasattr(config, 'MODEL_DATA_FILE') and config.MODEL_DATA_FILE.exists():\n",
        "                return pd.read_csv(config.MODEL_DATA_FILE)\n",
        "            elif hasattr(config, 'PROCESSED_DATA_DIR'):\n",
        "                csv_path = config.PROCESSED_DATA_DIR / \"field_goal_modeling_data.csv\"\n",
        "                if csv_path.exists():\n",
        "                    return pd.read_csv(csv_path)\n",
        "            \n",
        "            st.warning(\"Preprocessed data not available and modules not loaded\")\n",
        "            return pd.DataFrame()\n",
        "        except Exception as e:\n",
        "            st.warning(f\"Error loading data files: {e}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        raw = load_raw_data()\n",
        "        if raw.empty:\n",
        "            return pd.DataFrame()\n",
        "            \n",
        "        # Update preprocessor config\n",
        "        preprocessor.update_config(\n",
        "            min_distance=getattr(config, 'MIN_DISTANCE', 20),\n",
        "            max_distance=getattr(config, 'MAX_DISTANCE', 60),\n",
        "            min_kicker_attempts=getattr(config, 'MIN_KICKER_ATTEMPTS', 10),\n",
        "            season_types=getattr(config, 'SEASON_TYPES', ['Reg', 'Post']),\n",
        "            include_performance_history=True,\n",
        "            include_statistical_features=False,\n",
        "            include_player_status=True,\n",
        "            performance_window=12,\n",
        "        )\n",
        "        \n",
        "        # Apply feature lists if available\n",
        "        feature_lists = getattr(config, 'FEATURE_LISTS', {})\n",
        "        if feature_lists:\n",
        "            preprocessor.update_feature_lists(**feature_lists)\n",
        "        \n",
        "        return preprocessor.preprocess_complete(raw)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error in data preprocessing: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "@st.cache_data(show_spinner=False)\n",
        "def get_bayesian_metrics(suite_dir: Path, df: pd.DataFrame) -> pd.DataFrame:\n",
        "    metrics_path = suite_dir / \"metrics.json\"\n",
        "    if metrics_path.exists():\n",
        "        # Load the exact metrics from training time\n",
        "        try:\n",
        "            with open(metrics_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                metrics = json.load(f)\n",
        "                st.sidebar.success(\"✅ Loaded saved metrics from training\")\n",
        "        except Exception as e:\n",
        "            st.sidebar.warning(f\"Error loading metrics file: {e}\")\n",
        "            metrics = {}\n",
        "    else:\n",
        "        # Fallback: recompute on-the-fly (if modules are available)\n",
        "        if not BAYESIAN_AVAILABLE or not DATA_PREPROCESSOR_AVAILABLE:\n",
        "            st.sidebar.warning(\"⚠️ Cannot compute metrics - modules not available\")\n",
        "            metrics = {}\n",
        "        else:\n",
        "            try:\n",
        "                st.sidebar.warning(\"⚠️ No saved metrics found - recomputing\")\n",
        "                suite = BayesianModelSuite.load_suite(suite_dir)\n",
        "                pre = DataPreprocessor()\n",
        "                \n",
        "                # Safely access config attributes\n",
        "                min_distance = getattr(config, 'MIN_DISTANCE', 20)\n",
        "                max_distance = getattr(config, 'MAX_DISTANCE', 60)\n",
        "                min_kicker_attempts = getattr(config, 'MIN_KICKER_ATTEMPTS', 10)\n",
        "                season_types = getattr(config, 'SEASON_TYPES', ['Reg', 'Post'])\n",
        "                feature_lists = getattr(config, 'FEATURE_LISTS', {})\n",
        "                \n",
        "                pre.update_config(\n",
        "                    min_distance=min_distance,\n",
        "                    max_distance=max_distance,\n",
        "                    min_kicker_attempts=min_kicker_attempts,\n",
        "                    season_types=season_types,\n",
        "                    include_performance_history=False,\n",
        "                    include_statistical_features=False,\n",
        "                    include_player_status=True,\n",
        "                    performance_window=12,\n",
        "                )\n",
        "                \n",
        "                if feature_lists:\n",
        "                    pre.update_feature_lists(**feature_lists)\n",
        "                \n",
        "                metrics = suite.evaluate(df, preprocessor=pre)\n",
        "            except Exception as e:\n",
        "                st.sidebar.error(f\"Error computing metrics: {e}\")\n",
        "                metrics = {}\n",
        "\n",
        "    # Convert to DataFrame for display\n",
        "    dfm = pd.DataFrame.from_dict(\n",
        "        metrics, orient=\"index\", columns=[\"Value\"]\n",
        "    ).reset_index().rename(columns={\"index\":\"Metric\"})\n",
        "    return dfm\n",
        "\n",
        "# === Enhanced setup for cloud deployment ===\n",
        "def setup_for_cloud():\n",
        "    \"\"\"Automatically copy leaderboard files and setup paths for Streamlit Cloud compatibility.\"\"\"\n",
        "    try:\n",
        "        project_root = Path(__file__).parent.absolute()\n",
        "        source_dir = project_root / \"output\"\n",
        "        \n",
        "        # Check for missing leaderboard files in project root\n",
        "        expected_leaderboards = [\n",
        "            \"catboost_leaderboard.csv\", \"random_forest_leaderboard.csv\", \n",
        "            \"ridge_logistic_leaderboard.csv\", \"simple_logistic_leaderboard.csv\",\n",
        "            \"xgboost_leaderboard.csv\"\n",
        "        ]\n",
        "        \n",
        "        missing_leaderboards = [\n",
        "            f for f in expected_leaderboards \n",
        "            if not (project_root / f).exists()\n",
        "        ]\n",
        "        \n",
        "        # Check if we need to copy files and copy them if source exists\n",
        "        if missing_leaderboards and source_dir.exists():\n",
        "            import shutil\n",
        "            for csv_file in source_dir.glob(\"*leaderboard.csv\"):\n",
        "                target_file = project_root / csv_file.name\n",
        "                if not target_file.exists():\n",
        "                    shutil.copy2(csv_file, target_file)\n",
        "                    print(f\"✅ Copied {csv_file.name} to project root\")\n",
        "        \n",
        "        # === NEW: Ensure Bayesian leaderboard is available ===\n",
        "        bayesian_leaderboard_files = [\n",
        "            \"leaderboard.csv\",  # Main Bayesian leaderboard\n",
        "            \"bayesian_features.csv\"  # Bayesian features data\n",
        "        ]\n",
        "        \n",
        "        for filename in bayesian_leaderboard_files:\n",
        "            source_file = source_dir / filename\n",
        "            target_file = project_root / filename\n",
        "            \n",
        "            if source_file.exists() and not target_file.exists():\n",
        "                import shutil\n",
        "                shutil.copy2(source_file, target_file)\n",
        "                print(f\"✅ Copied {filename} to project root for cloud deployment\")\n",
        "        \n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"⚠️ Warning: Setup for cloud failed: {e}\")\n",
        "        return False\n",
        "\n",
        "def find_bayesian_leaderboard() -> Optional[Path]:\n",
        "    \"\"\"Find the Bayesian EPA leaderboard file in various locations.\"\"\"\n",
        "    project_root = Path(__file__).parent.absolute()\n",
        "    \n",
        "    # Priority order for leaderboard search\n",
        "    possible_locations = [\n",
        "        project_root / \"leaderboard.csv\",  # Cloud deployment location\n",
        "        project_root / \"output\" / \"leaderboard.csv\",  # Local development location\n",
        "        project_root / \"data\" / \"processed\" / \"leaderboard.csv\",  # Alternative location\n",
        "    ]\n",
        "    \n",
        "    # Check config location if available\n",
        "    try:\n",
        "        possible_locations.append(config.LEADERBOARD_FILE)\n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    for location in possible_locations:\n",
        "        if location.exists():\n",
        "            return location\n",
        "    \n",
        "    return None\n",
        "\n",
        "# === Enhanced Bayesian model loading functions ===\n",
        "def find_bayesian_data_file() -> Optional[Path]:\n",
        "    \"\"\"Find the bayesian_features.csv file in multiple possible locations.\"\"\"\n",
        "    project_root = Path(__file__).parent.absolute()\n",
        "    \n",
        "    # List of possible locations for the bayesian features file\n",
        "    possible_locations = [\n",
        "        # Primary location from config\n",
        "        getattr(config, 'MODEL_DATA_FILE', None),\n",
        "        # Alternative locations\n",
        "        project_root / \"output\" / \"bayesian_features.csv\",\n",
        "        project_root / \"bayesian_features.csv\",\n",
        "        project_root / \"data\" / \"processed\" / \"bayesian_features.csv\",\n",
        "        Path(\"output/bayesian_features.csv\"),\n",
        "        Path(\"bayesian_features.csv\"),\n",
        "    ]\n",
        "    \n",
        "    # Filter out None values and check each location\n",
        "    for location in possible_locations:\n",
        "        if location is not None and Path(location).exists():\n",
        "            return Path(location)\n",
        "    \n",
        "    return None\n",
        "\n",
        "@st.cache_resource\n",
        "def get_bayesian_suite(suite_path: Path) -> Optional[BayesianModelSuite]:\n",
        "    \"\"\"Cache BayesianModelSuite instance for reuse.\"\"\"\n",
        "    if not BAYESIAN_AVAILABLE:\n",
        "        return None\n",
        "    try:\n",
        "        return BayesianModelSuite.load_suite(suite_path)\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading Bayesian suite: {e}\")\n",
        "        return None\n",
        "\n",
        "@st.cache_data(ttl=3600)\n",
        "def find_bayesian_suite_dirs() -> List[Path]:\n",
        "    \"\"\"Find Bayesian suite directories with caching.\"\"\"\n",
        "    project_root = Path(__file__).parent.absolute()\n",
        "    \n",
        "    # List of possible locations for Bayesian suites\n",
        "    possible_roots = [\n",
        "        getattr(config, 'MODEL_DIR', None),\n",
        "        project_root / \"models\" / \"bayesian\",\n",
        "        Path(\"models/bayesian\"),\n",
        "        project_root / \"bayesian\",\n",
        "    ]\n",
        "    \n",
        "    suite_dirs = []\n",
        "    for suite_root in possible_roots:\n",
        "        if suite_root is not None and Path(suite_root).exists():\n",
        "            # Find valid suite directories\n",
        "            dirs = sorted(\n",
        "                [d for d in Path(suite_root).iterdir()\n",
        "                 if d.is_dir() and (d/\"meta.json\").exists() and (d/\"trace.nc\").exists()],\n",
        "                reverse=True\n",
        "            )\n",
        "            suite_dirs.extend(dirs)\n",
        "    \n",
        "    # Remove duplicates while preserving order\n",
        "    seen = set()\n",
        "    unique_dirs = []\n",
        "    for d in suite_dirs:\n",
        "        if d not in seen:\n",
        "            seen.add(d)\n",
        "            unique_dirs.append(d)\n",
        "    \n",
        "    return unique_dirs\n",
        "\n",
        "@st.cache_data(ttl=3600)\n",
        "def load_bayesian_data_with_fallback() -> Optional[pd.DataFrame]:\n",
        "    \"\"\"Load Bayesian features data with optimized caching.\"\"\"\n",
        "    data_file = find_bayesian_data_file()\n",
        "    \n",
        "    if data_file is not None:\n",
        "        try:\n",
        "            # Try Parquet first\n",
        "            parquet_path = data_file.with_suffix('.parquet')\n",
        "            if parquet_path.exists():\n",
        "                return pd.read_parquet(parquet_path)\n",
        "            # Fallback to CSV\n",
        "            return pd.read_csv(data_file)\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error loading Bayesian data: {e}\")\n",
        "    \n",
        "    # Fallback: Try to generate data from raw sources\n",
        "    if DATA_LOADER_AVAILABLE and DATA_PREPROCESSOR_AVAILABLE:\n",
        "        try:\n",
        "            # Use cached loader\n",
        "            loader = get_data_loader()\n",
        "            if loader is None:\n",
        "                return None\n",
        "            df_raw = loader.load_complete_dataset()\n",
        "            \n",
        "            # Create minimal preprocessor for Bayesian models\n",
        "            from src.nfl_kicker_analysis.data.feature_engineering import FeatureEngineer\n",
        "            engineer = FeatureEngineer()\n",
        "            return engineer.create_all_features(df_raw)\n",
        "            \n",
        "        except Exception as e:\n",
        "            st.error(\"Error generating Bayesian features. Please check data availability.\")\n",
        "    \n",
        "    return None\n",
        "\n",
        "# Run setup\n",
        "setup_for_cloud()\n",
        "\n",
        "# === Main Streamlit App ===\n",
        "\n",
        "if model_type == \"Point Estimate Models\":\n",
        "    st.sidebar.subheader(\"🏆 Point Estimate Models\")\n",
        "    if all_models:\n",
        "        model_names = list(all_models.keys())\n",
        "        selected_model = st.selectbox(\"Choose best model\", model_names)\n",
        "        try:\n",
        "            if selected_model in fs_models:\n",
        "                model = load_model(selected_model, base_dir=point_estimate_dir)\n",
        "            elif selected_model in mlflow_models:\n",
        "                model = load_model(selected_model)\n",
        "            else:\n",
        "                st.warning(f\"Model '{selected_model}' not found\")\n",
        "                model = None\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error loading model '{selected_model}'\")\n",
        "            model = None\n",
        "    else:\n",
        "        st.info(\"No trained models found. Showing available data without model predictions.\")\n",
        "        selected_model = None\n",
        "        model = None\n",
        "\n",
        "    # Create tabs: metrics, leaderboard, and EDA\n",
        "    if model:\n",
        "        lb_tab, metrics_tab, eda_tab = st.tabs(\n",
        "            [\"🏅 Leaderboard\", \"📈 Model Metrics\", \"📊 EDA & Analytics\"]\n",
        "        )\n",
        "    else:\n",
        "        lb_tab, eda_tab = st.tabs(\n",
        "            [\"🏅 Leaderboard\", \"📊 EDA & Analytics\"]\n",
        "        )\n",
        "\n",
        "    # ── Tab A: Leaderboard ─────────────────────────\n",
        "    with lb_tab:\n",
        "        st.header(f\"{selected_model.replace('_',' ').title()} Leaderboard\")\n",
        "        possible_files = []\n",
        "        project_root = Path(__file__).parent.absolute()\n",
        "        possible_files.extend([\n",
        "            project_root / \"output\" / f\"{selected_model}_leaderboard.csv\",\n",
        "            project_root / \"data\" / \"processed\" / f\"{selected_model}_leaderboard.csv\",\n",
        "            project_root / f\"{selected_model}_leaderboard.csv\",\n",
        "        ])\n",
        "        if hasattr(config, 'OUTPUT_DIR'):\n",
        "            possible_files.append(config.OUTPUT_DIR / f\"{selected_model}_leaderboard.csv\")\n",
        "        if hasattr(config, 'PROCESSED_DATA_DIR'):\n",
        "            possible_files.append(config.PROCESSED_DATA_DIR / f\"{selected_model}_leaderboard.csv\")\n",
        "\n",
        "        leaderboard_found = False\n",
        "        for lb_file in possible_files:\n",
        "            if lb_file.exists():\n",
        "                try:\n",
        "                    df_lb = pd.read_csv(lb_file)\n",
        "                    if model is not None:\n",
        "                        try:\n",
        "                            version, accuracy = get_best_model_info(selected_model)\n",
        "                            if accuracy is not None:\n",
        "                                st.write(f\"**Accuracy:** {accuracy:.3f}\")\n",
        "                        except Exception:\n",
        "                            pass\n",
        "                    st.dataframe(df_lb)\n",
        "                    leaderboard_found = True\n",
        "                    break\n",
        "                except Exception:\n",
        "                    continue\n",
        "\n",
        "        if not leaderboard_found:\n",
        "            if model is not None and not load_preprocessed_data().empty:\n",
        "                st.info(\"Generating leaderboard from available data...\")\n",
        "                try:\n",
        "                    data = load_preprocessed_data()\n",
        "                    if 'player_name' in data.columns and 'success' in data.columns:\n",
        "                        leaderboard = (\n",
        "                            data.groupby('player_name')\n",
        "                            .agg({\n",
        "                                'success': ['count', 'sum', 'mean'],\n",
        "                                'attempt_yards': 'mean'\n",
        "                            })\n",
        "                            .round(3)\n",
        "                        )\n",
        "                        leaderboard.columns = ['attempts', 'made', 'success_rate', 'avg_distance']\n",
        "                        leaderboard = leaderboard.reset_index()\n",
        "                        leaderboard = leaderboard[leaderboard['attempts'] >= 10]\n",
        "                        leaderboard = leaderboard.sort_values('success_rate', ascending=False)\n",
        "                        leaderboard['rank'] = range(1, len(leaderboard) + 1)\n",
        "                        leaderboard = leaderboard[['rank', 'player_name', 'attempts', 'success_rate', 'avg_distance']]\n",
        "                        st.dataframe(leaderboard)\n",
        "                        leaderboard_found = True\n",
        "                except Exception:\n",
        "                    st.error(\"Error generating leaderboard from available data\")\n",
        "            if not leaderboard_found:\n",
        "                st.warning(\"No leaderboard available. The model may not have been trained yet.\")\n",
        "\n",
        "    # ── Tab B: Full metrics table (only if model loaded) ────\n",
        "    if model:\n",
        "        with metrics_tab:\n",
        "            st.header(f\"{selected_model.replace('_',' ').title()} Metrics\")\n",
        "            try:\n",
        "                df_metrics = get_metrics_df(selected_model)\n",
        "                st.table(df_metrics)  # shows all logged metrics\n",
        "            except Exception as e:\n",
        "                st.error(f\"Error loading metrics: {str(e)}\")\n",
        "\n",
        "    # ── Tab C: EDA & Analytics (always available) ───────────\n",
        "    with eda_tab:\n",
        "        st.header(\"📊 Exploratory Data Analysis & Diagnostics\")\n",
        "        if not EDA_AVAILABLE:\n",
        "            st.info(\"Showing basic data information\")\n",
        "            try:\n",
        "                data = load_preprocessed_data()\n",
        "                if not data.empty:\n",
        "                    st.write(\"Data Shape:\", data.shape)\n",
        "                    st.write(\"Data Info:\")\n",
        "                    st.dataframe(data.head())\n",
        "                    if 'success' in data.columns:\n",
        "                        success_rate = data['success'].mean()\n",
        "                        st.metric(\"Overall Success Rate\", f\"{success_rate:.3f}\")\n",
        "            except Exception:\n",
        "                st.error(\"Error loading data\")\n",
        "        else:\n",
        "            try:\n",
        "                data = load_preprocessed_data()\n",
        "                if data.empty:\n",
        "                    st.warning(\"No data available for analysis\")\n",
        "                    st.stop()\n",
        "\n",
        "                st.subheader(\"Overall Outcome Distribution\")\n",
        "                _, fig_out = outcome_summary(data)\n",
        "                st.pyplot(fig_out)\n",
        "\n",
        "                st.subheader(\"Success Rate vs Distance\")\n",
        "                _, fig_dist = distance_analysis(data)\n",
        "                st.pyplot(fig_dist)\n",
        "\n",
        "                st.subheader(\"Temporal Trends & Age\")\n",
        "                _, fig_temp = temporal_analysis(data)\n",
        "                st.pyplot(fig_temp)\n",
        "\n",
        "                st.subheader(\"Kicker Performance Dashboard\")\n",
        "                _, fig_kick = kicker_performance_analysis(data)\n",
        "                st.pyplot(fig_kick)\n",
        "\n",
        "                st.subheader(\"Feature Correlation Matrix\")\n",
        "                fig_corr = feature_engineering(data)\n",
        "                st.pyplot(fig_corr)\n",
        "\n",
        "            except Exception:\n",
        "                st.error(\"Error generating EDA plots\")\n",
        "                st.info(\"Please ensure the data files are available\")\n",
        "\n",
        "else:\n",
        "    st.sidebar.subheader(\"🔬 Uncertainty Interval Models\")\n",
        "    suite_dirs = find_bayesian_suite_dirs()\n",
        "    if not suite_dirs:\n",
        "        st.info(\"No Bayesian model suites available. Showing EDA and Technical Paper sections.\")\n",
        "        eda_tab, paper_tab = st.tabs([\"📊 EDA & Analytics\", \"📄 Technical Paper\"])\n",
        "        with eda_tab:\n",
        "            st.header(\"📊 Exploratory Data Analysis & Diagnostics\")\n",
        "            df = load_preprocessed_data()\n",
        "            if not df.empty and EDA_AVAILABLE:\n",
        "                try:\n",
        "                    st.subheader(\"Overall Outcome Distribution\")\n",
        "                    _, fig_out = outcome_summary(df)\n",
        "                    st.pyplot(fig_out)\n",
        "                    st.subheader(\"Success Rate vs Distance\")\n",
        "                    _, fig_dist = distance_analysis(df)\n",
        "                    st.pyplot(fig_dist)\n",
        "                    st.subheader(\"Temporal Trends & Age\")\n",
        "                    _, fig_temp = temporal_analysis(df)\n",
        "                    st.pyplot(fig_temp)\n",
        "                    st.subheader(\"Kicker Performance Dashboard\")\n",
        "                    _, fig_kick = kicker_performance_analysis(df)\n",
        "                    st.pyplot(fig_kick)\n",
        "                    st.subheader(\"Feature Correlation Matrix\")\n",
        "                    fig_corr = feature_engineering(df)\n",
        "                    st.pyplot(fig_corr)\n",
        "                except Exception:\n",
        "                    st.error(\"Error generating EDA plots\")\n",
        "            else:\n",
        "                st.warning(\"EDA data not available\")\n",
        "        with paper_tab:\n",
        "            st.header(\"📄 Technical Paper\")\n",
        "            try:\n",
        "                paper_file = Path(\"data/paper_details/FINAL_PAPER.txt\")\n",
        "                if paper_file.exists():\n",
        "                    with open(paper_file, 'r', encoding='utf-8') as f:\n",
        "                        paper_content = f.read()\n",
        "                    st.markdown(paper_content)\n",
        "                else:\n",
        "                    st.warning(\"Technical paper not found\")\n",
        "            except Exception:\n",
        "                st.error(\"Error loading technical paper\")\n",
        "        st.stop()\n",
        "    else:\n",
        "        selected = st.sidebar.selectbox(\"Choose Bayesian suite\", [d.name for d in suite_dirs])\n",
        "        suite_path = suite_dirs[[d.name for d in suite_dirs].index(selected)]\n",
        "        df = load_bayesian_data_with_fallback()\n",
        "        if df is None:\n",
        "            st.info(\"Bayesian features not available. Showing EDA and Technical Paper sections.\")\n",
        "            eda_tab, paper_tab = st.tabs([\"📊 EDA & Analytics\", \"📄 Technical Paper\"])\n",
        "            with eda_tab:\n",
        "                st.header(\"📊 Exploratory Data Analysis & Diagnostics\")\n",
        "                df_basic = load_preprocessed_data()\n",
        "                if not df_basic.empty and EDA_AVAILABLE:\n",
        "                    try:\n",
        "                        st.subheader(\"Overall Outcome Distribution\")\n",
        "                        _, fig_out = outcome_summary(df_basic)\n",
        "                        st.pyplot(fig_out)\n",
        "                        st.subheader(\"Success Rate vs Distance\")\n",
        "                        _, fig_dist = distance_analysis(df_basic)\n",
        "                        st.pyplot(fig_dist)\n",
        "                        st.subheader(\"Temporal Trends & Age\")\n",
        "                        _, fig_temp = temporal_analysis(df_basic)\n",
        "                        st.pyplot(fig_temp)\n",
        "                        st.subheader(\"Kicker Performance Dashboard\")\n",
        "                        _, fig_kick = kicker_performance_analysis(df_basic)\n",
        "                        st.pyplot(fig_kick)\n",
        "                        st.subheader(\"Feature Correlation Matrix\")\n",
        "                        fig_corr = feature_engineering(df_basic)\n",
        "                        st.pyplot(fig_corr)\n",
        "                    except Exception:\n",
        "                        st.error(\"Error generating EDA plots\")\n",
        "                else:\n",
        "                    st.warning(\"EDA data not available\")\n",
        "            with paper_tab:\n",
        "                st.header(\"📄 Technical Paper\")\n",
        "                try:\n",
        "                    paper_file = Path(\"data/paper_details/FINAL_PAPER.txt\")\n",
        "                    if paper_file.exists():\n",
        "                        with open(paper_file, 'r', encoding='utf-8') as f:\n",
        "                            paper_content = f.read()\n",
        "                        st.markdown(paper_content)\n",
        "                    else:\n",
        "                        st.warning(\"Technical paper not found\")\n",
        "                except Exception:\n",
        "                    st.error(\"Error loading technical paper\")\n",
        "            st.stop()\n",
        "\n",
        "        # Load Bayesian suite and create tabs\n",
        "        try:\n",
        "            suite = BayesianModelSuite.load_suite(suite_path)\n",
        "            lb_tab, metrics_tab, analysis_tab, eda_tab, paper_tab = st.tabs([\n",
        "                \"🏅 EPA-FG⁺ Leaderboard\",\n",
        "                \"📈 Model Metrics\",\n",
        "                \"🎯 Kicker Analysis\",\n",
        "                \"📊 EDA & Analytics\",\n",
        "                \"📄 Technical Paper\"\n",
        "            ])\n",
        "\n",
        "            # Tab A: EPA-FG⁺ Leaderboard\n",
        "            with lb_tab:\n",
        "                st.header(\"EPA-FG⁺ Leaderboard\")\n",
        "                try:\n",
        "                    leaderboard_file = find_bayesian_leaderboard()\n",
        "                    if leaderboard_file and leaderboard_file.exists():\n",
        "                        df_lb = pd.read_csv(leaderboard_file)\n",
        "                        st.dataframe(df_lb)\n",
        "                    else:\n",
        "                        st.info(\"Generating EPA-FG⁺ leaderboard...\")\n",
        "                        df_lb = suite.generate_epa_leaderboard(df)\n",
        "                        st.dataframe(df_lb)\n",
        "                except Exception:\n",
        "                    st.error(\"Error generating EPA-FG⁺ leaderboard\")\n",
        "\n",
        "            # Tab B: Model Metrics\n",
        "            with metrics_tab:\n",
        "                st.header(\"Model Metrics\")\n",
        "                try:\n",
        "                    df_metrics = get_bayesian_metrics(suite_path, df)\n",
        "                    st.table(df_metrics)\n",
        "                except Exception:\n",
        "                    st.error(\"Error loading model metrics\")\n",
        "\n",
        "            # Tab C: Kicker Analysis\n",
        "            with analysis_tab:\n",
        "                st.header(\"Individual Kicker Analysis\")\n",
        "                try:\n",
        "                    kicker_names = sorted(df['player_name'].unique())\n",
        "                    selected_kicker = st.selectbox(\"Select Kicker\", kicker_names)\n",
        "                    \n",
        "                    # Use cached metrics computation\n",
        "                    metrics = compute_kicker_metrics(df, selected_kicker)\n",
        "                    \n",
        "                    # Display key metrics in columns\n",
        "                    col1, col2, col3 = st.columns(3)\n",
        "                    with col1:\n",
        "                        st.metric(\"Total Attempts\", f\"{metrics['total_attempts']:,}\")\n",
        "                    with col2:\n",
        "                        st.metric(\"Success Rate\", f\"{metrics['success_rate']:.1f}%\")\n",
        "                    with col3:\n",
        "                        st.metric(\"Avg Distance\", f\"{metrics['avg_distance']:.1f} yards\")\n",
        "                    \n",
        "                    st.markdown(\"---\")\n",
        "                    \n",
        "                    # Create two columns for visualizations\n",
        "                    viz_col1, viz_col2 = st.columns(2)\n",
        "                    \n",
        "                    with viz_col1:\n",
        "                        st.subheader(\"Make Probability Distribution\")\n",
        "                        fig_post = plot_kicker_skill_posterior(suite, df, selected_kicker)\n",
        "                        st.pyplot(fig_post)\n",
        "                    \n",
        "                    with viz_col2:\n",
        "                        st.subheader(\"Prediction vs Actuals by Distance\")\n",
        "                        fig_dist = plot_distance_comparison(suite, df, selected_kicker)\n",
        "                        st.pyplot(fig_dist)\n",
        "                        st.caption(\"Note: Predictions are binned into 5-yard intervals for clearer visualization.\")\n",
        "                        \n",
        "                except Exception as e:\n",
        "                    st.error(f\"Error generating kicker analysis plots: {e}\")\n",
        "\n",
        "            # Tab D: EDA & Analytics\n",
        "            with eda_tab:\n",
        "                st.header(\"📊 Exploratory Data Analysis & Diagnostics\")\n",
        "                if not EDA_AVAILABLE:\n",
        "                    st.info(\"Showing basic data information\")\n",
        "                    try:\n",
        "                        if not df.empty:\n",
        "                            st.write(\"Data Shape:\", df.shape)\n",
        "                            st.write(\"Data Info:\")\n",
        "                            st.dataframe(df.head())\n",
        "                            if 'success' in df.columns:\n",
        "                                success_rate = df['success'].mean()\n",
        "                                st.metric(\"Overall Success Rate\", f\"{success_rate:.3f}\")\n",
        "                    except Exception:\n",
        "                        st.error(\"Error loading data\")\n",
        "                else:\n",
        "                    try:\n",
        "                        if df.empty:\n",
        "                            st.warning(\"No data available for analysis\")\n",
        "                            st.stop()\n",
        "\n",
        "                        st.subheader(\"Overall Outcome Distribution\")\n",
        "                        _, fig_out = outcome_summary(df)\n",
        "                        st.pyplot(fig_out)\n",
        "\n",
        "                        st.subheader(\"Success Rate vs Distance\")\n",
        "                        _, fig_dist = distance_analysis(df)\n",
        "                        st.pyplot(fig_dist)\n",
        "\n",
        "                        st.subheader(\"Temporal Trends & Age\")\n",
        "                        _, fig_temp = temporal_analysis(df)\n",
        "                        st.pyplot(fig_temp)\n",
        "\n",
        "                        st.subheader(\"Kicker Performance Dashboard\")\n",
        "                        _, fig_kick = kicker_performance_analysis(df)\n",
        "                        st.pyplot(fig_kick)\n",
        "\n",
        "                        st.subheader(\"Feature Correlation Matrix\")\n",
        "                        fig_corr = feature_engineering(df)\n",
        "                        st.pyplot(fig_corr)\n",
        "                    except Exception:\n",
        "                        st.error(\"Error generating EDA plots\")\n",
        "\n",
        "            # Tab E: Technical Paper\n",
        "            with paper_tab:\n",
        "                st.header(\"📄 Technical Paper\")\n",
        "                try:\n",
        "                    paper_file = Path(\"data/paper_details/FINAL_PAPER.txt\")\n",
        "                    if paper_file.exists():\n",
        "                        with open(paper_file, 'r', encoding='utf-8') as f:\n",
        "                            paper_content = f.read()\n",
        "                        st.markdown(paper_content)\n",
        "                    else:\n",
        "                        st.warning(\"Technical paper not found\")\n",
        "                except Exception:\n",
        "                    st.error(\"Error loading technical paper\")\n",
        "\n",
        "        except Exception:\n",
        "            st.error(\"Error loading Bayesian model suite\")\n",
        "            st.info(\"Showing EDA and Technical Paper sections only\")\n",
        "            eda_tab, paper_tab = st.tabs([\"📊 EDA & Analytics\", \"📄 Technical Paper\"])\n",
        "            # ... (rest of the fallback code remains the same)\n",
        "\n",
        "# Note: Technical Paper section is now handled within the tabs above\n",
        "\n",
        "# ────────────────────────── Technical Paper Section ──────────────────────────\n",
        "st.markdown(\"---\")\n",
        "st.header(\"📄 Technical Paper\")\n",
        "\n",
        "try:\n",
        "    with open(\"data/paper_details/FINAL_PAPER.txt\", \"r\") as f:\n",
        "        paper_content = f.read()\n",
        "    \n",
        "    sections = paper_content.split(\"```mermaid\")\n",
        "    st.markdown(sections[0])\n",
        "    \n",
        "    for i, section in enumerate(sections[1:], 1):\n",
        "        mermaid_and_rest = section.split(\"```\", 2)\n",
        "        if len(mermaid_and_rest) >= 2:\n",
        "            mermaid_content = mermaid_and_rest[0].strip()\n",
        "            st.write(\"\")\n",
        "            st.code(mermaid_content, language=\"mermaid\")\n",
        "            st.write(\"\")\n",
        "            if len(mermaid_and_rest) > 1:\n",
        "                st.markdown(mermaid_and_rest[1])\n",
        "    \n",
        "    st.markdown(\"---\")\n",
        "    st.caption(\"© 2025 Geoffrey Hadfield. All rights reserved.\")\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    st.error(\"Technical paper not found. Please ensure the paper file exists.\")\n",
        "except Exception:\n",
        "    st.error(\"Error loading technical paper\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.17"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
